{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3acb4f-edae-4342-9a26-7f8bf3a57e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv pip install torch transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1898cf2f-ccf6-4cc8-8aec-c2ce42f7b561",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.11/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.11/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_593934/1582511767.py\", line 2, in <module>\n",
      "    import torch\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Model imports\n",
    "from model import SmartContractVulnerabilityGAN\n",
    "\n",
    "# Training imports\n",
    "from train import VulnerabilityDetectionTrainer\n",
    "\n",
    "# Data processing imports\n",
    "from data_processing import SmartContractDataset, preprocess_contract\n",
    "\n",
    "# Optional but useful imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # for progress bars\n",
    "import logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d5051a-5c5f-46fc-b3d6-25b687eb0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "Number of GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c41c22-96f2-4183-900e-0569d247a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"jainabh/smart_contracts_malicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c698aa2-b74e-4a7e-b83e-0a7da5cdd333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['contract_source', 'malicious'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d24547d-35e4-4991-b50e-21e01c7fe09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2000 * 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3db09ac-8da3-4c43-9aad-43c483ebfe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with correct dimensions\n",
    "model = SmartContractVulnerabilityGAN(d_model=768)  # Use 768 to match CodeBERT's output\n",
    "\n",
    "# Move model to CUDA\n",
    "model = model.cuda()\n",
    "\n",
    "# Load and preprocess data\n",
    "train_contracts = ds['train'][0:1400]['contract_source']  # Changed from 'contract_source' to 'source_code'\n",
    "train_labels = ds['train'][0:1400]['malicious']\n",
    "val_contracts = ds['train'][1400:-1]['contract_source']  # Changed from 'contract_source' to 'source_code'\n",
    "val_labels = ds['train'][1400:-1]['malicious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c68e47-73e6-4643-8734-6305337869a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "def parse_solidity_to_ast(code: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse Solidity code into a simplified AST structure\n",
    "    \"\"\"\n",
    "    def extract_contract_info(code: str) -> Dict[str, Any]:\n",
    "        # Extract contract name\n",
    "        contract_match = re.search(r'contract\\s+(\\w+)', code)\n",
    "        contract_name = contract_match.group(1) if contract_match else \"Unknown\"\n",
    "        \n",
    "        # Extract functions\n",
    "        functions = []\n",
    "        function_pattern = r'function\\s+(\\w+)\\s*\\(([^)]*)\\)\\s*(?:public|private|internal|external)?\\s*(?:view|pure|payable)?\\s*(?:returns\\s*\\(([^)]*)\\))?\\s*{'\n",
    "        for match in re.finditer(function_pattern, code):\n",
    "            func_name = match.group(1)\n",
    "            params = match.group(2).split(',') if match.group(2) else []\n",
    "            returns = match.group(3).split(',') if match.group(3) else []\n",
    "            \n",
    "            functions.append({\n",
    "                'name': func_name,\n",
    "                'parameters': [p.strip() for p in params],\n",
    "                'returns': [r.strip() for r in returns]\n",
    "            })\n",
    "        \n",
    "        # Extract state variables\n",
    "        variables = []\n",
    "        var_pattern = r'(?:uint|address|string|bool|mapping)\\s+(?:\\w+)\\s+(\\w+)'\n",
    "        for match in re.finditer(var_pattern, code):\n",
    "            variables.append(match.group(1))\n",
    "        \n",
    "        return {\n",
    "            'type': 'Contract',\n",
    "            'name': contract_name,\n",
    "            'functions': functions,\n",
    "            'variables': variables\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Clean the code\n",
    "        code = re.sub(r'//.*?\\n|/\\*.*?\\*/', '', code)  # Remove comments\n",
    "        code = re.sub(r'\\s+', ' ', code)  # Normalize whitespace\n",
    "        \n",
    "        # Parse the code\n",
    "        ast = extract_contract_info(code)\n",
    "        return ast\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing code: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def prepare_code2vec_input(ast: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert AST to code2vec input format\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    \n",
    "    def extract_paths(node: Dict[str, Any], current_path: List[str] = None):\n",
    "        if current_path is None:\n",
    "            current_path = []\n",
    "            \n",
    "        # Add current node to path\n",
    "        if 'name' in node:\n",
    "            current_path.append(node['name'])\n",
    "            \n",
    "        # Process functions\n",
    "        if 'functions' in node:\n",
    "            for func in node['functions']:\n",
    "                func_path = current_path + [func['name']]\n",
    "                paths.append(' '.join(func_path))\n",
    "                \n",
    "                # Add parameter paths\n",
    "                for param in func['parameters']:\n",
    "                    param_path = func_path + [param]\n",
    "                    paths.append(' '.join(param_path))\n",
    "                \n",
    "                # Add return paths\n",
    "                for ret in func['returns']:\n",
    "                    ret_path = func_path + [ret]\n",
    "                    paths.append(' '.join(ret_path))\n",
    "        \n",
    "        # Process variables\n",
    "        if 'variables' in node:\n",
    "            for var in node['variables']:\n",
    "                var_path = current_path + [var]\n",
    "                paths.append(' '.join(var_path))\n",
    "    \n",
    "    extract_paths(ast)\n",
    "    return paths\n",
    "\n",
    "# Example usage:\n",
    "def process_contract_for_code2vec(code: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process a Solidity contract for code2vec\n",
    "    \"\"\"\n",
    "    # Parse code to AST\n",
    "    ast = parse_solidity_to_ast(code)\n",
    "    if ast is None:\n",
    "        return []\n",
    "    \n",
    "    # Convert AST to code2vec input format\n",
    "    paths = prepare_code2vec_input(ast)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ddce1da-3c4c-4af7-8f2d-fc2ca8da0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset that includes path embeddings\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, AutoModel  # Add these imports\n",
    "\n",
    "class SmartContractDatasetWithPaths(Dataset):\n",
    "    def __init__(self, contracts, labels, tokenizer, code2vec_model):\n",
    "        self.contracts = contracts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.code2vec_model = code2vec_model\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.contracts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        contract = self.contracts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Parse contract to AST and generate paths\n",
    "        ast = parse_solidity_to_ast(contract)\n",
    "        paths = prepare_code2vec_input(ast)\n",
    "        \n",
    "        # Convert paths to string for tokenization\n",
    "        paths_str = ' '.join([' '.join(path) for path in paths])\n",
    "        \n",
    "        # Tokenize contract\n",
    "        contract_inputs = self.tokenizer(\n",
    "            contract,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize paths\n",
    "        path_inputs = self.tokenizer(\n",
    "            paths_str,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': contract_inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': contract_inputs['attention_mask'].squeeze(0),\n",
    "            'path_input_ids': path_inputs['input_ids'].squeeze(0),\n",
    "            'path_attention_mask': path_inputs['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf0c4cf1-34c7-4cd6-8a24-306cb9b734c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and code2vec model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "code2vec_model = AutoModel.from_pretrained('microsoft/codebert-base').cuda()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SmartContractDatasetWithPaths(\n",
    "    train_contracts, \n",
    "    train_labels,\n",
    "    tokenizer,\n",
    "    code2vec_model\n",
    ")\n",
    "\n",
    "val_dataset = SmartContractDatasetWithPaths(\n",
    "    val_contracts,\n",
    "    val_labels,\n",
    "    tokenizer,\n",
    "    code2vec_model\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9013060-daab-41aa-b7c1-67f564808280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training epoch...\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "from train import VulnerabilityDetectionTrainer\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Create a directory for checkpoints\n",
    "checkpoint_dir = 'v4-checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = VulnerabilityDetectionTrainer(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Start timer for this epoch\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    g_loss, d_loss, decoder_loss = trainer.train_epoch()\n",
    "    val_loss = trainer.validate()\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Print training progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Generator Loss: {g_loss:.4f}\")\n",
    "    print(f\"Discriminator Loss: {d_loss:.4f}\")\n",
    "    print(f\"Decoder Loss: {decoder_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            # Model states\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'generator_state_dict': model.generator.state_dict(),\n",
    "            'discriminator_state_dict': model.discriminator.state_dict(),\n",
    "            'decoder_state_dict': model.decoder.state_dict(),\n",
    "    \n",
    "            # Optimizer states\n",
    "            'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "            'optimizer_decoder_state_dict': trainer.optimizer_decoder.state_dict(),\n",
    "    \n",
    "            # Loss values\n",
    "            'g_loss': g_loss,\n",
    "            'd_loss': d_loss,\n",
    "            'decoder_loss': decoder_loss,\n",
    "            'val_loss': val_loss,\n",
    "    \n",
    "            # Model configuration\n",
    "            'model_config': {\n",
    "                'd_model': model.d_model,\n",
    "                'vocab_size': model.vocab_size,\n",
    "                'max_length': model.max_length\n",
    "            },\n",
    "    \n",
    "            # Training metadata\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'epoch_time': epoch_time\n",
    "        }\n",
    "    \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}_model_v4.pt')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "    \n",
    "        # Save best model if validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = os.path.join(checkpoint_dir, 'best_model_v4.pt')\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9317f5c-11ec-486b-b153-e9aebb559647",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d417285-4a34-46d4-8247-f3ef3cba0820",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Re-train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c75f370-9542-40c5-b329-b5285a629b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "from train import VulnerabilityDetectionTrainer\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Create a directory for checkpoints\n",
    "checkpoint_dir = 'v3-checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = VulnerabilityDetectionTrainer(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fae40e6f-dce6-4c18-981e-c0e53f9cd6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v3-checkpoints/checkpoint_epoch_30_model_v3.pt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd835ecb-c7c1-4b26-96bc-0e4904f46bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 20\n",
      "Previous validation loss: 0.0070\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_20_model_v3.pt')  # Change this to your checkpoint file\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load model states\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "model.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "model.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "# Load optimizer states\n",
    "trainer.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "trainer.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "trainer.optimizer_decoder.load_state_dict(checkpoint['optimizer_decoder_state_dict'])\n",
    "\n",
    "# Get the epoch to start from and best validation loss\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_val_loss = checkpoint['val_loss']\n",
    "\n",
    "print(f\"Loaded checkpoint from epoch {start_epoch + 1}\")\n",
    "print(f\"Previous validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26779f5-56bc-4d32-9c6a-81ce63c676c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training epoch...\n"
     ]
    }
   ],
   "source": [
    "# Training loop - start from the next epoch\n",
    "num_epochs = 120\n",
    "\n",
    "for epoch in range(start_epoch + 1, num_epochs):  # Start from the next epoch\n",
    "    # Start timer for this epoch\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    g_loss, d_loss, decoder_loss = trainer.train_epoch()\n",
    "    val_loss = trainer.validate()\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Print training progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Generator Loss: {g_loss:.4f}\")\n",
    "    print(f\"Discriminator Loss: {d_loss:.4f}\")\n",
    "    print(f\"Decoder Loss: {decoder_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            # Model states\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'generator_state_dict': model.generator.state_dict(),\n",
    "            'discriminator_state_dict': model.discriminator.state_dict(),\n",
    "            'decoder_state_dict': model.decoder.state_dict(),\n",
    "            \n",
    "            # Optimizer states\n",
    "            'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "            'optimizer_decoder_state_dict': trainer.optimizer_decoder.state_dict(),\n",
    "            \n",
    "            # Loss values\n",
    "            'g_loss': g_loss,\n",
    "            'd_loss': d_loss,\n",
    "            'decoder_loss': decoder_loss,\n",
    "            'val_loss': val_loss,\n",
    "            \n",
    "            # Model configuration\n",
    "            'model_config': {\n",
    "                'vocab_size': model.decoder.vocab_size,\n",
    "                'max_length': model.decoder.max_length\n",
    "            },\n",
    "            \n",
    "            # Training metadata\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'epoch_time': epoch_time\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}_model_v3.pt')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Saved checkpoint for epoch {epoch+1}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = os.path.join(checkpoint_dir, 'best_model_v3.pt')\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9277d-d896-44c6-af23-db7512b8f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1088dc1-ee3e-457b-980f-cbfa67896dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6116b2b-55dc-4842-8794-88283e9e2acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ca8665-9d47-4383-8fbf-ec85abb2ba8d",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "\n",
    "1. Input Processing:\n",
    "Initial input: [32, 512] (batch_size=32, sequence_length=512)\n",
    "After embedding: [32, 512, 512] (batch_size=32, sequence_length=512, embedding_dim=512)\n",
    "This is correct because the embedding layer converts each token to a 512-dimensional vector\n",
    "\n",
    "2. Path Embeddings Processing:\n",
    "Initial path embeddings: [32, 768] (batch_size=32, code2vec_dim=768)\n",
    "After path embedding layer: [32, 512] (batch_size=32, transformer_dim=512)\n",
    "The linear layer converts from code2vec's 768 dimensions to transformer's 512 dimensions\n",
    "After expansion: [32, 512, 512] (batch_size=32, sequence_length=512, transformer_dim=512)\n",
    "The path embeddings are expanded to match the sequence length\n",
    "\n",
    "3. Final Shape:\n",
    "[32, 512, 512] (batch_size=32, sequence_length=512, transformer_dim=512)\n",
    "This is the correct shape for the transformer layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9d4bfa-25f5-4b56-b679-44fc3102049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for epoch 10\n"
     ]
    }
   ],
   "source": [
    "checkpoint = {\n",
    "    # Model states\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'generator_state_dict': model.generator.state_dict(),\n",
    "    'discriminator_state_dict': model.discriminator.state_dict(),\n",
    "    'decoder_state_dict': model.decoder.state_dict(),\n",
    "    \n",
    "    # Optimizer states\n",
    "    'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "    'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "    'optimizer_decoder_state_dict': trainer.optimizer_decoder.state_dict(),\n",
    "    \n",
    "    # Loss values\n",
    "    'g_loss': g_loss,\n",
    "    'd_loss': d_loss,\n",
    "    'decoder_loss': decoder_loss,\n",
    "    'val_loss': val_loss,\n",
    "    \n",
    "    # Model configuration\n",
    "    'model_config': {\n",
    "        #'d_model': model.d_model,\n",
    "        'vocab_size': model.decoder.vocab_size,\n",
    "        'max_length': model.decoder.max_length\n",
    "    },\n",
    "    \n",
    "    # Training metadata\n",
    "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'epoch_time': epoch_time\n",
    "}\n",
    "\n",
    "# Save regular checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}_model_v3.pt')\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Saved checkpoint for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "812eafcf-c1de-4283-905d-9943fcacffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [119/120]\n",
      "Generator Loss: 10.5703\n",
      "Discriminator Loss: 0.0062\n",
      "Validation Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "print(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "print(f\"Generator Loss: {g_loss:.4f}\")\n",
    "print(f\"Discriminator Loss: {d_loss:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f2e17-41c2-4553-a974-d92ec9add514",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Save model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a542cf7e-3f9a-405f-b855-74f4ed22cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training loop\n",
    "# Save the final model and training state\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "    'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "    'g_loss': g_loss,\n",
    "    'd_loss': d_loss,\n",
    "    'val_loss': val_loss,\n",
    "    'model_config': {\n",
    "        'd_model': 768,\n",
    "    }\n",
    "}, 'final_model_v3.pt')\n",
    "\n",
    "# If you want to save just the model for inference\n",
    "torch.save(model.state_dict(), 'model_weights_v3.pt')\n",
    "\n",
    "# If you want to save the entire model\n",
    "torch.save(model, 'full_model_v3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4518ba2-796c-4576-b707-dce9b7b50f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model with additional information\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "    'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "    'g_loss': g_loss,\n",
    "    'd_loss': d_loss,\n",
    "    'val_loss': val_loss,\n",
    "    'model_config': {\n",
    "        'd_model': 768\n",
    "    },\n",
    "    'training_config': {\n",
    "        'learning_rate': 0.0002,\n",
    "        'beta1': 0.5,\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    'training_history': {\n",
    "        'g_losses': g_loss,  # List of generator losses\n",
    "        'd_losses': d_loss,  # List of discriminator losses\n",
    "        'val_losses': val_loss  # List of validation losses\n",
    "    }\n",
    "}, 'final_model_v3_with_history.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bdb45a-1d83-4f1c-ba3d-537122d6d20d",
   "metadata": {},
   "source": [
    "# Load Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c635a8-7b46-4a65-8310-6752630ea0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full training state\n",
    "checkpoint = torch.load('final_model_v3.pt')\n",
    "model = SmartContractVulnerabilityGAN(**checkpoint['model_config'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.cuda() \n",
    "\n",
    "# Initialize trainer with loaded model\n",
    "trainer = VulnerabilityDetectionTrainer(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader\n",
    ")\n",
    "\n",
    "# Load optimizer states if needed\n",
    "trainer.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "trainer.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73a2faac-91c0-42d9-8166-27b46ca15918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you just want to load the model weights\n",
    "model = SmartContractVulnerabilityGAN(d_model=768)\n",
    "model.load_state_dict(torch.load('model_weights.pt'))\n",
    "model = model.cuda()  # Move to GPU if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084e260-1dab-4400-8cd1-23de24a06f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you saved the entire model\n",
    "model = torch.load('full_model.pt')\n",
    "model = model.cuda()  # Move to GPU if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d56c1-6276-4a53-a46f-eef634bdbcb5",
   "metadata": {},
   "source": [
    "## Model Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f7a67e-c86b-4b37-83cd-ac2bd8f74be7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of model: <class 'model.SmartContractVulnerabilityGAN'>\n",
      "\n",
      "Model Components:\n",
      "--------------------------------------------------\n",
      "\n",
      "Model Attributes:\n",
      "- T_destination\n",
      "- add_module\n",
      "- apply\n",
      "- bfloat16\n",
      "- buffers\n",
      "- call_super_init\n",
      "- children\n",
      "- codebert\n",
      "- compile\n",
      "- cpu\n",
      "- cuda\n",
      "- decode_embeddings\n",
      "- decoder\n",
      "- discriminator\n",
      "- double\n",
      "- dump_patches\n",
      "- eval\n",
      "- extra_repr\n",
      "- float\n",
      "- forward\n",
      "- generate_code\n",
      "- generator\n",
      "- get_buffer\n",
      "- get_extra_state\n",
      "- get_parameter\n",
      "- get_submodule\n",
      "- half\n",
      "- ipu\n",
      "- load_state_dict\n",
      "- modules\n",
      "- named_buffers\n",
      "- named_children\n",
      "- named_modules\n",
      "- named_parameters\n",
      "- parameters\n",
      "- register_backward_hook\n",
      "- register_buffer\n",
      "- register_forward_hook\n",
      "- register_forward_pre_hook\n",
      "- register_full_backward_hook\n",
      "- register_full_backward_pre_hook\n",
      "- register_load_state_dict_post_hook\n",
      "- register_module\n",
      "- register_parameter\n",
      "- register_state_dict_pre_hook\n",
      "- requires_grad_\n",
      "- set_extra_state\n",
      "- share_memory\n",
      "- state_dict\n",
      "- to\n",
      "- to_empty\n",
      "- tokenizer\n",
      "- train\n",
      "- training\n",
      "- transformer\n",
      "- type\n",
      "- xpu\n",
      "- zero_grad\n",
      "\n",
      "Model Structure:\n",
      "SmartContractVulnerabilityGAN(\n",
      "  (codebert): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (transformer): SmartContractTransformer(\n",
      "    (transformer): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (generator): Generator(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=256, out_features=768, bias=True)\n",
      "      (7): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (discriminator): Discriminator(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (6): LeakyReLU(negative_slope=0.2)\n",
      "      (7): Dropout(p=0.3, inplace=False)\n",
      "      (8): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (9): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (decoder): CodeDecoder(\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=1536, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1536, out_features=768, bias=True)\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embedding): Embedding(50000, 768)\n",
      "    (output_layer): Linear(in_features=768, out_features=50000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of model:\", type(model))\n",
    "\n",
    "# If it's a SmartContractVulnerabilityGAN object, we can inspect its components directly\n",
    "print(\"\\nModel Components:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Print model attributes\n",
    "print(\"\\nModel Attributes:\")\n",
    "for attr in dir(model):\n",
    "    if not attr.startswith('_'):  # Skip private attributes\n",
    "        print(f\"- {attr}\")\n",
    "\n",
    "# Print model structure\n",
    "print(\"\\nModel Structure:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b9b5e4b-7ac0-4cdf-a029-11b68f2d6b00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model State Dict:\n",
      "- codebert.embeddings.word_embeddings.weight\n",
      "- codebert.embeddings.position_embeddings.weight\n",
      "- codebert.embeddings.token_type_embeddings.weight\n",
      "- codebert.embeddings.LayerNorm.weight\n",
      "- codebert.embeddings.LayerNorm.bias\n",
      "- codebert.encoder.layer.0.attention.self.query.weight\n",
      "- codebert.encoder.layer.0.attention.self.query.bias\n",
      "- codebert.encoder.layer.0.attention.self.key.weight\n",
      "- codebert.encoder.layer.0.attention.self.key.bias\n",
      "- codebert.encoder.layer.0.attention.self.value.weight\n",
      "- codebert.encoder.layer.0.attention.self.value.bias\n",
      "- codebert.encoder.layer.0.attention.output.dense.weight\n",
      "- codebert.encoder.layer.0.attention.output.dense.bias\n",
      "- codebert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.0.intermediate.dense.weight\n",
      "- codebert.encoder.layer.0.intermediate.dense.bias\n",
      "- codebert.encoder.layer.0.output.dense.weight\n",
      "- codebert.encoder.layer.0.output.dense.bias\n",
      "- codebert.encoder.layer.0.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.0.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.1.attention.self.query.weight\n",
      "- codebert.encoder.layer.1.attention.self.query.bias\n",
      "- codebert.encoder.layer.1.attention.self.key.weight\n",
      "- codebert.encoder.layer.1.attention.self.key.bias\n",
      "- codebert.encoder.layer.1.attention.self.value.weight\n",
      "- codebert.encoder.layer.1.attention.self.value.bias\n",
      "- codebert.encoder.layer.1.attention.output.dense.weight\n",
      "- codebert.encoder.layer.1.attention.output.dense.bias\n",
      "- codebert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.1.intermediate.dense.weight\n",
      "- codebert.encoder.layer.1.intermediate.dense.bias\n",
      "- codebert.encoder.layer.1.output.dense.weight\n",
      "- codebert.encoder.layer.1.output.dense.bias\n",
      "- codebert.encoder.layer.1.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.1.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.2.attention.self.query.weight\n",
      "- codebert.encoder.layer.2.attention.self.query.bias\n",
      "- codebert.encoder.layer.2.attention.self.key.weight\n",
      "- codebert.encoder.layer.2.attention.self.key.bias\n",
      "- codebert.encoder.layer.2.attention.self.value.weight\n",
      "- codebert.encoder.layer.2.attention.self.value.bias\n",
      "- codebert.encoder.layer.2.attention.output.dense.weight\n",
      "- codebert.encoder.layer.2.attention.output.dense.bias\n",
      "- codebert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.2.intermediate.dense.weight\n",
      "- codebert.encoder.layer.2.intermediate.dense.bias\n",
      "- codebert.encoder.layer.2.output.dense.weight\n",
      "- codebert.encoder.layer.2.output.dense.bias\n",
      "- codebert.encoder.layer.2.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.2.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.3.attention.self.query.weight\n",
      "- codebert.encoder.layer.3.attention.self.query.bias\n",
      "- codebert.encoder.layer.3.attention.self.key.weight\n",
      "- codebert.encoder.layer.3.attention.self.key.bias\n",
      "- codebert.encoder.layer.3.attention.self.value.weight\n",
      "- codebert.encoder.layer.3.attention.self.value.bias\n",
      "- codebert.encoder.layer.3.attention.output.dense.weight\n",
      "- codebert.encoder.layer.3.attention.output.dense.bias\n",
      "- codebert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.3.intermediate.dense.weight\n",
      "- codebert.encoder.layer.3.intermediate.dense.bias\n",
      "- codebert.encoder.layer.3.output.dense.weight\n",
      "- codebert.encoder.layer.3.output.dense.bias\n",
      "- codebert.encoder.layer.3.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.3.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.4.attention.self.query.weight\n",
      "- codebert.encoder.layer.4.attention.self.query.bias\n",
      "- codebert.encoder.layer.4.attention.self.key.weight\n",
      "- codebert.encoder.layer.4.attention.self.key.bias\n",
      "- codebert.encoder.layer.4.attention.self.value.weight\n",
      "- codebert.encoder.layer.4.attention.self.value.bias\n",
      "- codebert.encoder.layer.4.attention.output.dense.weight\n",
      "- codebert.encoder.layer.4.attention.output.dense.bias\n",
      "- codebert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.4.intermediate.dense.weight\n",
      "- codebert.encoder.layer.4.intermediate.dense.bias\n",
      "- codebert.encoder.layer.4.output.dense.weight\n",
      "- codebert.encoder.layer.4.output.dense.bias\n",
      "- codebert.encoder.layer.4.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.4.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.5.attention.self.query.weight\n",
      "- codebert.encoder.layer.5.attention.self.query.bias\n",
      "- codebert.encoder.layer.5.attention.self.key.weight\n",
      "- codebert.encoder.layer.5.attention.self.key.bias\n",
      "- codebert.encoder.layer.5.attention.self.value.weight\n",
      "- codebert.encoder.layer.5.attention.self.value.bias\n",
      "- codebert.encoder.layer.5.attention.output.dense.weight\n",
      "- codebert.encoder.layer.5.attention.output.dense.bias\n",
      "- codebert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.5.intermediate.dense.weight\n",
      "- codebert.encoder.layer.5.intermediate.dense.bias\n",
      "- codebert.encoder.layer.5.output.dense.weight\n",
      "- codebert.encoder.layer.5.output.dense.bias\n",
      "- codebert.encoder.layer.5.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.5.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.6.attention.self.query.weight\n",
      "- codebert.encoder.layer.6.attention.self.query.bias\n",
      "- codebert.encoder.layer.6.attention.self.key.weight\n",
      "- codebert.encoder.layer.6.attention.self.key.bias\n",
      "- codebert.encoder.layer.6.attention.self.value.weight\n",
      "- codebert.encoder.layer.6.attention.self.value.bias\n",
      "- codebert.encoder.layer.6.attention.output.dense.weight\n",
      "- codebert.encoder.layer.6.attention.output.dense.bias\n",
      "- codebert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.6.intermediate.dense.weight\n",
      "- codebert.encoder.layer.6.intermediate.dense.bias\n",
      "- codebert.encoder.layer.6.output.dense.weight\n",
      "- codebert.encoder.layer.6.output.dense.bias\n",
      "- codebert.encoder.layer.6.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.6.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.7.attention.self.query.weight\n",
      "- codebert.encoder.layer.7.attention.self.query.bias\n",
      "- codebert.encoder.layer.7.attention.self.key.weight\n",
      "- codebert.encoder.layer.7.attention.self.key.bias\n",
      "- codebert.encoder.layer.7.attention.self.value.weight\n",
      "- codebert.encoder.layer.7.attention.self.value.bias\n",
      "- codebert.encoder.layer.7.attention.output.dense.weight\n",
      "- codebert.encoder.layer.7.attention.output.dense.bias\n",
      "- codebert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.7.intermediate.dense.weight\n",
      "- codebert.encoder.layer.7.intermediate.dense.bias\n",
      "- codebert.encoder.layer.7.output.dense.weight\n",
      "- codebert.encoder.layer.7.output.dense.bias\n",
      "- codebert.encoder.layer.7.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.7.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.8.attention.self.query.weight\n",
      "- codebert.encoder.layer.8.attention.self.query.bias\n",
      "- codebert.encoder.layer.8.attention.self.key.weight\n",
      "- codebert.encoder.layer.8.attention.self.key.bias\n",
      "- codebert.encoder.layer.8.attention.self.value.weight\n",
      "- codebert.encoder.layer.8.attention.self.value.bias\n",
      "- codebert.encoder.layer.8.attention.output.dense.weight\n",
      "- codebert.encoder.layer.8.attention.output.dense.bias\n",
      "- codebert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.8.intermediate.dense.weight\n",
      "- codebert.encoder.layer.8.intermediate.dense.bias\n",
      "- codebert.encoder.layer.8.output.dense.weight\n",
      "- codebert.encoder.layer.8.output.dense.bias\n",
      "- codebert.encoder.layer.8.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.8.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.9.attention.self.query.weight\n",
      "- codebert.encoder.layer.9.attention.self.query.bias\n",
      "- codebert.encoder.layer.9.attention.self.key.weight\n",
      "- codebert.encoder.layer.9.attention.self.key.bias\n",
      "- codebert.encoder.layer.9.attention.self.value.weight\n",
      "- codebert.encoder.layer.9.attention.self.value.bias\n",
      "- codebert.encoder.layer.9.attention.output.dense.weight\n",
      "- codebert.encoder.layer.9.attention.output.dense.bias\n",
      "- codebert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.9.intermediate.dense.weight\n",
      "- codebert.encoder.layer.9.intermediate.dense.bias\n",
      "- codebert.encoder.layer.9.output.dense.weight\n",
      "- codebert.encoder.layer.9.output.dense.bias\n",
      "- codebert.encoder.layer.9.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.9.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.10.attention.self.query.weight\n",
      "- codebert.encoder.layer.10.attention.self.query.bias\n",
      "- codebert.encoder.layer.10.attention.self.key.weight\n",
      "- codebert.encoder.layer.10.attention.self.key.bias\n",
      "- codebert.encoder.layer.10.attention.self.value.weight\n",
      "- codebert.encoder.layer.10.attention.self.value.bias\n",
      "- codebert.encoder.layer.10.attention.output.dense.weight\n",
      "- codebert.encoder.layer.10.attention.output.dense.bias\n",
      "- codebert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.10.intermediate.dense.weight\n",
      "- codebert.encoder.layer.10.intermediate.dense.bias\n",
      "- codebert.encoder.layer.10.output.dense.weight\n",
      "- codebert.encoder.layer.10.output.dense.bias\n",
      "- codebert.encoder.layer.10.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.10.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.11.attention.self.query.weight\n",
      "- codebert.encoder.layer.11.attention.self.query.bias\n",
      "- codebert.encoder.layer.11.attention.self.key.weight\n",
      "- codebert.encoder.layer.11.attention.self.key.bias\n",
      "- codebert.encoder.layer.11.attention.self.value.weight\n",
      "- codebert.encoder.layer.11.attention.self.value.bias\n",
      "- codebert.encoder.layer.11.attention.output.dense.weight\n",
      "- codebert.encoder.layer.11.attention.output.dense.bias\n",
      "- codebert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.11.intermediate.dense.weight\n",
      "- codebert.encoder.layer.11.intermediate.dense.bias\n",
      "- codebert.encoder.layer.11.output.dense.weight\n",
      "- codebert.encoder.layer.11.output.dense.bias\n",
      "- codebert.encoder.layer.11.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.11.output.LayerNorm.bias\n",
      "- codebert.pooler.dense.weight\n",
      "- codebert.pooler.dense.bias\n",
      "- transformer.transformer.self_attn.in_proj_weight\n",
      "- transformer.transformer.self_attn.in_proj_bias\n",
      "- transformer.transformer.self_attn.out_proj.weight\n",
      "- transformer.transformer.self_attn.out_proj.bias\n",
      "- transformer.transformer.linear1.weight\n",
      "- transformer.transformer.linear1.bias\n",
      "- transformer.transformer.linear2.weight\n",
      "- transformer.transformer.linear2.bias\n",
      "- transformer.transformer.norm1.weight\n",
      "- transformer.transformer.norm1.bias\n",
      "- transformer.transformer.norm2.weight\n",
      "- transformer.transformer.norm2.bias\n",
      "- transformer.layer_norm.weight\n",
      "- transformer.layer_norm.bias\n",
      "- generator.main.0.weight\n",
      "- generator.main.0.bias\n",
      "- generator.main.1.weight\n",
      "- generator.main.1.bias\n",
      "- generator.main.3.weight\n",
      "- generator.main.3.bias\n",
      "- generator.main.4.weight\n",
      "- generator.main.4.bias\n",
      "- generator.main.6.weight\n",
      "- generator.main.6.bias\n",
      "- discriminator.main.0.weight\n",
      "- discriminator.main.0.bias\n",
      "- discriminator.main.1.weight\n",
      "- discriminator.main.1.bias\n",
      "- discriminator.main.4.weight\n",
      "- discriminator.main.4.bias\n",
      "- discriminator.main.5.weight\n",
      "- discriminator.main.5.bias\n",
      "- discriminator.main.8.weight\n",
      "- discriminator.main.8.bias\n",
      "- decoder.pos_encoder\n",
      "- decoder.decoder.layers.0.self_attn.in_proj_weight\n",
      "- decoder.decoder.layers.0.self_attn.in_proj_bias\n",
      "- decoder.decoder.layers.0.self_attn.out_proj.weight\n",
      "- decoder.decoder.layers.0.self_attn.out_proj.bias\n",
      "- decoder.decoder.layers.0.multihead_attn.in_proj_weight\n",
      "- decoder.decoder.layers.0.multihead_attn.in_proj_bias\n",
      "- decoder.decoder.layers.0.multihead_attn.out_proj.weight\n",
      "- decoder.decoder.layers.0.multihead_attn.out_proj.bias\n",
      "- decoder.decoder.layers.0.linear1.weight\n",
      "- decoder.decoder.layers.0.linear1.bias\n",
      "- decoder.decoder.layers.0.linear2.weight\n",
      "- decoder.decoder.layers.0.linear2.bias\n",
      "- decoder.decoder.layers.0.norm1.weight\n",
      "- decoder.decoder.layers.0.norm1.bias\n",
      "- decoder.decoder.layers.0.norm2.weight\n",
      "- decoder.decoder.layers.0.norm2.bias\n",
      "- decoder.decoder.layers.0.norm3.weight\n",
      "- decoder.decoder.layers.0.norm3.bias\n",
      "- decoder.decoder.layers.1.self_attn.in_proj_weight\n",
      "- decoder.decoder.layers.1.self_attn.in_proj_bias\n",
      "- decoder.decoder.layers.1.self_attn.out_proj.weight\n",
      "- decoder.decoder.layers.1.self_attn.out_proj.bias\n",
      "- decoder.decoder.layers.1.multihead_attn.in_proj_weight\n",
      "- decoder.decoder.layers.1.multihead_attn.in_proj_bias\n",
      "- decoder.decoder.layers.1.multihead_attn.out_proj.weight\n",
      "- decoder.decoder.layers.1.multihead_attn.out_proj.bias\n",
      "- decoder.decoder.layers.1.linear1.weight\n",
      "- decoder.decoder.layers.1.linear1.bias\n",
      "- decoder.decoder.layers.1.linear2.weight\n",
      "- decoder.decoder.layers.1.linear2.bias\n",
      "- decoder.decoder.layers.1.norm1.weight\n",
      "- decoder.decoder.layers.1.norm1.bias\n",
      "- decoder.decoder.layers.1.norm2.weight\n",
      "- decoder.decoder.layers.1.norm2.bias\n",
      "- decoder.decoder.layers.1.norm3.weight\n",
      "- decoder.decoder.layers.1.norm3.bias\n",
      "- decoder.decoder.layers.2.self_attn.in_proj_weight\n",
      "- decoder.decoder.layers.2.self_attn.in_proj_bias\n",
      "- decoder.decoder.layers.2.self_attn.out_proj.weight\n",
      "- decoder.decoder.layers.2.self_attn.out_proj.bias\n",
      "- decoder.decoder.layers.2.multihead_attn.in_proj_weight\n",
      "- decoder.decoder.layers.2.multihead_attn.in_proj_bias\n",
      "- decoder.decoder.layers.2.multihead_attn.out_proj.weight\n",
      "- decoder.decoder.layers.2.multihead_attn.out_proj.bias\n",
      "- decoder.decoder.layers.2.linear1.weight\n",
      "- decoder.decoder.layers.2.linear1.bias\n",
      "- decoder.decoder.layers.2.linear2.weight\n",
      "- decoder.decoder.layers.2.linear2.bias\n",
      "- decoder.decoder.layers.2.norm1.weight\n",
      "- decoder.decoder.layers.2.norm1.bias\n",
      "- decoder.decoder.layers.2.norm2.weight\n",
      "- decoder.decoder.layers.2.norm2.bias\n",
      "- decoder.decoder.layers.2.norm3.weight\n",
      "- decoder.decoder.layers.2.norm3.bias\n",
      "- decoder.embedding.weight\n",
      "- decoder.output_layer.weight\n",
      "- decoder.output_layer.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel State Dict:\")\n",
    "for key in model.state_dict().keys():\n",
    "    print(f\"- {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a484d444-0ca8-4797-8cb4-bc6d43761d32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter Shapes:\n",
      "- codebert.embeddings.word_embeddings.weight: torch.Size([50265, 768])\n",
      "- codebert.embeddings.position_embeddings.weight: torch.Size([514, 768])\n",
      "- codebert.embeddings.token_type_embeddings.weight: torch.Size([1, 768])\n",
      "- codebert.embeddings.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.embeddings.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.0.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.0.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.0.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.1.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.1.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.1.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.2.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.2.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.2.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.3.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.3.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.3.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.4.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.4.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.4.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.5.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.5.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.5.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.6.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.6.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.6.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.7.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.7.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.7.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.8.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.8.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.8.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.9.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.9.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.9.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.10.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.10.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.10.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.11.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.11.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.11.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.pooler.dense.weight: torch.Size([768, 768])\n",
      "- codebert.pooler.dense.bias: torch.Size([768])\n",
      "- transformer.transformer.self_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- transformer.transformer.self_attn.in_proj_bias: torch.Size([2304])\n",
      "- transformer.transformer.self_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- transformer.transformer.self_attn.out_proj.bias: torch.Size([768])\n",
      "- transformer.transformer.linear1.weight: torch.Size([768, 768])\n",
      "- transformer.transformer.linear1.bias: torch.Size([768])\n",
      "- transformer.transformer.linear2.weight: torch.Size([768, 768])\n",
      "- transformer.transformer.linear2.bias: torch.Size([768])\n",
      "- transformer.transformer.norm1.weight: torch.Size([768])\n",
      "- transformer.transformer.norm1.bias: torch.Size([768])\n",
      "- transformer.transformer.norm2.weight: torch.Size([768])\n",
      "- transformer.transformer.norm2.bias: torch.Size([768])\n",
      "- transformer.layer_norm.weight: torch.Size([768])\n",
      "- transformer.layer_norm.bias: torch.Size([768])\n",
      "- generator.main.0.weight: torch.Size([512, 768])\n",
      "- generator.main.0.bias: torch.Size([512])\n",
      "- generator.main.1.weight: torch.Size([512])\n",
      "- generator.main.1.bias: torch.Size([512])\n",
      "- generator.main.3.weight: torch.Size([256, 512])\n",
      "- generator.main.3.bias: torch.Size([256])\n",
      "- generator.main.4.weight: torch.Size([256])\n",
      "- generator.main.4.bias: torch.Size([256])\n",
      "- generator.main.6.weight: torch.Size([768, 256])\n",
      "- generator.main.6.bias: torch.Size([768])\n",
      "- discriminator.main.0.weight: torch.Size([512, 768])\n",
      "- discriminator.main.0.bias: torch.Size([512])\n",
      "- discriminator.main.1.weight: torch.Size([512])\n",
      "- discriminator.main.1.bias: torch.Size([512])\n",
      "- discriminator.main.4.weight: torch.Size([256, 512])\n",
      "- discriminator.main.4.bias: torch.Size([256])\n",
      "- discriminator.main.5.weight: torch.Size([256])\n",
      "- discriminator.main.5.bias: torch.Size([256])\n",
      "- discriminator.main.8.weight: torch.Size([1, 256])\n",
      "- discriminator.main.8.bias: torch.Size([1])\n",
      "- decoder.pos_encoder: torch.Size([1, 512, 768])\n",
      "- decoder.decoder.layers.0.self_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.0.self_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.0.self_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.0.multihead_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.0.multihead_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.0.multihead_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.0.multihead_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.0.linear1.weight: torch.Size([1536, 768])\n",
      "- decoder.decoder.layers.0.linear1.bias: torch.Size([1536])\n",
      "- decoder.decoder.layers.0.linear2.weight: torch.Size([768, 1536])\n",
      "- decoder.decoder.layers.0.linear2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm1.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm1.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm2.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm3.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm3.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.self_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.1.self_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.1.self_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.multihead_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.1.multihead_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.1.multihead_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.1.multihead_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.linear1.weight: torch.Size([1536, 768])\n",
      "- decoder.decoder.layers.1.linear1.bias: torch.Size([1536])\n",
      "- decoder.decoder.layers.1.linear2.weight: torch.Size([768, 1536])\n",
      "- decoder.decoder.layers.1.linear2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm1.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm1.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm2.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm3.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm3.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.self_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.2.self_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.2.self_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.multihead_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.2.multihead_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.2.multihead_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.2.multihead_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.linear1.weight: torch.Size([1536, 768])\n",
      "- decoder.decoder.layers.2.linear1.bias: torch.Size([1536])\n",
      "- decoder.decoder.layers.2.linear2.weight: torch.Size([768, 1536])\n",
      "- decoder.decoder.layers.2.linear2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm1.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm1.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm2.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm3.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm3.bias: torch.Size([768])\n",
      "- decoder.embedding.weight: torch.Size([50000, 768])\n",
      "- decoder.output_layer.weight: torch.Size([50000, 768])\n",
      "- decoder.output_layer.bias: torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nParameter Shapes:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"- {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4c1baab-ba74-4351-ad40-73e789233402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Configuration:\n",
      "- d_model: N/A\n",
      "- vocab_size: 50000\n",
      "- max_length: 512\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"- d_model: {model.d_model if hasattr(model, 'd_model') else 'N/A'}\")\n",
    "print(f\"- vocab_size: {model.decoder.vocab_size if hasattr(model, 'decoder') else 'N/A'}\")\n",
    "print(f\"- max_length: {model.decoder.max_length if hasattr(model, 'decoder') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7fe1a1c-e84e-4c4c-a0dd-584d27020773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generator Architecture:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SmartContractVulnerabilityGAN' object has no attribute 'CodeDecoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m'\u001b[39m\u001b[33mgenerator\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerator Architecture:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCodeDecoder\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1687\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SmartContractVulnerabilityGAN' object has no attribute 'CodeDecoder'"
     ]
    }
   ],
   "source": [
    "if hasattr(model, 'generator'):\n",
    "    print(\"\\nGenerator Architecture:\")\n",
    "    print(model.CodeDecoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeeee1b-7d47-43c7-b359-156adc04026c",
   "metadata": {},
   "source": [
    "### This is a GAN (Generative Adversarial Network) combined with a Transformer architecture for smart contract vulnerability detection.\n",
    "\n",
    "#### Here's the technical breakdown:\n",
    "#### 1. Architecture Components:\n",
    "-Transformer Encoder: Processes smart contract code using self-attention\n",
    "-Generator: Creates synthetic vulnerable code patterns\n",
    "-Discriminator: Distinguishes between real and synthetic vulnerabilities\n",
    "\n",
    "#### 2. Input Processing:\n",
    "-Takes smart contract code and its AST (Abstract Syntax Tree) paths\n",
    "-Uses CodeBERT to generate embeddings (768-dimensional vectors)\n",
    "-Processes both contract code and path information\n",
    "\n",
    "#### 3. Training Process:\n",
    "3.1. Generator Training:\n",
    "-Takes random noise and contract embeddings\n",
    "-Generates synthetic vulnerable code patterns\n",
    "-Tries to fool the discriminator\n",
    "\n",
    "3.2. Discriminator Training:\n",
    "-Takes real contract embeddings and generator outputs\n",
    "-Learns to distinguish real from synthetic vulnerabilities\n",
    "-Uses binary classification (real/fake)\n",
    "\n",
    "#### 4. Output:\n",
    "-Vulnerability scores for input contracts\n",
    "-Synthetic vulnerable code patterns for training\n",
    "-Binary classification of real vs. synthetic vulnerabilities\n",
    "\n",
    "#### The model essentially learns to:\n",
    "-Understand code patterns through the transformer\n",
    "-Generate realistic vulnerable code examples\n",
    "-Detect vulnerabilities in real contracts\n",
    "-Improve detection through adversarial training\n",
    "\n",
    "#### This approach combines the strengths of:\n",
    "Transformers for code understanding\n",
    "GANs for synthetic data generation\n",
    "Binary classification for vulnerability detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7bafd2-5e74-4e56-ad5f-2de3d7c88431",
   "metadata": {},
   "source": [
    "# 3. Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e828df90-6702-4081-b06f-b65d2d9a47b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint type: <class 'dict'>\n",
      "Loaded model from model_state_dict\n",
      "Model loaded successfully\n",
      "Using device: cuda\n",
      "Vulnerability Score: 0.731036365032196\n",
      "Is Vulnerable: True\n",
      "\n",
      "Synthetic Vulnerable Code:\n",
      "<s>pragma <pad><pad>\n",
      "\n",
      "Generated Synthetic Code:\n",
      "\n",
      "Sample 1:\n",
      "<s>/** tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem tradem\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "class VulnerabilityPredictor:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"\n",
    "        Initialize the vulnerability predictor\n",
    "        Args:\n",
    "            model_path: Path to the trained model checkpoint\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load model and tokenizer\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            # Load the checkpoint\n",
    "            checkpoint = torch.load(model_path, map_location=self.device)\n",
    "            print(f\"Loaded checkpoint type: {type(checkpoint)}\")\n",
    "            \n",
    "            # Initialize the model architecture\n",
    "            self.model = SmartContractVulnerabilityGAN(\n",
    "                d_model=768,  # CodeBERT's hidden size\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Load the state dict\n",
    "            if isinstance(checkpoint, dict):\n",
    "                if 'model_state_dict' in checkpoint:\n",
    "                    self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    print(\"Loaded model from model_state_dict\")\n",
    "                else:\n",
    "                    self.model.load_state_dict(checkpoint)\n",
    "                    print(\"Loaded model from checkpoint dict\")\n",
    "            else:\n",
    "                self.model = checkpoint\n",
    "                print(\"Loaded full model from checkpoint\")\n",
    "                \n",
    "            self.model.eval()\n",
    "            \n",
    "            # Initialize CodeBERT\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "            self.codebert = AutoModel.from_pretrained(\"microsoft/codebert-base\").to(self.device)\n",
    "            \n",
    "            # Get decoder from model\n",
    "            self.decoder = self.model.decoder\n",
    "            \n",
    "            print(\"Model loaded successfully\")\n",
    "            print(f\"Using device: {self.device}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def preprocess_contract(self, contract_code):\n",
    "        \"\"\"\n",
    "        Preprocess contract code for model input\n",
    "        \"\"\"\n",
    "        # Tokenize contract\n",
    "        tokens = self.tokenizer(\n",
    "            contract_code,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_ids = tokens['input_ids'].to(self.device)\n",
    "        attention_mask = tokens['attention_mask'].to(self.device)\n",
    "        \n",
    "        return input_ids, attention_mask\n",
    "    \n",
    "    def predict_vulnerability(self, contract_code, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict if a contract is vulnerable and generate synthetic vulnerable code\n",
    "        Returns: (is_vulnerable, vulnerability_score, synthetic_code)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                # Preprocess contract\n",
    "                input_ids, attention_mask = self.preprocess_contract(contract_code)\n",
    "                \n",
    "                # Get model prediction\n",
    "                outputs = self.model(\n",
    "                    contract_ids=input_ids,\n",
    "                    path_ids=input_ids,\n",
    "                    contract_attention_mask=attention_mask,\n",
    "                    path_attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                # Get vulnerability score\n",
    "                real_scores = outputs['real_scores']\n",
    "                vulnerability_score = torch.sigmoid(real_scores).item()\n",
    "                is_vulnerable = vulnerability_score > threshold\n",
    "                \n",
    "                # Generate synthetic vulnerable code if contract is vulnerable\n",
    "                synthetic_code = None\n",
    "                if is_vulnerable:\n",
    "                    try:\n",
    "                        # Get synthetic embeddings from generator\n",
    "                        synthetic_embeddings = outputs['synthetic']\n",
    "                        \n",
    "                        # Decode synthetic embeddings to code\n",
    "                        decoded_code = outputs['decoded_code']\n",
    "                        tokens = torch.argmax(decoded_code, dim=-1)\n",
    "                        \n",
    "                        # Convert tokens to code\n",
    "                        synthetic_code = self.tokenizer.decode(tokens[0])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Failed to generate synthetic code: {str(e)}\")\n",
    "                        synthetic_code = None\n",
    "                \n",
    "                return is_vulnerable, vulnerability_score, synthetic_code\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in vulnerability prediction: {str(e)}\")\n",
    "                return False, 0.0, None\n",
    "    \n",
    "    def analyze_vulnerability(self, contract_code):\n",
    "        \"\"\"\n",
    "        Comprehensive vulnerability analysis\n",
    "        Returns: Dictionary containing vulnerability analysis results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            is_vulnerable, score, synthetic_code = self.predict_vulnerability(contract_code)\n",
    "            \n",
    "            analysis = {\n",
    "                'is_vulnerable': is_vulnerable,\n",
    "                'vulnerability_score': score,\n",
    "                'synthetic_vulnerable_code': synthetic_code if is_vulnerable else None,\n",
    "                'original_code': contract_code,\n",
    "                'analysis_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            return analysis\n",
    "        except Exception as e:\n",
    "            print(f\"Error in vulnerability analysis: {str(e)}\")\n",
    "            return {\n",
    "                'is_vulnerable': False,\n",
    "                'vulnerability_score': 0.0,\n",
    "                'synthetic_vulnerable_code': None,\n",
    "                'original_code': contract_code,\n",
    "                'analysis_timestamp': datetime.now().isoformat(),\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def generate_synthetic_vulnerable_code(self, num_samples=1):\n",
    "        \"\"\"\n",
    "        Generate synthetic vulnerable code samples\n",
    "        Args:\n",
    "            num_samples: Number of samples to generate\n",
    "        Returns: List of generated code samples\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                samples = []\n",
    "                for _ in range(num_samples):\n",
    "                    # Generate code using model's generate_code method\n",
    "                    code = self.model.generate_code(num_samples=1)\n",
    "                    samples.append(code)\n",
    "                \n",
    "                return samples\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating synthetic code: {str(e)}\")\n",
    "                return []\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize predictor\n",
    "    predictor = VulnerabilityPredictor('final_model_v3.pt')\n",
    "    \n",
    "    # Example contract\n",
    "    contract_code = \"\"\"\n",
    "    pragma solidity ^0.8.0;\n",
    "    contract Example {\n",
    "        uint256 private value;\n",
    "        \n",
    "        function setValue(uint256 _value) public {\n",
    "            value = _value;\n",
    "        }\n",
    "        \n",
    "        function getValue() public view returns (uint256) {\n",
    "            return value;\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get comprehensive analysis\n",
    "    analysis = predictor.analyze_vulnerability(contract_code)\n",
    "    print(f\"Vulnerability Score: {analysis['vulnerability_score']}\")\n",
    "    print(f\"Is Vulnerable: {analysis['is_vulnerable']}\")\n",
    "    if analysis['synthetic_vulnerable_code']:\n",
    "        print(\"\\nSynthetic Vulnerable Code:\")\n",
    "        print(analysis['synthetic_vulnerable_code'])\n",
    "    \n",
    "    # Generate synthetic vulnerable code\n",
    "    synthetic_samples = predictor.generate_synthetic_vulnerable_code(num_samples=1)\n",
    "    print(\"\\nGenerated Synthetic Code:\")\n",
    "    for i, code in enumerate(synthetic_samples, 1):\n",
    "        print(f\"\\nSample {i}:\")\n",
    "        print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a39804-39cc-434c-9387-0aaffb46e1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "debdb980-c052-4826-afa0-174a01046d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contract input shape: torch.Size([1, 146])\n",
      "Path input shape: torch.Size([1, 146])\n",
      "Vulnerability Score: 0.0000\n",
      "\n",
      "Generated Code:\n",
      "<s>/** tradem                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n"
     ]
    }
   ],
   "source": [
    "def analyze_contract(model, contract_code):\n",
    "    # First, let's prepare the input\n",
    "    inputs = model.tokenizer(\n",
    "        contract_code,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Create path inputs with the same length as contract inputs\n",
    "    path_inputs = model.tokenizer(\n",
    "        \"dummy path\" * 100,  # Make it longer to match contract length\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=inputs['input_ids'].size(1)  # Match contract length\n",
    "    )\n",
    "    path_inputs = {k: v.to(device) for k, v in path_inputs.items()}\n",
    "    \n",
    "    # Print shapes for debugging\n",
    "    print(f\"Contract input shape: {inputs['input_ids'].shape}\")\n",
    "    print(f\"Path input shape: {path_inputs['input_ids'].shape}\")\n",
    "    \n",
    "    # Get the model outputs\n",
    "    with torch.no_grad():\n",
    "        # Get CodeBERT embeddings for contract\n",
    "        contract_outputs = model.codebert(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask']\n",
    "        )\n",
    "        contract_embeddings = contract_outputs.last_hidden_state\n",
    "        \n",
    "        # Get CodeBERT embeddings for paths\n",
    "        path_outputs = model.codebert(\n",
    "            input_ids=path_inputs['input_ids'],\n",
    "            attention_mask=path_inputs['attention_mask']\n",
    "        )\n",
    "        path_embeddings = path_outputs.last_hidden_state\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined_embeddings = contract_embeddings + path_embeddings\n",
    "        \n",
    "        # Process through transformer\n",
    "        transformed = model.transformer(combined_embeddings)\n",
    "        \n",
    "        # Get mean representation\n",
    "        mean_embeddings = transformed.mean(dim=1)\n",
    "        \n",
    "        # Get vulnerability score from discriminator\n",
    "        vulnerability_score = model.discriminator(mean_embeddings).item()\n",
    "        \n",
    "        # Generate synthetic code\n",
    "        synthetic_embeddings = model.generator(mean_embeddings)\n",
    "        decoded_code = model.decoder(synthetic_embeddings, transformed)\n",
    "        tokens = torch.argmax(decoded_code, dim=-1)\n",
    "        generated_code = model.tokenizer.decode(tokens[0])\n",
    "    \n",
    "    return {\n",
    "        'vulnerability_score': vulnerability_score,\n",
    "        'generated_code': generated_code\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "contract_code = \"\"\"\n",
    "pragma solidity ^0.8.0;\n",
    "\n",
    "contract Example {\n",
    "    mapping(address => uint) private balances;\n",
    "    \n",
    "    function deposit() public payable {\n",
    "        balances[msg.sender] += msg.value;\n",
    "    }\n",
    "    \n",
    "    function withdraw(uint amount) public {\n",
    "        require(balances[msg.sender] >= amount);\n",
    "        balances[msg.sender] -= amount;\n",
    "        payable(msg.sender).transfer(amount);\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Make sure the model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Get the analysis\n",
    "results = analyze_contract(model, contract_code)\n",
    "\n",
    "print(f\"Vulnerability Score: {results['vulnerability_score']:.4f}\")\n",
    "print(\"\\nGenerated Code:\")\n",
    "print(results['generated_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc40eb9e-2043-4a43-a301-87db5be15d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71c6de-dd38-4a48-9d16-dc9fa7b1966e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
