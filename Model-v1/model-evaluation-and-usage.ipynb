{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3acb4f-edae-4342-9a26-7f8bf3a57e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv pip install torch transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1898cf2f-ccf6-4cc8-8aec-c2ce42f7b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Model imports\n",
    "from model import SmartContractTransformer\n",
    "\n",
    "# Training imports\n",
    "from train import SmartContractTrainer\n",
    "\n",
    "# Data processing imports\n",
    "from data_processing import SmartContractDataset, preprocess_contract\n",
    "\n",
    "# Optional but useful imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # for progress bars\n",
    "import logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d5051a-5c5f-46fc-b3d6-25b687eb0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "Number of GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c41c22-96f2-4183-900e-0569d247a989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since jainabh/smart_contracts_malicious couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/m20180848/.cache/huggingface/datasets/jainabh___smart_contracts_malicious/default/0.0.0/ba1b2cb9d02f16e398b6e50f59db70c5c3cb1b25 (last modified on Sat May  3 17:02:56 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"jainabh/smart_contracts_malicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c698aa2-b74e-4a7e-b83e-0a7da5cdd333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['contract_source', 'malicious'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d24547d-35e4-4991-b50e-21e01c7fe09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "train_contracts = ds['train'][0:1400]['contract_source']  # Changed from 'contract_source' to 'source_code'\n",
    "train_labels = ds['train'][0:1400]['malicious']\n",
    "val_contracts = ds['train'][1400:-1]['contract_source']  # Changed from 'contract_source' to 'source_code'\n",
    "val_labels = ds['train'][1400:-1]['malicious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c68e47-73e6-4643-8734-6305337869a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "\n",
    "def parse_solidity_to_ast(code: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse Solidity code into a simplified AST structure\n",
    "    \"\"\"\n",
    "    def extract_contract_info(code: str) -> Dict[str, Any]:\n",
    "        # Extract contract name\n",
    "        contract_match = re.search(r'contract\\s+(\\w+)', code)\n",
    "        contract_name = contract_match.group(1) if contract_match else \"Unknown\"\n",
    "        \n",
    "        # Extract functions\n",
    "        functions = []\n",
    "        function_pattern = r'function\\s+(\\w+)\\s*\\(([^)]*)\\)\\s*(?:public|private|internal|external)?\\s*(?:view|pure|payable)?\\s*(?:returns\\s*\\(([^)]*)\\))?\\s*{'\n",
    "        for match in re.finditer(function_pattern, code):\n",
    "            func_name = match.group(1)\n",
    "            params = match.group(2).split(',') if match.group(2) else []\n",
    "            returns = match.group(3).split(',') if match.group(3) else []\n",
    "            \n",
    "            functions.append({\n",
    "                'name': func_name,\n",
    "                'parameters': [p.strip() for p in params],\n",
    "                'returns': [r.strip() for r in returns]\n",
    "            })\n",
    "        \n",
    "        # Extract state variables\n",
    "        variables = []\n",
    "        var_pattern = r'(?:uint|address|string|bool|mapping)\\s+(?:\\w+)\\s+(\\w+)'\n",
    "        for match in re.finditer(var_pattern, code):\n",
    "            variables.append(match.group(1))\n",
    "        \n",
    "        return {\n",
    "            'type': 'Contract',\n",
    "            'name': contract_name,\n",
    "            'functions': functions,\n",
    "            'variables': variables\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Clean the code\n",
    "        code = re.sub(r'//.*?\\n|/\\*.*?\\*/', '', code)  # Remove comments\n",
    "        code = re.sub(r'\\s+', ' ', code)  # Normalize whitespace\n",
    "        \n",
    "        # Parse the code\n",
    "        ast = extract_contract_info(code)\n",
    "        return ast\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing code: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def prepare_code2vec_input(ast: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert AST to code2vec input format\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    \n",
    "    def extract_paths(node: Dict[str, Any], current_path: List[str] = None):\n",
    "        if current_path is None:\n",
    "            current_path = []\n",
    "            \n",
    "        # Add current node to path\n",
    "        if 'name' in node:\n",
    "            current_path.append(node['name'])\n",
    "            \n",
    "        # Process functions\n",
    "        if 'functions' in node:\n",
    "            for func in node['functions']:\n",
    "                func_path = current_path + [func['name']]\n",
    "                paths.append(' '.join(func_path))\n",
    "                \n",
    "                # Add parameter paths\n",
    "                for param in func['parameters']:\n",
    "                    param_path = func_path + [param]\n",
    "                    paths.append(' '.join(param_path))\n",
    "                \n",
    "                # Add return paths\n",
    "                for ret in func['returns']:\n",
    "                    ret_path = func_path + [ret]\n",
    "                    paths.append(' '.join(ret_path))\n",
    "        \n",
    "        # Process variables\n",
    "        if 'variables' in node:\n",
    "            for var in node['variables']:\n",
    "                var_path = current_path + [var]\n",
    "                paths.append(' '.join(var_path))\n",
    "    \n",
    "    extract_paths(ast)\n",
    "    return paths\n",
    "\n",
    "# Example usage:\n",
    "def process_contract_for_code2vec(code: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process a Solidity contract for code2vec\n",
    "    \"\"\"\n",
    "    # Parse code to AST\n",
    "    ast = parse_solidity_to_ast(code)\n",
    "    if ast is None:\n",
    "        return []\n",
    "    \n",
    "    # Convert AST to code2vec input format\n",
    "    paths = prepare_code2vec_input(ast)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ddce1da-3c4c-4af7-8f2d-fc2ca8da0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, AutoModel  # Add these imports\n",
    "\n",
    "class SmartContractDatasetWithPaths(Dataset):\n",
    "    def __init__(self, contracts, labels, tokenizer, code2vec_model, max_length=256):\n",
    "        self.contracts = contracts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.code2vec_model = code2vec_model\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.contracts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        contract = self.contracts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Parse contract to AST and generate paths\n",
    "        ast = parse_solidity_to_ast(contract)\n",
    "        paths = prepare_code2vec_input(ast)\n",
    "        \n",
    "        # Convert paths to string for tokenization\n",
    "        paths_str = ' '.join([''.join(path) for path in paths])\n",
    "        \n",
    "        # Tokenize contract with consistent max_length\n",
    "        contract_inputs = self.tokenizer(\n",
    "            contract,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize paths with consistent max_length\n",
    "        path_inputs = self.tokenizer(\n",
    "            paths_str,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get tensors and ensure they're 1D\n",
    "        input_ids = contract_inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = contract_inputs['attention_mask'].squeeze(0).bool()  # Convert to boolean\n",
    "        path_input_ids = path_inputs['input_ids'].squeeze(0)\n",
    "        path_attention_mask = path_inputs['attention_mask'].squeeze(0).bool()  # Convert to boolean\n",
    "        \n",
    "        # Create target_ids with the same length as input_ids\n",
    "        target_ids = input_ids.clone()\n",
    "        \n",
    "        # Ensure all tensors have the same length\n",
    "        if input_ids.size(0) < self.max_length:\n",
    "            padding = torch.zeros(self.max_length - input_ids.size(0), dtype=input_ids.dtype)\n",
    "            input_ids = torch.cat([input_ids, padding])\n",
    "            attention_mask = torch.cat([attention_mask, torch.zeros(self.max_length - attention_mask.size(0), dtype=torch.bool)])  # Use boolean padding\n",
    "            target_ids = torch.cat([target_ids, padding])\n",
    "        elif input_ids.size(0) > self.max_length:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            attention_mask = attention_mask[:self.max_length]\n",
    "            target_ids = target_ids[:self.max_length]\n",
    "        \n",
    "        # Ensure path tensors have the same length\n",
    "        if path_input_ids.size(0) < self.max_length:\n",
    "            padding = torch.zeros(self.max_length - path_input_ids.size(0), dtype=path_input_ids.dtype)\n",
    "            path_input_ids = torch.cat([path_input_ids, padding])\n",
    "            path_attention_mask = torch.cat([path_attention_mask, torch.zeros(self.max_length - path_attention_mask.size(0), dtype=torch.bool)])  # Use boolean padding\n",
    "        elif path_input_ids.size(0) > self.max_length:\n",
    "            path_input_ids = path_input_ids[:self.max_length]\n",
    "            path_attention_mask = path_attention_mask[:self.max_length]\n",
    "        \n",
    "        # Ensure all tensors are 1D\n",
    "        input_ids = input_ids.view(-1)\n",
    "        attention_mask = attention_mask.view(-1)\n",
    "        path_input_ids = path_input_ids.view(-1)\n",
    "        path_attention_mask = path_attention_mask.view(-1)\n",
    "        target_ids = target_ids.view(-1)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'path_input_ids': path_input_ids,\n",
    "            'path_attention_mask': path_attention_mask,\n",
    "            'target_ids': target_ids,\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0c4cf1-34c7-4cd6-8a24-306cb9b734c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and code2vec model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "code2vec_model = AutoModel.from_pretrained('microsoft/codebert-base').cuda()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SmartContractDatasetWithPaths(\n",
    "    train_contracts, \n",
    "    train_labels,\n",
    "    tokenizer,\n",
    "    code2vec_model\n",
    ")\n",
    "\n",
    "val_dataset = SmartContractDatasetWithPaths(\n",
    "    val_contracts,\n",
    "    val_labels,\n",
    "    tokenizer,\n",
    "    code2vec_model\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c75f370-9542-40c5-b329-b5285a629b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "from train import VulnerabilityDetectionTrainer\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Create a directory for checkpoints\n",
    "checkpoint_dir = 'v3-checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = VulnerabilityDetectionTrainer(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fae40e6f-dce6-4c18-981e-c0e53f9cd6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v3-checkpoints/checkpoint_epoch_30_model_v3.pt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd835ecb-c7c1-4b26-96bc-0e4904f46bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 20\n",
      "Previous validation loss: 0.0070\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_20_model_v3.pt')  # Change this to your checkpoint file\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load model states\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "model.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "model.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "# Load optimizer states\n",
    "trainer.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "trainer.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "trainer.optimizer_decoder.load_state_dict(checkpoint['optimizer_decoder_state_dict'])\n",
    "\n",
    "# Get the epoch to start from and best validation loss\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_val_loss = checkpoint['val_loss']\n",
    "\n",
    "print(f\"Loaded checkpoint from epoch {start_epoch + 1}\")\n",
    "print(f\"Previous validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26779f5-56bc-4d32-9c6a-81ce63c676c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training epoch...\n"
     ]
    }
   ],
   "source": [
    "# Training loop - start from the next epoch\n",
    "num_epochs = 120\n",
    "\n",
    "for epoch in range(start_epoch + 1, num_epochs):  # Start from the next epoch\n",
    "    # Start timer for this epoch\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    g_loss, d_loss, decoder_loss = trainer.train_epoch()\n",
    "    val_loss = trainer.validate()\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Print training progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Generator Loss: {g_loss:.4f}\")\n",
    "    print(f\"Discriminator Loss: {d_loss:.4f}\")\n",
    "    print(f\"Decoder Loss: {decoder_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            # Model states\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'generator_state_dict': model.generator.state_dict(),\n",
    "            'discriminator_state_dict': model.discriminator.state_dict(),\n",
    "            'decoder_state_dict': model.decoder.state_dict(),\n",
    "            \n",
    "            # Optimizer states\n",
    "            'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "            'optimizer_decoder_state_dict': trainer.optimizer_decoder.state_dict(),\n",
    "            \n",
    "            # Loss values\n",
    "            'g_loss': g_loss,\n",
    "            'd_loss': d_loss,\n",
    "            'decoder_loss': decoder_loss,\n",
    "            'val_loss': val_loss,\n",
    "            \n",
    "            # Model configuration\n",
    "            'model_config': {\n",
    "                'vocab_size': model.decoder.vocab_size,\n",
    "                'max_length': model.decoder.max_length\n",
    "            },\n",
    "            \n",
    "            # Training metadata\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'epoch_time': epoch_time\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}_model_v3.pt')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Saved checkpoint for epoch {epoch+1}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = os.path.join(checkpoint_dir, 'best_model_v3.pt')\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9277d-d896-44c6-af23-db7512b8f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1088dc1-ee3e-457b-980f-cbfa67896dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6116b2b-55dc-4842-8794-88283e9e2acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ca8665-9d47-4383-8fbf-ec85abb2ba8d",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "\n",
    "1. Input Processing:\n",
    "Initial input: [32, 512] (batch_size=32, sequence_length=512)\n",
    "After embedding: [32, 512, 512] (batch_size=32, sequence_length=512, embedding_dim=512)\n",
    "This is correct because the embedding layer converts each token to a 512-dimensional vector\n",
    "\n",
    "2. Path Embeddings Processing:\n",
    "Initial path embeddings: [32, 768] (batch_size=32, code2vec_dim=768)\n",
    "After path embedding layer: [32, 512] (batch_size=32, transformer_dim=512)\n",
    "The linear layer converts from code2vec's 768 dimensions to transformer's 512 dimensions\n",
    "After expansion: [32, 512, 512] (batch_size=32, sequence_length=512, transformer_dim=512)\n",
    "The path embeddings are expanded to match the sequence length\n",
    "\n",
    "3. Final Shape:\n",
    "[32, 512, 512] (batch_size=32, sequence_length=512, transformer_dim=512)\n",
    "This is the correct shape for the transformer layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9d4bfa-25f5-4b56-b679-44fc3102049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for epoch 10\n"
     ]
    }
   ],
   "source": [
    "checkpoint = {\n",
    "    # Model states\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'generator_state_dict': model.generator.state_dict(),\n",
    "    'discriminator_state_dict': model.discriminator.state_dict(),\n",
    "    'decoder_state_dict': model.decoder.state_dict(),\n",
    "    \n",
    "    # Optimizer states\n",
    "    'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "    'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "    'optimizer_decoder_state_dict': trainer.optimizer_decoder.state_dict(),\n",
    "    \n",
    "    # Loss values\n",
    "    'g_loss': g_loss,\n",
    "    'd_loss': d_loss,\n",
    "    'decoder_loss': decoder_loss,\n",
    "    'val_loss': val_loss,\n",
    "    \n",
    "    # Model configuration\n",
    "    'model_config': {\n",
    "        #'d_model': model.d_model,\n",
    "        'vocab_size': model.decoder.vocab_size,\n",
    "        'max_length': model.decoder.max_length\n",
    "    },\n",
    "    \n",
    "    # Training metadata\n",
    "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'epoch_time': epoch_time\n",
    "}\n",
    "\n",
    "# Save regular checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}_model_v3.pt')\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Saved checkpoint for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "812eafcf-c1de-4283-905d-9943fcacffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [119/120]\n",
      "Generator Loss: 10.5703\n",
      "Discriminator Loss: 0.0062\n",
      "Validation Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "print(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "print(f\"Generator Loss: {g_loss:.4f}\")\n",
    "print(f\"Discriminator Loss: {d_loss:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7bafd2-5e74-4e56-ad5f-2de3d7c88431",
   "metadata": {},
   "source": [
    "# 1. Load Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0523c01a-e501-427d-8e76-23a25e6f5210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import SmartContractTransformer\n",
    "from train import Discriminator\n",
    "\n",
    "def load_trained_model(checkpoint_path, device='cuda:1'):\n",
    "    \"\"\"\n",
    "    Load the trained model and discriminator from checkpoint\n",
    "    \"\"\"\n",
    "    # Initialize model and discriminator\n",
    "    model = SmartContractTransformer()\n",
    "    discriminator = Discriminator()\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load state dicts\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    \n",
    "    # Move to device\n",
    "    model = model.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    \n",
    "    # Set to eval mode\n",
    "    model.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    return model, discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d10b5e58-9742-416c-9cd8-38089f60f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "checkpoint_path = 'checkpoints_v1/latest_model.pt'  # or 'best_model_epoch_X.pt'\n",
    "model_loaded, discriminator = load_trained_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590b2fc-3760-4419-9835-3263cca1620f",
   "metadata": {},
   "source": [
    "# 2. Model Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "280ba2f6-c926-4fd4-bcd9-dd6720bb44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_data = val_dataset[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d27a670-56f4-49a0-afb4-efef7be51c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_contract(contract_data, model, discriminator, tokenizer=None, device='cuda:1'):\n",
    "    \"\"\"\n",
    "    Analyze a smart contract for vulnerabilities and generate synthetic version\n",
    "    \"\"\"\n",
    "    # Move input data to device\n",
    "    input_ids = contract_data['input_ids'].unsqueeze(0).to(device)\n",
    "    attention_mask = contract_data['attention_mask'].unsqueeze(0).to(device)\n",
    "    path_input_ids = contract_data['path_input_ids'].unsqueeze(0).to(device)\n",
    "    path_attention_mask = contract_data['path_attention_mask'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Decode original contract if tokenizer is provided\n",
    "    original_text = None\n",
    "    if tokenizer is not None:\n",
    "        original_text = tokenizer.decode(input_ids[0].cpu().tolist())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get vulnerability score\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            path_input_ids=path_input_ids,\n",
    "            path_attention_mask=path_attention_mask,\n",
    "            target_ids=None\n",
    "        )\n",
    "        \n",
    "        # Get discriminator predictions\n",
    "        vuln_pred, synth_pred = discriminator(outputs['encoder_output'])\n",
    "        vulnerability_score = torch.sigmoid(vuln_pred).item()\n",
    "        synthetic_score = torch.sigmoid(synth_pred).item()\n",
    "        \n",
    "        result = {\n",
    "            'vulnerability_score': vulnerability_score,\n",
    "            'synthetic_score': synthetic_score,\n",
    "            'is_vulnerable': vulnerability_score > 0.5,\n",
    "            'is_synthetic': synthetic_score > 0.5,\n",
    "            'original_contract': {\n",
    "                'text': original_text,\n",
    "                'input_ids': input_ids[0].cpu().tolist()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Generate synthetic contract\n",
    "        synthetic_outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            path_input_ids=path_input_ids,\n",
    "            path_attention_mask=path_attention_mask,\n",
    "            target_ids=None\n",
    "        )\n",
    "        \n",
    "        # Get the generated sequence\n",
    "        generated_sequence = synthetic_outputs['generated_sequence']\n",
    "        \n",
    "        # Get discriminator predictions for synthetic contract\n",
    "        synth_vuln_pred, synth_synth_pred = discriminator(synthetic_outputs['encoder_output'])\n",
    "        synth_vulnerability_score = torch.sigmoid(synth_vuln_pred).item()\n",
    "        synth_synthetic_score = torch.sigmoid(synth_synth_pred).item()\n",
    "        \n",
    "        result['synthetic_contract'] = {\n",
    "            'sequence': generated_sequence[0].cpu().tolist(),\n",
    "            'vulnerability_score': synth_vulnerability_score,\n",
    "            'synthetic_score': synth_synthetic_score,\n",
    "            'is_vulnerable': synth_vulnerability_score > 0.5,\n",
    "            'is_synthetic': synth_synthetic_score > 0.5\n",
    "        }\n",
    "        \n",
    "        # If tokenizer is provided, decode the synthetic contract\n",
    "        if tokenizer is not None:\n",
    "            synthetic_tokens = result['synthetic_contract']['sequence']\n",
    "            result['synthetic_contract']['text'] = tokenizer.decode(synthetic_tokens)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "530e0dbd-3fe7-4191-922e-d9fd9f1cf971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:384: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contract Analysis Results:\n",
      "--------------------------------------------------\n",
      "Vulnerability Score: 0.0151\n",
      "Synthetic Score: 0.7673\n",
      "Vulnerability Status: Safe\n",
      "Synthetic Status: Synthetic\n",
      "\n",
      "Original Contract:\n",
      "--------------------------------------------------\n",
      "<s>/**\n",
      " *Submitted for verification at Etherscan.io on 2020-11-22\n",
      "*/\n",
      "\n",
      "// SPDX-License-Identifier: MIT + WTFPL\n",
      "// File: contracts/uniswapv2/interfaces/IUniswapV2Factory.sol\n",
      "\n",
      "pragma solidity >=0.5.0;\n",
      "\n",
      "interface IUniswapV2Factory {\n",
      "    event PairCreated(address indexed token0, address indexed token1, address pair, uint);\n",
      "\n",
      "    function feeTo() external view returns (address);\n",
      "    function feeToSetter() external view returns (address);\n",
      "    function migrator() external view returns (address);\n",
      "\n",
      "    function getPair(address tokenA, address tokenB) external view returns (address pair);\n",
      "    function allPairs(uint) external view returns (address pair);\n",
      "    function allPairsLength() external view returns (uint);\n",
      "\n",
      "    function createPair(address tokenA, address tokenB) external</s>\n",
      "\n",
      "Synthetic Contract Analysis:\n",
      "--------------------------------------------------\n",
      "Vulnerability Score: 0.0151\n",
      "Synthetic Score: 0.7673\n",
      "Vulnerability Status: Safe\n",
      "Synthetic Status: Synthetic\n",
      "\n",
      "Generated Synthetic Contract:\n",
      "--------------------------------------------------\n",
      "<s>/**\n",
      " *Submitted for verification at Etherscan.io on 2020-09-12\n",
      "*/\n",
      "\n",
      "pragma solidity 0.6.6;\n",
      "\n",
      "\n",
      "/**\n",
      " * @dev Wrappers over Solidity's arithmetic operations with added overflow\n",
      " * checks.\n",
      " *\n",
      " * Arithmetic operations in Solidity wrap on overflow. This can easily result\n",
      " * in bugs, because programmers usually assume that an overflow raises an\n",
      " * error, which is the standard behavior in high level programming languages.\n",
      " * `SafeMath` restores this intuition by reverting the transaction when an\n",
      " * operation overflows.\n",
      " *\n",
      " * Using this library instead of the unchecked operations eliminates an entire\n",
      " * class of bugs, so it's recommended to use it always.\n",
      " */\n",
      "library SafeMath {\n",
      "  /**\n",
      "    * @dev Returns the addition of two unsigned integers, reverting on\n",
      "    * overflow.\n",
      "    *\n",
      "    * Counterpart to Solidity's `+` operator.\n",
      "    *\n",
      "    * Requirements:\n",
      "    * - Addition cannot overflow.\n",
      "    */\n",
      "  function add(uint256 a, uint256 b</s>\n"
     ]
    }
   ],
   "source": [
    "# Analyze the contract\n",
    "results = analyze_contract(contract_data, model_loaded, discriminator, tokenizer)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nContract Analysis Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Vulnerability Score: {results['vulnerability_score']:.4f}\")\n",
    "print(f\"Synthetic Score: {results['synthetic_score']:.4f}\")\n",
    "print(f\"Vulnerability Status: {'Vulnerable' if results['is_vulnerable'] else 'Safe'}\")\n",
    "print(f\"Synthetic Status: {'Synthetic' if results['is_synthetic'] else 'Real'}\")\n",
    "\n",
    "# Print original contract\n",
    "if results['original_contract']['text']:\n",
    "    print(\"\\nOriginal Contract:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(results['original_contract']['text'])\n",
    "    \n",
    "\n",
    "print(\"\\nSynthetic Contract Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Vulnerability Score: {results['synthetic_contract']['vulnerability_score']:.4f}\")\n",
    "print(f\"Synthetic Score: {results['synthetic_contract']['synthetic_score']:.4f}\")\n",
    "print(f\"Vulnerability Status: {'Vulnerable' if results['synthetic_contract']['is_vulnerable'] else 'Safe'}\")\n",
    "print(f\"Synthetic Status: {'Synthetic' if results['synthetic_contract']['is_synthetic'] else 'Real'}\")\n",
    "\n",
    "if 'text' in results['synthetic_contract']:\n",
    "    print(\"\\nGenerated Synthetic Contract:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(results['synthetic_contract']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadd5d6-6f7d-4f52-8f69-b912bc30d5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aee0c4-4061-4050-aed3-144a0c775e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b59e7-1f96-49ad-8d2a-0485b2ff953f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e94209-f5b9-4eea-bb24-2482d771a1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
