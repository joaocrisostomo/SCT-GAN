{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dae59a4-ee6b-42b7-b124-7a24cb4e6974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m20180848/.conda/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3b62ad-ab1b-4986-9e52-cc9b752e696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def parse_solidity_to_ast(code: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse Solidity code into a simplified AST structure\n",
    "    \"\"\"\n",
    "    def extract_contract_info(code: str) -> Dict[str, Any]:\n",
    "        # Extract contract name\n",
    "        contract_match = re.search(r'contract\\s+(\\w+)', code)\n",
    "        contract_name = contract_match.group(1) if contract_match else \"Unknown\"\n",
    "        \n",
    "        # Extract functions\n",
    "        functions = []\n",
    "        function_pattern = r'function\\s+(\\w+)\\s*\\(([^)]*)\\)\\s*(?:public|private|internal|external)?\\s*(?:view|pure|payable)?\\s*(?:returns\\s*\\(([^)]*)\\))?\\s*{'\n",
    "        for match in re.finditer(function_pattern, code):\n",
    "            func_name = match.group(1)\n",
    "            params = match.group(2).split(',') if match.group(2) else []\n",
    "            returns = match.group(3).split(',') if match.group(3) else []\n",
    "            \n",
    "            functions.append({\n",
    "                'name': func_name,\n",
    "                'parameters': [p.strip() for p in params],\n",
    "                'returns': [r.strip() for r in returns]\n",
    "            })\n",
    "        \n",
    "        # Extract state variables\n",
    "        variables = []\n",
    "        var_pattern = r'(?:uint|address|string|bool|mapping)\\s+(?:\\w+)\\s+(\\w+)'\n",
    "        for match in re.finditer(var_pattern, code):\n",
    "            variables.append(match.group(1))\n",
    "        \n",
    "        return {\n",
    "            'type': 'Contract',\n",
    "            'name': contract_name,\n",
    "            'functions': functions,\n",
    "            'variables': variables\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Clean the code\n",
    "        code = re.sub(r'//.*?\\n|/\\*.*?\\*/', '', code)  # Remove comments\n",
    "        code = re.sub(r'\\s+', ' ', code)  # Normalize whitespace\n",
    "        \n",
    "        # Parse the code\n",
    "        ast = extract_contract_info(code)\n",
    "        return ast\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing code: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def prepare_code2vec_input(ast: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert AST to codeBert input format\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    \n",
    "    def extract_paths(node: Dict[str, Any], current_path: List[str] = None):\n",
    "        if current_path is None:\n",
    "            current_path = []\n",
    "            \n",
    "        # Add current node to path\n",
    "        if 'name' in node:\n",
    "            current_path.append(node['name'])\n",
    "            \n",
    "        # Process functions\n",
    "        if 'functions' in node:\n",
    "            for func in node['functions']:\n",
    "                func_path = current_path + [func['name']]\n",
    "                paths.append(' '.join(func_path))\n",
    "                \n",
    "                # Add parameter paths\n",
    "                for param in func['parameters']:\n",
    "                    param_path = func_path + [param]\n",
    "                    paths.append(' '.join(param_path))\n",
    "                \n",
    "                # Add return paths\n",
    "                for ret in func['returns']:\n",
    "                    ret_path = func_path + [ret]\n",
    "                    paths.append(' '.join(ret_path))\n",
    "        \n",
    "        # Process variables\n",
    "        if 'variables' in node:\n",
    "            for var in node['variables']:\n",
    "                var_path = current_path + [var]\n",
    "                paths.append(' '.join(var_path))\n",
    "    \n",
    "    extract_paths(ast)\n",
    "    return paths\n",
    "\n",
    "class SmartContractVulnerabilityDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 1024,\n",
    "        split: str = \"train\",\n",
    "        vulnerability_types: List[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: Path to the CSV file containing the dataset\n",
    "            tokenizer: Tokenizer for encoding the source code\n",
    "            max_length: Maximum sequence length\n",
    "            split: \"train\" or \"val\" to specify which split to load\n",
    "            vulnerability_types: List of vulnerability types to consider\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.vulnerability_types = vulnerability_types or [\n",
    "            'ARTHM', 'DOS', 'LE', 'RENT', 'TimeM', 'TimeO', 'Tx-Origin', 'UE'\n",
    "        ]\n",
    "        \n",
    "        # Load the dataset\n",
    "        self.data = self._load_dataset(data_path)\n",
    "        \n",
    "    def _load_dataset(self, data_path: str) -> List[Dict]:\n",
    "        \"\"\"Load and preprocess the dataset from CSV\"\"\"\n",
    "        dataset = []\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Split into train/val if needed\n",
    "        if self.split == \"train\":\n",
    "            df = df.sample(frac=0.8, random_state=42)\n",
    "        else:\n",
    "            df = df.sample(frac=0.2, random_state=42)\n",
    "        \n",
    "        # Process each contract\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                source_code = row['source_code']\n",
    "                contract_name = row['contract_name']\n",
    "                \n",
    "                # Parse AST and get paths\n",
    "                ast = parse_solidity_to_ast(source_code)\n",
    "                ast_paths = prepare_code2vec_input(ast) if ast else []\n",
    "                ast_path_text = ' '.join(ast_paths)\n",
    "                \n",
    "                # Split source code into lines\n",
    "                lines = source_code.split('\\n')\n",
    "                \n",
    "                # Create token-to-line mapping\n",
    "                token_to_line = []\n",
    "                current_line = 0\n",
    "                \n",
    "                # Tokenize each line separately to maintain mapping\n",
    "                for line in lines:\n",
    "                    line_tokens = self.tokenizer.encode(line, add_special_tokens=False)\n",
    "                    token_to_line.extend([current_line] * len(line_tokens))\n",
    "                    current_line += 1\n",
    "                \n",
    "                # Add special tokens\n",
    "                token_to_line = [0] + token_to_line + [0]  # [CLS] and [SEP] tokens\n",
    "                \n",
    "                # Truncate if too long\n",
    "                if len(token_to_line) > self.max_length:\n",
    "                    token_to_line = token_to_line[:self.max_length]\n",
    "                \n",
    "                # Pad if too short\n",
    "                if len(token_to_line) < self.max_length:\n",
    "                    token_to_line.extend([0] * (self.max_length - len(token_to_line)))\n",
    "                \n",
    "                # Create multi-label line labels for each vulnerability type\n",
    "                line_labels = self._create_multi_label_line_labels(source_code, row)\n",
    "                \n",
    "                # Create contract-level vulnerability labels\n",
    "                contract_labels = self._create_contract_vulnerability_labels(row)\n",
    "                \n",
    "                # Tokenize the source code\n",
    "                encoding = self.tokenizer(\n",
    "                    source_code,\n",
    "                    max_length=self.max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # Tokenize AST paths\n",
    "                ast_encoding = self.tokenizer(\n",
    "                    ast_path_text,\n",
    "                    max_length=self.max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # Convert line labels to tensor and ensure consistent shape\n",
    "                vuln_tensor = torch.zeros((len(self.vulnerability_types), self.max_length), dtype=torch.long)\n",
    "                for i, labels in enumerate(line_labels):\n",
    "                    if len(labels) > self.max_length:\n",
    "                        labels = labels[:self.max_length]\n",
    "                    vuln_tensor[i, :len(labels)] = torch.tensor(labels, dtype=torch.long)\n",
    "                \n",
    "                # Convert contract labels to tensor\n",
    "                contract_vuln_tensor = torch.tensor(contract_labels, dtype=torch.long)\n",
    "                \n",
    "                # Convert token_to_line to tensor\n",
    "                token_to_line_tensor = torch.tensor(token_to_line, dtype=torch.long)\n",
    "                \n",
    "                # Ensure attention masks are boolean\n",
    "                attention_mask = encoding['attention_mask'].squeeze(0).bool()\n",
    "                ast_attention_mask = ast_encoding['attention_mask'].squeeze(0).bool()\n",
    "                \n",
    "                # Ensure input_ids are the right length\n",
    "                input_ids = encoding['input_ids'].squeeze(0)\n",
    "                ast_input_ids = ast_encoding['input_ids'].squeeze(0)\n",
    "                \n",
    "                if len(input_ids) > self.max_length:\n",
    "                    input_ids = input_ids[:self.max_length]\n",
    "                if len(ast_input_ids) > self.max_length:\n",
    "                    ast_input_ids = ast_input_ids[:self.max_length]\n",
    "                \n",
    "                # Pad if necessary\n",
    "                if len(input_ids) < self.max_length:\n",
    "                    input_ids = torch.nn.functional.pad(input_ids, (0, self.max_length - len(input_ids)))\n",
    "                if len(ast_input_ids) < self.max_length:\n",
    "                    ast_input_ids = torch.nn.functional.pad(ast_input_ids, (0, self.max_length - len(ast_input_ids)))\n",
    "                \n",
    "                dataset.append({\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'ast_input_ids': ast_input_ids,\n",
    "                    'ast_attention_mask': ast_attention_mask,\n",
    "                    'vulnerable_lines': vuln_tensor,\n",
    "                    'contract_vulnerabilities': contract_vuln_tensor,\n",
    "                    'token_to_line': token_to_line_tensor,\n",
    "                    'source_code': source_code,\n",
    "                    'contract_name': contract_name\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing contract {contract_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def _create_contract_vulnerability_labels(self, row: pd.Series) -> List[int]:\n",
    "        \"\"\"Create contract-level vulnerability labels\"\"\"\n",
    "        contract_labels = []\n",
    "        for vuln_type in self.vulnerability_types:\n",
    "            # Check if contract has this vulnerability type\n",
    "            vuln_lines = row[f'{vuln_type}_lines']\n",
    "            if isinstance(vuln_lines, str):\n",
    "                try:\n",
    "                    vuln_lines = eval(vuln_lines)\n",
    "                except:\n",
    "                    vuln_lines = [vuln_lines]\n",
    "            \n",
    "            # Contract is vulnerable if it has any vulnerable lines\n",
    "            has_vulnerability = len(vuln_lines) > 0\n",
    "            contract_labels.append(1 if has_vulnerability else 0)\n",
    "        \n",
    "        return contract_labels\n",
    "    \n",
    "    def _create_multi_label_line_labels(self, source_code: str, row: pd.Series) -> List[List[int]]:\n",
    "        \"\"\"Create multi-label line labels for each vulnerability type\"\"\"\n",
    "        total_lines = len(source_code.split('\\n'))\n",
    "        line_labels = {vuln_type: [0] * total_lines for vuln_type in self.vulnerability_types}\n",
    "        \n",
    "        # Process each vulnerability type\n",
    "        for vuln_type in self.vulnerability_types:\n",
    "            vuln_lines = row[f'{vuln_type}_lines']\n",
    "            if isinstance(vuln_lines, str):\n",
    "                try:\n",
    "                    vuln_lines = eval(vuln_lines)\n",
    "                except:\n",
    "                    vuln_lines = [vuln_lines]\n",
    "            \n",
    "            # Process each vulnerable line/snippet\n",
    "            for line_or_snippet in vuln_lines:\n",
    "                if isinstance(line_or_snippet, int):\n",
    "                    # If it's a line number, mark that line\n",
    "                    if 0 <= line_or_snippet < total_lines:\n",
    "                        line_labels[vuln_type][line_or_snippet] = 1\n",
    "                else:\n",
    "                    # If it's a code snippet, find matching lines\n",
    "                    source_lines = source_code.split('\\n')\n",
    "                    for i, line in enumerate(source_lines):\n",
    "                        # Clean both the line and snippet for comparison\n",
    "                        clean_line = re.sub(r'\\s+', ' ', line.strip())\n",
    "                        clean_snippet = re.sub(r'\\s+', ' ', str(line_or_snippet).strip())\n",
    "                        if clean_snippet in clean_line:\n",
    "                            line_labels[vuln_type][i] = 1\n",
    "        \n",
    "        # Convert to list format\n",
    "        return [line_labels[vuln_type] for vuln_type in self.vulnerability_types]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        return self.data[idx]\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable length inputs\n",
    "    \"\"\"\n",
    "    # Get the maximum length in this batch for each type of tensor\n",
    "    max_input_len = max(item['input_ids'].size(0) for item in batch)\n",
    "    \n",
    "    # Pad all tensors to their respective maximum lengths\n",
    "    padded_batch = {\n",
    "        'input_ids': torch.stack([\n",
    "            torch.nn.functional.pad(item['input_ids'], (0, max_input_len - item['input_ids'].size(0)))\n",
    "            for item in batch\n",
    "        ]),\n",
    "        'attention_mask': torch.stack([\n",
    "            torch.nn.functional.pad(item['attention_mask'], (0, max_input_len - item['attention_mask'].size(0)))\n",
    "            for item in batch\n",
    "        ]),\n",
    "        'ast_input_ids': torch.stack([item['ast_input_ids'] for item in batch]),\n",
    "        'ast_attention_mask': torch.stack([item['ast_attention_mask'] for item in batch]),\n",
    "        'vulnerable_lines': torch.stack([item['vulnerable_lines'] for item in batch]),\n",
    "        'contract_vulnerabilities': torch.stack([item['contract_vulnerabilities'] for item in batch]),\n",
    "        'token_to_line': torch.stack([item['token_to_line'] for item in batch]),\n",
    "        'source_code': [item['source_code'] for item in batch],\n",
    "        'contract_name': [item['contract_name'] for item in batch]\n",
    "    }\n",
    "    \n",
    "    return padded_batch\n",
    "\n",
    "def create_dataloaders(\n",
    "    data_path: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    batch_size: int = 8,\n",
    "    max_length: int = 1024,\n",
    "    num_workers: int = 4,\n",
    "    vulnerability_types: List[str] = None\n",
    ") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Create train and validation dataloaders\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the CSV file containing the dataset\n",
    "        tokenizer: Tokenizer for encoding the source code\n",
    "        batch_size: Batch size for training\n",
    "        max_length: Maximum sequence length\n",
    "        num_workers: Number of workers for data loading\n",
    "        vulnerability_types: List of vulnerability types to consider\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_dataloader, val_dataloader)\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = SmartContractVulnerabilityDataset(\n",
    "        data_path=data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        split=\"train\",\n",
    "        vulnerability_types=vulnerability_types\n",
    "    )\n",
    "    \n",
    "    val_dataset = SmartContractVulnerabilityDataset(\n",
    "        data_path=data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        split=\"val\",\n",
    "        vulnerability_types=vulnerability_types\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders with custom collate function\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9fa1d2-bf27-4f7b-b6be-80408ea90a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m20180848/.conda/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1211 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader, val_dataloader = create_dataloaders(\n",
    "    data_path=\"contract_sources_with_vulnerabilities_2048_token_size.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8,\n",
    "    max_length=1024,\n",
    "    vulnerability_types=['ARTHM', 'DOS', 'LE', 'RENT', 'TimeM', 'TimeO', 'Tx-Origin', 'UE']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "279fb575-bb52-42ed-980f-4a2a3bb185c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "tensor([    0,  4862,  1073,  ...,   560, 12905,     2])\n",
      "1024\n",
      "attention_mask\n",
      "ast_input_ids\n",
      "ast_attention_mask\n",
      "vulnerable_lines\n",
      "token_to_line\n",
      "source_code\n",
      "contract_name\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for key, value in batch.items():\n",
    "    print(key)\n",
    "    if key == 'input_ids':\n",
    "        print(value[0])\n",
    "        print(len(value[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a631d-a10f-431b-ba9b-3f414e8e7771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m20180848/.conda/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2487 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataloader(dataloader: torch.utils.data.DataLoader, num_batches: int = 1):\n",
    "    \"\"\"\n",
    "    Inspect the contents of a dataloader\n",
    "    \n",
    "    Args:\n",
    "        dataloader: The dataloader to inspect\n",
    "        num_batches: Number of batches to inspect\n",
    "    \"\"\"\n",
    "    print(f\"\\nDataloader has {len(dataloader)} batches\")\n",
    "    print(f\"Batch size: {dataloader.batch_size}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        if batch_idx >= num_batches:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nBatch {batch_idx + 1}:\")\n",
    "        print(f\"Number of samples in batch: {len(batch['input_ids'])}\")\n",
    "        print(f\"Input shape: {batch['input_ids'].shape}\")\n",
    "        print(f\"Vulnerability labels shape: {batch['vulnerable_lines'].shape}\")\n",
    "        \n",
    "        # Print sample contract names\n",
    "        print(\"\\nSample contract names:\")\n",
    "        for name in batch['contract_name'][:2]:  # Show first 2 contracts\n",
    "            print(f\"- {name}\")\n",
    "            \n",
    "        # Print vulnerability statistics\n",
    "        vuln_labels = batch['vulnerable_lines']\n",
    "        total_vulns = vuln_labels.sum().item()\n",
    "        print(f\"\\nTotal vulnerable lines in batch: {total_vulns}\")\n",
    "        \n",
    "        # Print sample of source code\n",
    "        print(\"\\nSample source code (first 200 chars):\")\n",
    "        print(batch['source_code'][0][:200] + \"...\")\n",
    "        \n",
    "        break  # Only show first batch\n",
    "\n",
    "\n",
    "    \n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader, val_dataloader = create_dataloaders(\n",
    "    data_path=\"contract_sources_with_vulnerabilities.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8,\n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "# Inspect train dataloader\n",
    "print(\"\\nInspecting training dataloader:\")\n",
    "inspect_dataloader(train_dataloader)\n",
    "\n",
    "# Inspect validation dataloader\n",
    "print(\"\\nInspecting validation dataloader:\")\n",
    "inspect_dataloader(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4835271-e2f6-49eb-8c4c-770865775126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
