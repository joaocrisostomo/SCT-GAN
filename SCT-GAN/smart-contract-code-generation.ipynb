{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58f55485-2c52-452f-88eb-da6bebedc5ab",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/m20180848/.local/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/m20180848/.local/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/anaconda/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/anaconda/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/anaconda/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc70e70-3388-48a1-9891-528c5230273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m20180848/.conda/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from inference import *\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Model imports\n",
    "from model import SmartContractTransformer\n",
    "from inference import SmartContractAnalyzer\n",
    "\n",
    "# Optional but useful imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # for progress bars\n",
    "import logging\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import re\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33baac5-1132-4019-9a19-9de07ce9f3fe",
   "metadata": {},
   "source": [
    "# 0. AUX Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf829a4-aab2-4666-9d54-1f5ed0de035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def parse_solidity_to_ast(code: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse Solidity code into a simplified AST structure\n",
    "    \"\"\"\n",
    "    def extract_contract_info(code: str) -> Dict[str, Any]:\n",
    "        # Extract contract name\n",
    "        contract_match = re.search(r'contract\\s+(\\w+)', code)\n",
    "        contract_name = contract_match.group(1) if contract_match else \"Unknown\"\n",
    "        \n",
    "        # Extract functions\n",
    "        functions = []\n",
    "        function_pattern = r'function\\s+(\\w+)\\s*\\(([^)]*)\\)\\s*(?:public|private|internal|external)?\\s*(?:view|pure|payable)?\\s*(?:returns\\s*\\(([^)]*)\\))?\\s*{'\n",
    "        for match in re.finditer(function_pattern, code):\n",
    "            func_name = match.group(1)\n",
    "            params = match.group(2).split(',') if match.group(2) else []\n",
    "            returns = match.group(3).split(',') if match.group(3) else []\n",
    "            \n",
    "            functions.append({\n",
    "                'name': func_name,\n",
    "                'parameters': [p.strip() for p in params],\n",
    "                'returns': [r.strip() for r in returns]\n",
    "            })\n",
    "        \n",
    "        # Extract state variables\n",
    "        variables = []\n",
    "        var_pattern = r'(?:uint|address|string|bool|mapping)\\s+(?:\\w+)\\s+(\\w+)'\n",
    "        for match in re.finditer(var_pattern, code):\n",
    "            variables.append(match.group(1))\n",
    "        \n",
    "        return {\n",
    "            'type': 'Contract',\n",
    "            'name': contract_name,\n",
    "            'functions': functions,\n",
    "            'variables': variables\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Clean the code\n",
    "        code = re.sub(r'//.*?\\n|/\\*.*?\\*/', '', code)  # Remove comments\n",
    "        code = re.sub(r'\\s+', ' ', code)  # Normalize whitespace\n",
    "        \n",
    "        # Parse the code\n",
    "        ast = extract_contract_info(code)\n",
    "        return ast\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing code: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def prepare_code2vec_input(ast: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert AST to codeBert input format\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    \n",
    "    def extract_paths(node: Dict[str, Any], current_path: List[str] = None):\n",
    "        if current_path is None:\n",
    "            current_path = []\n",
    "            \n",
    "        # Add current node to path\n",
    "        if 'name' in node:\n",
    "            current_path.append(node['name'])\n",
    "            \n",
    "        # Process functions\n",
    "        if 'functions' in node:\n",
    "            for func in node['functions']:\n",
    "                func_path = current_path + [func['name']]\n",
    "                paths.append(' '.join(func_path))\n",
    "                \n",
    "                # Add parameter paths\n",
    "                for param in func['parameters']:\n",
    "                    param_path = func_path + [param]\n",
    "                    paths.append(' '.join(param_path))\n",
    "                \n",
    "                # Add return paths\n",
    "                for ret in func['returns']:\n",
    "                    ret_path = func_path + [ret]\n",
    "                    paths.append(' '.join(ret_path))\n",
    "        \n",
    "        # Process variables\n",
    "        if 'variables' in node:\n",
    "            for var in node['variables']:\n",
    "                var_path = current_path + [var]\n",
    "                paths.append(' '.join(var_path))\n",
    "    \n",
    "    extract_paths(ast)\n",
    "    return paths\n",
    "\n",
    "class SmartContractVulnerabilityDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 1024,\n",
    "        split: str = \"train\",\n",
    "        vulnerability_types: List[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: Path to the CSV file containing the dataset\n",
    "            tokenizer: Tokenizer for encoding the source code\n",
    "            max_length: Maximum sequence length\n",
    "            split: \"train\" or \"val\" to specify which split to load\n",
    "            vulnerability_types: List of vulnerability types to consider\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.vulnerability_types = vulnerability_types or [\n",
    "            'ARTHM', 'DOS', 'LE', 'RENT', 'TimeM', 'TimeO', 'Tx-Origin', 'UE'\n",
    "        ]\n",
    "        \n",
    "        # Load the dataset\n",
    "        self.data = self._load_dataset(data_path)\n",
    "        \n",
    "    def _load_dataset(self, data_path: str) -> List[Dict]:\n",
    "        \"\"\"Load and preprocess the dataset from CSV\"\"\"\n",
    "        dataset = []\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Split into train/val if needed\n",
    "        if self.split == \"train\":\n",
    "            df = df.sample(frac=0.8, random_state=42)\n",
    "        else:\n",
    "            df = df.sample(frac=0.2, random_state=42)\n",
    "        \n",
    "        # Process each contract\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                source_code = row['source_code']\n",
    "                contract_name = row['contract_name']\n",
    "                \n",
    "                # Parse AST and get paths\n",
    "                ast = parse_solidity_to_ast(source_code)\n",
    "                ast_paths = prepare_code2vec_input(ast) if ast else []\n",
    "                ast_path_text = ' '.join(ast_paths)\n",
    "                \n",
    "                # Split source code into lines\n",
    "                lines = source_code.split('\\n')\n",
    "                \n",
    "                # Create token-to-line mapping\n",
    "                token_to_line = []\n",
    "                current_line = 0\n",
    "                \n",
    "                # Tokenize each line separately to maintain mapping\n",
    "                for line in lines:\n",
    "                    line_tokens = self.tokenizer.encode(line, add_special_tokens=False)\n",
    "                    token_to_line.extend([current_line] * len(line_tokens))\n",
    "                    current_line += 1\n",
    "                \n",
    "                # Add special tokens\n",
    "                token_to_line = [0] + token_to_line + [0]  # [CLS] and [SEP] tokens\n",
    "                \n",
    "                # Truncate if too long\n",
    "                if len(token_to_line) > self.max_length:\n",
    "                    token_to_line = token_to_line[:self.max_length]\n",
    "                \n",
    "                # Pad if too short\n",
    "                if len(token_to_line) < self.max_length:\n",
    "                    token_to_line.extend([0] * (self.max_length - len(token_to_line)))\n",
    "                \n",
    "                # Create multi-label line labels for each vulnerability type\n",
    "                line_labels = self._create_multi_label_line_labels(source_code, row)\n",
    "                \n",
    "                # Create contract-level vulnerability labels\n",
    "                contract_labels = self._create_contract_vulnerability_labels(row)\n",
    "                \n",
    "                # Tokenize the source code\n",
    "                encoding = self.tokenizer(\n",
    "                    source_code,\n",
    "                    max_length=self.max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # Tokenize AST paths\n",
    "                ast_encoding = self.tokenizer(\n",
    "                    ast_path_text,\n",
    "                    max_length=self.max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # Convert line labels to tensor and ensure consistent shape\n",
    "                vuln_tensor = torch.zeros((len(self.vulnerability_types), self.max_length), dtype=torch.long)\n",
    "                for i, labels in enumerate(line_labels):\n",
    "                    if len(labels) > self.max_length:\n",
    "                        labels = labels[:self.max_length]\n",
    "                    vuln_tensor[i, :len(labels)] = torch.tensor(labels, dtype=torch.long)\n",
    "                \n",
    "                # Convert contract labels to tensor\n",
    "                contract_vuln_tensor = torch.tensor(contract_labels, dtype=torch.long)\n",
    "                \n",
    "                # Convert token_to_line to tensor\n",
    "                token_to_line_tensor = torch.tensor(token_to_line, dtype=torch.long)\n",
    "                \n",
    "                # Ensure attention masks are boolean\n",
    "                attention_mask = encoding['attention_mask'].squeeze(0).bool()\n",
    "                ast_attention_mask = ast_encoding['attention_mask'].squeeze(0).bool()\n",
    "                \n",
    "                # Ensure input_ids are the right length\n",
    "                input_ids = encoding['input_ids'].squeeze(0)\n",
    "                ast_input_ids = ast_encoding['input_ids'].squeeze(0)\n",
    "                \n",
    "                if len(input_ids) > self.max_length:\n",
    "                    input_ids = input_ids[:self.max_length]\n",
    "                if len(ast_input_ids) > self.max_length:\n",
    "                    ast_input_ids = ast_input_ids[:self.max_length]\n",
    "                \n",
    "                # Pad if necessary\n",
    "                if len(input_ids) < self.max_length:\n",
    "                    input_ids = torch.nn.functional.pad(input_ids, (0, self.max_length - len(input_ids)))\n",
    "                if len(ast_input_ids) < self.max_length:\n",
    "                    ast_input_ids = torch.nn.functional.pad(ast_input_ids, (0, self.max_length - len(ast_input_ids)))\n",
    "                \n",
    "                dataset.append({\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'ast_input_ids': ast_input_ids,\n",
    "                    'ast_attention_mask': ast_attention_mask,\n",
    "                    'vulnerable_lines': vuln_tensor,\n",
    "                    'contract_vulnerabilities': contract_vuln_tensor,\n",
    "                    'token_to_line': token_to_line_tensor,\n",
    "                    'source_code': source_code,\n",
    "                    'contract_name': contract_name\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing contract {contract_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def _create_contract_vulnerability_labels(self, row: pd.Series) -> List[int]:\n",
    "        \"\"\"Create contract-level vulnerability labels\"\"\"\n",
    "        contract_labels = []\n",
    "        for vuln_type in self.vulnerability_types:\n",
    "            # Check if contract has this vulnerability type\n",
    "            vuln_lines = row[f'{vuln_type}_lines']\n",
    "            if isinstance(vuln_lines, str):\n",
    "                try:\n",
    "                    vuln_lines = eval(vuln_lines)\n",
    "                except:\n",
    "                    vuln_lines = [vuln_lines]\n",
    "            \n",
    "            # Contract is vulnerable if it has any vulnerable lines\n",
    "            has_vulnerability = len(vuln_lines) > 0\n",
    "            contract_labels.append(1 if has_vulnerability else 0)\n",
    "        \n",
    "        return contract_labels\n",
    "    \n",
    "    def _create_multi_label_line_labels(self, source_code: str, row: pd.Series) -> List[List[int]]:\n",
    "        \"\"\"Create multi-label line labels for each vulnerability type\"\"\"\n",
    "        total_lines = len(source_code.split('\\n'))\n",
    "        line_labels = {vuln_type: [0] * total_lines for vuln_type in self.vulnerability_types}\n",
    "        \n",
    "        # Process each vulnerability type\n",
    "        for vuln_type in self.vulnerability_types:\n",
    "            vuln_lines = row[f'{vuln_type}_lines']\n",
    "            if isinstance(vuln_lines, str):\n",
    "                try:\n",
    "                    vuln_lines = eval(vuln_lines)\n",
    "                except:\n",
    "                    vuln_lines = [vuln_lines]\n",
    "            \n",
    "            # Process each vulnerable line/snippet\n",
    "            for line_or_snippet in vuln_lines:\n",
    "                if isinstance(line_or_snippet, int):\n",
    "                    # If it's a line number, mark that line\n",
    "                    if 0 <= line_or_snippet < total_lines:\n",
    "                        line_labels[vuln_type][line_or_snippet] = 1\n",
    "                else:\n",
    "                    # If it's a code snippet, find matching lines\n",
    "                    source_lines = source_code.split('\\n')\n",
    "                    for i, line in enumerate(source_lines):\n",
    "                        # Clean both the line and snippet for comparison\n",
    "                        clean_line = re.sub(r'\\s+', ' ', line.strip())\n",
    "                        clean_snippet = re.sub(r'\\s+', ' ', str(line_or_snippet).strip())\n",
    "                        if clean_snippet in clean_line:\n",
    "                            line_labels[vuln_type][i] = 1\n",
    "        \n",
    "        # Convert to list format\n",
    "        return [line_labels[vuln_type] for vuln_type in self.vulnerability_types]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        return self.data[idx]\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable length inputs\n",
    "    \"\"\"\n",
    "    # Get the maximum length in this batch for each type of tensor\n",
    "    max_input_len = max(item['input_ids'].size(0) for item in batch)\n",
    "    \n",
    "    # Pad all tensors to their respective maximum lengths\n",
    "    padded_batch = {\n",
    "        'input_ids': torch.stack([\n",
    "            torch.nn.functional.pad(item['input_ids'], (0, max_input_len - item['input_ids'].size(0)))\n",
    "            for item in batch\n",
    "        ]),\n",
    "        'attention_mask': torch.stack([\n",
    "            torch.nn.functional.pad(item['attention_mask'], (0, max_input_len - item['attention_mask'].size(0)))\n",
    "            for item in batch\n",
    "        ]),\n",
    "        'ast_input_ids': torch.stack([item['ast_input_ids'] for item in batch]),\n",
    "        'ast_attention_mask': torch.stack([item['ast_attention_mask'] for item in batch]),\n",
    "        'vulnerable_lines': torch.stack([item['vulnerable_lines'] for item in batch]),\n",
    "        'contract_vulnerabilities': torch.stack([item['contract_vulnerabilities'] for item in batch]),\n",
    "        'token_to_line': torch.stack([item['token_to_line'] for item in batch]),\n",
    "        'source_code': [item['source_code'] for item in batch],\n",
    "        'contract_name': [item['contract_name'] for item in batch]\n",
    "    }\n",
    "    \n",
    "    return padded_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92266a-e5c0-47bc-9d78-5b478d9c4d34",
   "metadata": {},
   "source": [
    "# 0 Constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24465794-d11b-40f6-97a3-a41d831968de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"contract_sources_with_vulnerabilities_2048_token_size.csv\"\n",
    "MODEL_PATH = \"checkpoints_v5_2048_output/best_model_augmented_gan_epoch_106.pt\"\n",
    "TOKENIZER_NAME = \"microsoft/codebert-base\"\n",
    "VULNERABILITY_TYPES = ['ARTHM', 'DOS', 'LE', 'RENT', 'TimeM', 'TimeO', 'Tx-Origin', 'UE']\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "MODEL_LINE_CODE_VULNERABILITY_THRESHOLD = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06292e0-265d-4a18-bf13-872a78843c25",
   "metadata": {},
   "source": [
    "# 1 Load Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335b0e10-576d-42f7-830a-ce60b449e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    data_path: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    batch_size: int = 8,\n",
    "    max_length: int = 1024,\n",
    "    num_workers: int = 4,\n",
    "    vulnerability_types: List[str] = None\n",
    ") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    \n",
    "    val_dataset = SmartContractVulnerabilityDataset(\n",
    "        data_path=data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        split=\"val\",\n",
    "        vulnerability_types=vulnerability_types\n",
    "    )\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    return  dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bea9d5c-1371-4d31-9b39-d84ae55e03f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m20180848/.conda/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1211 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "val_dataloader = create_dataloaders(\n",
    "    data_path=DATA_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8,\n",
    "    max_length=1024,\n",
    "    vulnerability_types=VULNERABILITY_TYPES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b43db63-0e15-4e48-b9ec-399aa5c68e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataloader.dataset.data[0]['vulnerable_lines'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea56f5f-154f-4ffe-9360-3a227bada3e1",
   "metadata": {},
   "source": [
    "# 1. Contract Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c7b4c3-0222-4a37-82e9-f96522cc4d2d",
   "metadata": {},
   "source": [
    "## 1.2 Load Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d960e29-408a-48a5-86b7-88d2b4fa1837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m20180848/.conda/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/m20180848/.conda/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Initialized line feature extractor layer 1 with small random weights\n",
      "DEBUG: Initialized line feature extractor layer 3 with small random weights\n",
      "DEBUG: Initialized custom line feature extractor with small weights\n",
      "Model loaded from checkpoints_v5_2048_output/best_model_augmented_gan_epoch_106.pt\n",
      "Training epoch: 106\n",
      "Best validation loss: 0.773994717746973\n",
      "Training config: GAN=True\n"
     ]
    }
   ],
   "source": [
    "model_path=MODEL_PATH\n",
    "device = DEVICE\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = SmartContractTransformer(\n",
    "        d_model=768,\n",
    "        nhead=8,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        max_length=1024,\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        num_vulnerability_types=8,\n",
    "        use_gan=True \n",
    "    )\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    print(f\"Training epoch: {checkpoint.get('epoch', 'Unknown')}\")\n",
    "    print(f\"Best validation loss: {checkpoint.get('val_loss', 'Unknown')}\")\n",
    "    print(f\"Training config: GAN={checkpoint.get('use_gan', 'Unknown')}\")\n",
    "else:\n",
    "    # Direct state dict\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "    \n",
    "vulnerability_types = [\n",
    "    'ARTHM', 'DOS', 'LE', 'RENT', 'TimeM', 'TimeO', 'Tx-Origin', 'UE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e4baff-1b79-47bd-b80a-89d9d490f9a8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/**\n",
      " * Source Code first verified at https://etherscan.io on Tuesday, December 18, 2018\n",
      " (UTC) */\n",
      "\n",
      "pragma solidity ^0.4.23;\n",
      "\n",
      "library SafeMath {\n",
      "\n",
      "  /**\n",
      "  * @dev Multiplies two numbers, throws on overflow.\n",
      "  */\n",
      "  function mul(uint256 a, uint256 b) internal pure returns (uint256 c) {\n",
      "\n",
      "    if (a == 0) {\n",
      "      return 0;\n",
      "    }\n",
      "\n",
      "    c = a * b;\n",
      "    assert(c / a == b);\n",
      "    return c;\n",
      "  }\n",
      "\n",
      "  /**\n",
      "  * @dev Integer division of two numbers, truncating the quotient.\n",
      "  */\n",
      "  function div(uint256 a, uint256 b) internal pure returns (uint256) {\n",
      "\n",
      "    return a / b;\n",
      "  }\n",
      "\n",
      "  /**\n",
      "  * @dev Subtracts two numbers, throws on overflow (i.e. if subtrahend is greater than minuend).\n",
      "  */\n",
      "  function sub(uint256 a, uint256 b) internal pure returns (uint256) {\n",
      "    assert(b <= a);\n",
      "    return a - b;\n",
      "  }\n",
      "\n",
      "  /**\n",
      "  * @dev Adds two numbers, throws on overflow.\n",
      "  */\n",
      "  function add(uint256 a, uint256 b) internal pure returns (uint256 c) {\n",
      "    c = a + b;\n",
      "    assert(c >= a);\n",
      "    return c;\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "contract ERC20Basic {\n",
      "    \n",
      "  function totalSupply() public view returns (uint256);\n",
      "  function balanceOf(address who) public view returns (uint256);\n",
      "  function transfer(address to, uint256 value) public returns (bool);\n",
      "  event Transfer(address indexed from, address indexed to, uint256 value);\n",
      "  \n",
      "}\n",
      "\n",
      "contract ERC20 is ERC20Basic {\n",
      "    \n",
      "  function allowance(address owner, address spender)\n",
      "    public view returns (uint256);\n",
      "\n",
      "  function transferFrom(address from, address to, uint256 value)\n",
      "    public returns (bool);\n",
      "\n",
      "  function approve(address spender, uint256 value) public returns (bool);\n",
      "  event Approval(\n",
      "    address indexed owner,\n",
      "    address indexed spender,\n",
      "    uint256 value\n",
      "  );\n",
      "}\n",
      "\n",
      "contract DetailedERC20 is ERC20 {\n",
      "  string public name;\n",
      "  string public symbol;\n",
      "  uint8 public decimals;\n",
      "\n",
      "  constructor(string _name, string _symbol, uint8 _decimals) public {\n",
      "    name = _name;\n",
      "    symbol = _symbol;\n",
      "    decimals = _decimals;\n",
      "  }\n",
      "}\n",
      "\n",
      "/**\n",
      " * @title 实现ERC20基本合约的接口 \n",
      " * @dev 基本的StandardToken，不包含allowances.\n",
      " */\n",
      "contract BasicToken is ERC20Basic {\n",
      "  using SafeMath for uint256;\n",
      "\n",
      "  mapping(address => uint256) balances;\n",
      "\n",
      "  uint256 totalSupply_;\n",
      "  \n",
      "  function totalSupply() public view returns (uint256) {\n",
      "    return totalSupply_;\n",
      "  }\n",
      "\n",
      "  function transfer(address _to, uint256 _value) public returns (bool) {\n",
      "    require(_to != address(0));\n",
      "    require(_value <= balances[msg.sender]);\n",
      "    balances[msg.sender] = balances[msg.sender].sub(_value);\n",
      "    balances[_to] = balances[_to].add(_value);\n",
      "    emit Transfer(msg.sender, _to, _value);\n",
      "    return true;\n",
      "  }\n",
      "\n",
      "  function balanceOf(address _owner) public view returns (uint256) {\n",
      "    return balances[_owner];\n",
      "  }\n",
      "\n",
      "}\n",
      "\n",
      "contract StandardToken is ERC20, BasicToken {\n",
      "  mapping (address => mapping (address => uint256)) internal allowed;\n",
      "\n",
      "  /**\n",
      "   * @dev 从一个地址向另外一个地址转token\n",
      "   * @param _from 转账的from地址\n",
      "   * @param _to address 转账的to地址\n",
      "   * @param _value uint256 转账token数量\n",
      "   */\n",
      "  function transferFrom(\n",
      "    address _from,\n",
      "    address _to,\n",
      "    uint256 _value\n",
      "  )\n",
      "    public\n",
      "    returns (bool)\n",
      "  {\n",
      "    // 做合法性检查\n",
      "    require(_to != address(0));\n",
      "    require(_value <= balances[_from]);\n",
      "    require(_value <= allowed[_from][msg.sender]);\n",
      "    balances[_from] = balances[_from].sub(_value);\n",
      "    balances[_to] = balances[_to].add(_value);\n",
      "    allowed[_from][msg.sender] = allowed[_from][msg.sender].sub(_value);\n",
      "    emit Transfer(_from, _to, _value);\n",
      "    return true;\n",
      "  }\n",
      "\n",
      "  function approve(address _spender, uint256 _value) public returns (bool) {\n",
      "    allowed[msg.sender][_spender] = _value;\n",
      "    emit Approval(msg.sender, _spender, _value);\n",
      "    return true;\n",
      "  }\n",
      "\n",
      "  function allowance(\n",
      "    address _owner,\n",
      "    address _spender\n",
      "   )\n",
      "    public\n",
      "    view\n",
      "    returns (uint256)\n",
      "  {\n",
      "    return allowed[_owner][_spender];\n",
      "  }\n",
      "\n",
      "}\n",
      "\n",
      "contract BurnableToken is BasicToken {\n",
      "\n",
      "  event Burn(address indexed burner, uint256 value);\n",
      "\n",
      "}\n",
      "\n",
      "contract MintableToken is StandardToken {\n",
      "  event Mint(address indexed to, uint256 amount);\n",
      "  event MintFinished();\n",
      "\n",
      "  bool public mintingFinished = false;\n",
      "\n",
      "\n",
      "  modifier canMint() {\n",
      "    require(!mintingFinished);\n",
      "    _;\n",
      "  }\n",
      "\n",
      "\n",
      "  /**\n",
      "   * @dev Function to stop minting new tokens.\n",
      "   * @return True if the operation was successful.\n",
      "   */\n",
      "  function finishMinting() public  canMint returns (bool) {\n",
      "    mintingFinished = true;\n",
      "    emit MintFinished();\n",
      "    return true;\n",
      "  }\n",
      "}\n",
      "\n",
      "contract StandardBurnableToken is BurnableToken, StandardToken,MintableToken {\n",
      "\n",
      "\n",
      "  \n",
      "}\n",
      "\n",
      "contract valuehometoken is StandardBurnableToken {\n",
      "    string public name = 'value home token';\n",
      "    string public symbol = 'VHT';\n",
      "    uint8 public decimals = 8;\n",
      "    uint256 public INITIAL_SUPPLY = 50000000000000000; \n",
      "    \n",
      "  constructor() public {\n",
      "    totalSupply_ = INITIAL_SUPPLY;\n",
      "    balances[msg.sender] = INITIAL_SUPPLY;\n",
      "  }\n",
      "\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(val_dataloader.dataset.data[5]['source_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c110a268-e8e8-4fd7-af60-07f244bc39d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m20180848/.conda/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Initialized line feature extractor layer 1 with small random weights\n",
      "DEBUG: Initialized line feature extractor layer 3 with small random weights\n",
      "DEBUG: Initialized custom line feature extractor with small weights\n",
      "Model loaded from checkpoints_v5_2048_output/best_model_augmented_gan_epoch_106.pt\n",
      "Training epoch: 106\n",
      "Best validation loss: 0.773994717746973\n",
      "Training config: GAN=True\n"
     ]
    }
   ],
   "source": [
    "model = SmartContractTransformer(\n",
    "        d_model=768,\n",
    "        nhead=8,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        max_length=1024,\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        num_vulnerability_types=8,\n",
    "        use_gan=True \n",
    "    )\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    print(f\"Training epoch: {checkpoint.get('epoch', 'Unknown')}\")\n",
    "    print(f\"Best validation loss: {checkpoint.get('val_loss', 'Unknown')}\")\n",
    "    print(f\"Training config: GAN={checkpoint.get('use_gan', 'Unknown')}\")\n",
    "else:\n",
    "    # Direct state dict\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "contract_code = val_dataloader.dataset.data[5]['source_code']\n",
    "ast = parse_solidity_to_ast(contract_code)\n",
    "ast_paths = prepare_code2vec_input(ast) if ast else []\n",
    "ast_path_text = ' '.join(ast_paths)\n",
    "\n",
    "# Tokenize inputs\n",
    "contract_encoding = tokenizer(\n",
    "    contract_code,\n",
    "    max_length=1024,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "ast_encoding = tokenizer(\n",
    "    ast_path_text,\n",
    "    max_length=1024,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids = contract_encoding['input_ids'].to(device)\n",
    "attention_mask = contract_encoding['attention_mask'].to(device)\n",
    "ast_input_ids = ast_encoding['input_ids'].to(device)\n",
    "ast_attention_mask = ast_encoding['attention_mask'].to(device)\n",
    "\n",
    "model._debug_mode = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        ast_input_ids=ast_input_ids,\n",
    "        ast_attention_mask=ast_attention_mask,\n",
    "        target_ids=input_ids,  # Use input_ids as target for inference\n",
    "        token_to_line=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f3a30c-333b-487a-b2f3-ac3a9ac8b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "    \n",
    "vulnerability_types = [\n",
    "    'ARTHM', 'DOS', 'LE', 'RENT', 'TimeM', 'TimeO', 'Tx-Origin', 'UE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c5fb43a-4dbe-4c22-b86a-07f822d582b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_code = val_dataloader.dataset.data[5]['source_code']\n",
    "ast = parse_solidity_to_ast(contract_code)\n",
    "ast_paths = prepare_code2vec_input(ast) if ast else []\n",
    "ast_path_text = ' '.join(ast_paths)\n",
    "\n",
    "# Tokenize inputs\n",
    "contract_encoding = tokenizer(\n",
    "    contract_code,\n",
    "    max_length=1024,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "ast_encoding = tokenizer(\n",
    "    ast_path_text,\n",
    "    max_length=1024,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "806a4a45-145f-479b-9e66-3b8ac417be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = contract_encoding['input_ids'].to(device)\n",
    "attention_mask = contract_encoding['attention_mask'].to(device)\n",
    "ast_input_ids = ast_encoding['input_ids'].to(device)\n",
    "ast_attention_mask = ast_encoding['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86e0c118-59b7-475c-acfa-35f39a64f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._debug_mode = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        ast_input_ids=ast_input_ids,\n",
    "        ast_attention_mask=ast_attention_mask,\n",
    "        target_ids=input_ids,  # Use input_ids as target for inference\n",
    "        token_to_line=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1314bf9d-d5e4-444a-a01c-bbd08d5c85bd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[-11.3019,  -8.9195,   3.9378,  ..., -11.2757, -11.2904, -11.2999],\n",
       "         [-12.9732,  -5.3152,   4.2387,  ..., -12.9763, -12.8868, -12.9534],\n",
       "         [ -1.0804,   4.8225,   9.7027,  ...,  -1.1027,  -1.0450,  -1.1748],\n",
       "         ...,\n",
       "         [-16.8429,  -9.4144,   3.8516,  ..., -16.7851, -16.8252, -16.8731],\n",
       "         [-10.9757,  -6.3530,   8.9934,  ..., -10.9440, -11.0186, -11.0578],\n",
       "         [ -7.8117,  -4.9354,  22.3554,  ...,  -7.8987,  -7.8499,  -7.9367]],\n",
       "        device='cuda:0'),\n",
       " 'target_ids': tensor([49795, 50121, 50118,  ..., 44547, 18134,     2], device='cuda:0'),\n",
       " 'contract_vulnerability_logits': tensor([[ 4.8884e-01, -2.7327e-01, -2.4371e-01,  1.0683e-04, -2.8266e-01,\n",
       "          -3.0378e-01, -3.6681e+00, -3.8108e-01]], device='cuda:0'),\n",
       " 'line_vulnerability_logits': tensor([[[-46.0587, -82.9822, -96.5135,  ..., -76.5406, -90.5835, -93.7482],\n",
       "          [-34.9762, -70.2816, -79.0512,  ..., -63.8398, -74.7924, -72.7562],\n",
       "          [-30.4257, -69.3439, -73.3391,  ..., -59.0498, -69.0454, -67.1862],\n",
       "          ...,\n",
       "          [-45.2418, -55.3923, -24.7761,  ..., -56.5787, -21.3052, -68.1177],\n",
       "          [-47.8102, -59.1415, -28.4515,  ..., -54.0955, -25.0239, -65.5754],\n",
       "          [-53.3203, -64.2164, -41.7273,  ..., -64.5735, -37.3309, -69.1087]]],\n",
       "        device='cuda:0'),\n",
       " 'encoder_output': tensor([[-7.2687e-02, -7.7804e-02,  1.9981e-02,  3.1393e-02, -3.4212e-02,\n",
       "          -8.7987e-02,  4.5678e-02,  1.1647e-01, -5.2725e-03, -6.7698e-02,\n",
       "          -2.4850e-02,  1.3761e-02,  3.4831e-02, -1.0819e-02,  1.6124e-01,\n",
       "          -2.6777e-02, -3.9366e-02, -4.7473e-02,  5.6483e-02,  4.0877e-02,\n",
       "           5.7248e-03,  1.8163e-02,  7.6596e-02,  3.7377e-02, -1.1252e-02,\n",
       "           1.3043e-02, -5.8682e-02, -3.6347e-02,  3.5010e-03,  9.7805e-02,\n",
       "           3.8726e-02, -1.2310e-02,  9.7659e-02, -6.8331e-02,  1.0632e-03,\n",
       "          -9.4458e-02, -1.4493e-02, -1.3532e-02,  2.3896e-02, -4.5156e-02,\n",
       "          -1.3619e-01, -6.2215e-02, -3.0007e-02,  4.7280e-02,  6.5162e-04,\n",
       "          -1.5042e-01, -8.7743e-02,  9.7681e-02, -1.1526e-01, -2.7632e-02,\n",
       "           1.0115e-01,  3.2544e-02, -8.8746e-02,  5.5385e-02, -7.1320e-02,\n",
       "          -3.6575e-02, -5.9402e-02, -2.4116e-02, -1.8033e-01, -1.6053e-01,\n",
       "          -7.7947e-02, -1.2164e-02, -1.9725e-01,  1.0351e-01, -7.6390e-02,\n",
       "           1.7805e-01, -1.1102e-02,  4.3241e-02, -6.2349e-02,  9.6147e-02,\n",
       "          -5.7162e-03,  1.0685e-01, -2.1528e-01,  4.5599e-02,  1.0523e-02,\n",
       "          -1.7418e-01, -5.7822e-02,  8.5050e-02,  3.8846e-02,  4.0539e-02,\n",
       "           2.2227e-02,  1.5897e-01,  1.0960e-01,  6.4437e-02, -4.0271e-02,\n",
       "           2.9498e-02, -1.2470e-01, -9.6371e-02,  1.1955e-02, -2.1945e-02,\n",
       "           4.5971e-02,  7.8239e-02, -9.8217e-02, -5.4597e-02, -3.3991e-02,\n",
       "          -1.3818e-01,  1.7589e-01, -8.4333e-02, -2.8588e-02, -4.1043e-02,\n",
       "          -1.2127e-01, -3.4819e-02, -1.5191e-01, -1.2566e-01,  2.6170e-02,\n",
       "          -1.5302e-02, -1.2207e-01,  1.0305e-01, -4.9218e-02,  8.4594e-02,\n",
       "           2.0447e-05, -2.6918e-02, -1.4360e-01, -1.0624e-01,  5.5846e-02,\n",
       "          -1.1206e-01, -5.4105e-02, -8.5405e-02,  3.2969e-02, -1.7406e-02,\n",
       "          -1.3189e-01, -1.4039e-01,  2.9189e-02,  1.6892e-02, -7.5502e-02,\n",
       "           1.4524e-01,  3.4008e-02, -8.1082e-02,  1.1243e-01, -1.0914e-02,\n",
       "           2.5868e-03,  7.6140e-02, -3.7732e-02, -1.2560e-01,  3.0949e-02,\n",
       "          -3.8886e-02, -5.6541e-02,  3.2240e-02,  1.7402e-01,  2.4580e-03,\n",
       "          -5.7912e-02, -1.6412e-01,  2.6799e-03,  2.8299e-02, -8.2866e-02,\n",
       "          -8.9309e-02,  1.2101e-03,  1.3146e-02,  1.4962e-02, -4.7424e-02,\n",
       "          -4.7900e-02,  1.7070e-02, -6.7498e-02,  3.7668e-02,  1.4042e-01,\n",
       "           7.0179e-02,  2.0318e-02,  1.4591e-02, -6.1469e-02, -3.1885e-03,\n",
       "          -1.0004e-01,  3.7880e-02,  9.1898e-02, -2.3378e-02,  2.6257e-02,\n",
       "          -1.3054e-01, -9.9781e-03,  6.4160e-02, -6.2655e-02,  1.2260e-02,\n",
       "          -1.2261e-01,  6.1045e-02,  2.9073e-02, -1.0824e-01,  1.1402e-01,\n",
       "          -5.4764e-02, -4.2214e-02, -1.5395e-02,  3.6123e-02,  4.1416e-02,\n",
       "          -3.2018e-02,  1.3118e-01,  1.7914e-03,  9.1206e-02,  3.1424e-02,\n",
       "          -2.6212e-02,  6.7899e-02, -1.0664e-01, -5.7090e-02, -6.3168e-02,\n",
       "           1.1366e-01, -6.0466e-02, -1.3317e-01, -1.1533e-01, -7.0603e-02,\n",
       "          -1.3034e-01, -3.2385e-02,  7.5773e-02,  1.2085e-01,  2.8877e-02,\n",
       "          -4.2605e-02, -9.3801e-02, -7.3375e-02, -5.7796e-02, -7.6143e-02,\n",
       "           2.2727e-02, -5.6813e-02, -1.3440e-01,  1.7889e-04, -4.1620e-02,\n",
       "          -6.9523e-02,  2.5914e-02, -1.1023e-02,  5.2492e-02, -1.8669e-02,\n",
       "          -6.1048e-02, -7.3808e-02, -7.6840e-02,  8.5076e-02, -1.4939e-02,\n",
       "          -7.8625e-02, -1.0012e-01,  1.8623e-03,  1.1560e-02,  3.5798e-02,\n",
       "          -4.7299e-02, -1.2662e-01, -4.9809e-02,  6.3706e-02, -4.2877e-02,\n",
       "          -6.8176e-02,  1.4958e-02, -3.1859e-02, -5.5777e-02,  4.3282e-02,\n",
       "          -3.9351e-02,  9.5357e-02, -1.0152e-01,  3.9839e-02, -2.6674e-02,\n",
       "          -1.4130e-02, -5.0704e-03, -9.1193e-02, -5.2339e-02,  1.2622e-01,\n",
       "          -1.9684e-01,  4.7527e-02, -7.3768e-02, -3.3544e-02, -4.9169e-02,\n",
       "           2.0601e-02, -1.8273e-02, -5.2308e-02, -1.5599e-01, -2.6962e-02,\n",
       "          -2.6730e-02,  1.9871e-02, -9.0294e-02, -1.0917e-01, -1.8317e-01,\n",
       "           3.8450e-02, -1.4010e-02,  3.1341e-02, -1.1352e-01,  3.4254e-02,\n",
       "          -1.1350e-01,  8.1134e-02, -5.0881e-02,  3.9040e-02, -7.1862e-02,\n",
       "           3.3646e-02, -1.2871e-01,  2.9620e-02, -1.1709e-01,  1.7216e-01,\n",
       "          -2.0942e-03,  1.6917e-01,  1.4789e-02,  4.9889e-02,  2.1032e-02,\n",
       "           8.6715e-02, -5.0219e-02,  8.7727e-02, -1.6151e-01,  2.5281e-02,\n",
       "          -1.9135e-01,  1.2838e-01, -1.2148e-01,  1.1082e-01,  1.5701e-02,\n",
       "           9.3218e-02,  3.8380e-02,  1.4637e-01,  4.5015e-02,  1.0652e-01,\n",
       "           4.5613e-02,  2.7168e-02, -3.7644e-03, -1.4952e-02,  9.4368e-02,\n",
       "           1.5433e-02,  1.2609e-02,  8.6968e-02,  8.3371e-02,  1.4141e-01,\n",
       "           1.3235e-01,  6.1723e-02,  1.0118e-01,  4.5492e-02,  4.6018e-02,\n",
       "          -4.9002e-02,  2.7333e-02, -1.5687e-01,  1.4784e-01, -8.3702e-02,\n",
       "           9.0102e-02, -3.5845e-02,  7.8064e-02, -4.7458e-04,  3.2101e-02,\n",
       "          -6.7406e-02, -4.8774e-02, -1.1545e-01, -5.9316e-03,  7.6720e-02,\n",
       "          -8.4280e-02,  8.4104e-03, -7.6055e-02, -2.2781e-02, -1.1686e-01,\n",
       "          -8.0860e-02, -1.6795e-01,  4.6959e-02, -8.6218e-02, -8.7676e-02,\n",
       "          -8.6716e-02, -1.7836e-02,  1.9919e-02,  7.1432e-02, -4.7827e-02,\n",
       "           5.7498e-02,  9.3029e-03, -3.9485e-02,  4.1662e-02,  1.0197e-01,\n",
       "           4.1576e-02,  1.0801e-01,  5.5265e-03,  4.0805e-03, -5.2697e-02,\n",
       "           7.0422e-03,  1.3955e-02,  1.6927e-02, -1.3647e-02,  1.9774e-02,\n",
       "          -1.4308e-01,  8.6076e-02,  7.3545e-02,  1.0307e-01,  3.2762e-02,\n",
       "           5.1771e-02, -6.1299e-02,  1.2042e-01,  4.9620e-02,  3.0138e-02,\n",
       "           2.9848e-02,  4.0776e-02,  6.7556e-02,  4.2934e-02,  7.5988e-02,\n",
       "           9.7847e-02,  6.8075e-02, -3.3754e-03,  1.9039e-01, -2.9328e-02,\n",
       "           2.0715e-01,  4.1796e-02,  1.0971e-01,  3.5520e-02,  1.2761e-01,\n",
       "           5.3431e-03, -5.5335e-02,  5.4598e-02,  6.0470e-02,  3.4437e-02,\n",
       "           1.3705e-01, -1.0274e-01,  6.8725e-02, -1.9490e-02,  4.4522e-02,\n",
       "          -4.6900e-02,  5.7033e-02, -9.5651e-02,  6.4208e-02, -9.5099e-03,\n",
       "           1.3604e-01,  1.0100e-01, -6.6093e-03, -7.0831e-02,  5.7028e-02,\n",
       "          -6.8190e-02,  2.0532e-01, -6.3646e-02,  8.1710e-02, -2.1846e-01,\n",
       "           9.5250e-02, -1.7048e-02,  5.1936e-02, -1.6998e-01,  8.5768e-03,\n",
       "          -9.4339e-02,  2.5509e-01, -1.2140e-01,  2.2260e-01, -2.2940e-01,\n",
       "           1.6366e-02, -1.7735e-01, -5.2569e-02, -1.6865e-01,  1.0803e-01,\n",
       "          -2.0314e-01, -1.1296e-01, -1.5396e-01, -1.6601e-02, -9.8764e-02,\n",
       "          -1.4879e-01, -9.9324e-02, -2.3950e-03, -9.0845e-02, -9.5387e-02,\n",
       "          -4.2521e-03, -4.9213e-02, -1.0255e-01, -6.9110e-02, -1.0451e-01,\n",
       "          -2.3307e-02, -3.3282e-02, -9.7242e-02, -1.0504e-01, -5.5851e-02,\n",
       "           6.7688e-02, -4.6873e-02, -2.8833e-02, -1.5485e-01, -4.5200e-02,\n",
       "          -1.9572e-01, -1.6418e-02, -1.9241e-01, -1.8506e-01, -4.8089e-02,\n",
       "          -2.4848e-03, -4.0285e-02, -3.0789e-02,  1.7672e-02, -1.3146e-02,\n",
       "          -1.3432e-01,  5.4854e-02, -3.2026e-02, -2.4170e-02, -1.1276e-01,\n",
       "          -7.8569e-03, -1.1436e-01, -1.1100e-01, -8.0794e-03, -9.7509e-02,\n",
       "          -9.6986e-02,  1.5161e-03, -1.4672e-01,  2.5980e-02, -1.9548e-01,\n",
       "          -4.5613e-02, -9.9907e-02,  1.1157e-01, -1.0439e-01,  7.3713e-03,\n",
       "          -1.0699e-01, -3.2355e-02, -9.7598e-02,  6.6119e-02, -1.7152e-01,\n",
       "           5.1351e-02, -8.8871e-02,  5.0042e-02, -5.2017e-02, -1.2347e-01,\n",
       "          -2.0860e-01, -3.8765e-02, -1.5000e-01,  4.7145e-02,  2.1369e-02,\n",
       "           3.8175e-02, -1.0186e-01,  1.9837e-02, -1.6823e-01, -1.2248e-02,\n",
       "           4.6104e-02, -5.3864e-03, -5.8797e-02,  4.8773e-02, -7.5286e-02,\n",
       "          -5.2317e-03, -1.4942e-01, -5.3880e-02, -2.8860e-02,  7.2642e-02,\n",
       "          -1.3156e-01, -4.7507e-02, -4.6758e-03,  1.3308e-02, -9.5725e-02,\n",
       "          -1.5839e-01, -3.0709e-02,  4.5719e-02, -4.1324e-02,  2.1351e-02,\n",
       "          -2.8469e-02,  2.1890e-02, -7.5920e-02, -6.5599e-02,  1.4769e-02,\n",
       "           3.3452e-02,  7.5492e-02, -5.2642e-02, -7.7003e-02, -8.0186e-02,\n",
       "          -4.2528e-02, -4.6517e-03,  5.1747e-02,  9.6102e-02,  1.1348e-02,\n",
       "          -3.2558e-02, -6.4220e-02, -5.2443e-02, -5.7484e-02,  3.7249e-02,\n",
       "           9.6528e-02, -1.8387e-01, -9.4488e-02, -1.0945e-02, -5.4378e-02,\n",
       "          -2.4760e-02, -5.1283e-04, -1.4824e-02,  8.8953e-02, -6.2040e-02,\n",
       "           2.5414e-02, -1.3077e-02,  8.3750e-02, -2.8756e-03,  4.4266e-02,\n",
       "          -4.5938e-02,  7.1267e-02, -4.2939e-02,  3.1082e-02, -1.3593e-01,\n",
       "           6.8396e-02, -9.6999e-02, -1.5735e-01, -7.1366e-02,  4.6903e-02,\n",
       "          -1.2652e-01, -3.3970e-02, -1.4388e-01,  4.7196e-02, -6.5352e-02,\n",
       "           7.6768e-03, -1.2491e-01, -1.8568e-02, -1.4604e-01, -5.8706e-02,\n",
       "           3.4563e-03,  1.3411e-01, -2.4666e-01,  1.1393e-01, -5.9928e-02,\n",
       "          -1.3592e-01, -1.3225e-01, -4.2855e-02,  7.8864e-02,  1.4565e-02,\n",
       "          -6.6280e-02,  1.7971e-01, -7.5623e-02, -1.0218e-01, -1.2104e-01,\n",
       "           7.6228e-02, -1.4309e-01,  3.6522e-02,  1.0771e-02,  3.9348e-02,\n",
       "          -9.9486e-02,  1.6272e-01, -1.6163e-01,  2.2888e-02, -1.3643e-01,\n",
       "           7.0261e-02, -7.9911e-02, -3.9515e-02, -8.9686e-02, -5.0415e-02,\n",
       "          -2.9711e-01,  2.9131e-02, -1.8189e-01, -1.2838e-01, -1.0379e-01,\n",
       "          -4.2765e-02, -3.9703e-02,  1.2533e-01,  5.6494e-03, -7.3340e-02,\n",
       "          -1.9395e-01,  5.5994e-03, -2.2905e-01,  1.2298e-01, -1.7879e-01,\n",
       "           6.7734e-03, -1.1472e-01,  8.8164e-02, -1.0999e-01,  7.1036e-02,\n",
       "          -1.8741e-02,  4.1903e-02, -3.3723e-02,  6.7285e-02, -1.3163e-01,\n",
       "          -2.0493e-02, -9.1879e-02, -1.3705e-01, -1.7661e-01,  7.0612e-02,\n",
       "          -1.9370e-01, -1.1195e-02, -8.1226e-02,  1.9314e-03, -1.8583e-01,\n",
       "          -5.4073e-02, -1.0236e-01, -5.9863e-02,  5.2707e-02, -1.7489e-02,\n",
       "          -1.8027e-01, -1.8973e-02, -1.8306e-01, -5.9391e-02, -1.0738e-01,\n",
       "           1.6745e-02, -1.7804e-01, -1.2690e-01,  5.0785e-02,  3.6578e-02,\n",
       "          -1.2534e-01,  7.4148e-02, -6.4653e-02, -1.7827e-01, -7.2395e-02,\n",
       "           3.5962e-02, -2.6816e-01, -9.9922e-03,  4.7714e-03,  3.1245e-02,\n",
       "          -2.0466e-01,  1.1851e-01,  3.0304e-03,  6.8161e-02, -2.8166e-02,\n",
       "           1.1179e-01, -2.7518e-02, -1.2265e-01,  3.1768e-02,  2.9659e-02,\n",
       "          -1.5367e-01, -4.9375e-02,  2.6940e-02,  1.0876e-01, -7.4070e-02,\n",
       "          -2.0298e-02, -2.1753e-01, -2.6431e-01, -1.6464e-01,  5.0947e-03,\n",
       "          -1.1326e-01,  1.0873e-01, -3.5094e-02,  2.3003e-02,  7.0075e-02,\n",
       "          -8.1930e-02,  1.6541e-02, -1.4671e-01, -8.1064e-02,  2.4678e-02,\n",
       "          -1.9963e-01, -2.7809e-02, -5.1772e-02, -2.1010e-02,  2.9327e-02,\n",
       "           6.4900e-02, -1.3506e-01,  2.1012e-02, -1.9272e-01,  6.8502e-02,\n",
       "          -6.1859e-02, -1.2916e-01, -1.1950e-01, -1.5831e-02, -3.3163e-01,\n",
       "          -1.1188e-02, -4.7133e-02, -1.1184e-01, -2.0841e-02, -8.0926e-02,\n",
       "           8.3693e-02,  8.9444e-02, -8.8146e-02,  1.3333e-01,  7.4577e-03,\n",
       "          -1.6242e-01,  6.3178e-02, -1.8946e-02, -6.0366e-02,  9.2487e-02,\n",
       "          -2.1562e-03, -7.1422e-02,  3.4905e-02,  7.2994e-02, -1.3089e-01,\n",
       "           1.5658e-01,  6.3614e-02,  8.5995e-03, -4.3067e-02, -1.0194e-01,\n",
       "          -1.4506e-01, -4.3922e-02, -8.5730e-02, -1.8155e-01, -7.8441e-02,\n",
       "          -5.6326e-02, -1.9044e-02,  9.3270e-03, -1.1085e-02, -4.9621e-02,\n",
       "          -6.0122e-02,  7.6130e-03, -1.1041e-01, -8.8210e-02, -1.4419e-01,\n",
       "           1.2983e-02,  9.1301e-02,  8.3512e-02,  9.7290e-03, -1.1583e-01,\n",
       "          -2.6758e-02, -9.9334e-02,  7.9409e-03, -1.0248e-01, -6.8904e-02,\n",
       "          -8.4814e-02, -1.8050e-01, -1.2618e-01, -1.3756e-01, -3.5501e-03,\n",
       "          -1.7689e-01,  1.4149e-02,  3.1362e-02, -3.4195e-03, -1.4477e-01,\n",
       "          -8.4125e-02, -3.0633e-01, -8.8483e-02]], device='cuda:0'),\n",
       " 'discriminator_logits': tensor([[1.3743]], device='cuda:0')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e51da7-e8c1-4ef1-b053-eb67180df335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np # Added for token frequency debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cca90114-f477-423e-a6a2-874e8197dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_working_logits(model, outputs, tokenizer, max_length=2048, temperature=0.7, top_k=50, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Generate synthetic smart contract from the logits produced by your working forward pass.\n",
    "    This function uses the actual logits from your model outputs to generate tokens.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained SmartContractTransformer model\n",
    "        outputs: Outputs from your working forward pass containing logits\n",
    "        tokenizer: The tokenizer for decoding\n",
    "        max_length: Maximum generation length\n",
    "        temperature: Sampling temperature\n",
    "        top_k: Top-k sampling parameter\n",
    "        top_p: Nucleus sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Generated source code as string\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Get the logits from your working outputs\n",
    "    if 'logits' not in outputs:\n",
    "        print(\"Error: logits not found in model outputs.\")\n",
    "        print(\"Available keys:\", list(outputs.keys()))\n",
    "        return None\n",
    "    \n",
    "    logits = outputs['logits']  # [batch_size * seq_len, vocab_size]\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    \n",
    "    # Reshape logits to [batch_size, seq_len, vocab_size]\n",
    "    batch_size = 1  # Assuming single batch\n",
    "    seq_len = logits.size(0) // batch_size\n",
    "    vocab_size = logits.size(1)\n",
    "    \n",
    "    logits_reshaped = logits.view(batch_size, seq_len, vocab_size)\n",
    "    print(f\"Reshaped logits: {logits_reshaped.shape}\")\n",
    "    \n",
    "    # Apply temperature scaling\n",
    "    logits_reshaped = logits_reshaped / temperature\n",
    "    \n",
    "    # Apply top-k filtering\n",
    "    if top_k > 0:\n",
    "        top_k_logits, top_k_indices = torch.topk(logits_reshaped, top_k, dim=-1)\n",
    "        logits_mask = torch.full_like(logits_reshaped, float('-inf'))\n",
    "        logits_mask.scatter_(-1, top_k_indices, top_k_logits)\n",
    "        logits_reshaped = logits_mask\n",
    "    \n",
    "    # Apply nucleus sampling\n",
    "    if top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits_reshaped, descending=True, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(-1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits_reshaped = logits_reshaped.masked_fill(indices_to_remove, float('-inf'))\n",
    "    \n",
    "    # Sample tokens from the logits\n",
    "    probs = torch.softmax(logits_reshaped, dim=-1)\n",
    "    sampled_tokens = torch.multinomial(probs.view(-1, vocab_size), num_samples=1)\n",
    "    sampled_tokens = sampled_tokens.view(batch_size, seq_len)\n",
    "    \n",
    "    print(f\"Sampled tokens shape: {sampled_tokens.shape}\")\n",
    "    print(f\"Sampled tokens (first 20): {sampled_tokens[0, :20].cpu().numpy()}\")\n",
    "    print(f\"Sampled tokens (last 20): {sampled_tokens[0, -20:].cpu().numpy()}\")\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_tokens = sampled_tokens[0].cpu().numpy()\n",
    "    \n",
    "    # Filter out padding and special tokens\n",
    "    special_tokens = {0, 1, 2, 3}  # PAD, UNK, CLS, SEP\n",
    "    filtered_tokens = [t for t in generated_tokens if t not in special_tokens and t != tokenizer.pad_token_id]\n",
    "    \n",
    "    print(f\"Filtered tokens (first 20): {filtered_tokens[:20]}\")\n",
    "    print(f\"Total filtered tokens: {len(filtered_tokens)}\")\n",
    "    \n",
    "    if filtered_tokens:\n",
    "        generated_code = tokenizer.decode(\n",
    "            filtered_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        print(f\"Generated code length: {len(generated_code)}\")\n",
    "        return generated_code\n",
    "    else:\n",
    "        print(\"No meaningful tokens found!\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b371fb1e-2bbc-40aa-8a21-c955be3aebe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1978   131\n",
      " 50121 50118 50121 50118 49051 50121 50118  1437]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437 46486     4 40095  1640  9226 41014  4397 50121\n",
      " 50118  1437  1437  1437  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1978, 131, 50121, 50118, 50121, 50118, 49051, 50121, 50118, 1437]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 1580\n"
     ]
    }
   ],
   "source": [
    "generated_code = generate_from_working_logits(\n",
    "    model=model,\n",
    "    outputs=outputs,  # Your existing outputs with logits\n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9557307c-9227-4716-a67a-0ffa8645151d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pragma solidity ^0.4.24;\\r\\n\\r\\n/*\\r\\n    ��退�逎��了���交易亀�挜��，批戎。������仧�扎��不胡�要�������送�诡�。�戌我退���玎���掜接用了�OS區�敀��戼�戎��于�掜�����OS贬��敞�。��甴���逨�OS��\\r\\n     ����了��仨亡涯捡�仠明���尯接甡�。�从�掄\\uf39c賡��与���戯接甡�。�截��。�����釨亡����，�退�逑�掯掁��方��祿���瓈����挕�����鸡���\\uf344�app洣���泎��\\r\\n    �逈�專�Dapp接�����站釜�要������玦戨倀��贪��退��息�昳注輌戠甤昹君甀�立令昈秄逼�����戎甄我退��尶�注亁俴昅注����句叠����贪家�泎�站亶����OS贬���尴�贬�������O，��。�������尼���。�小。�戯��採漌。�\\n    ���玄�胄。��\\uf3b6�����仿甿.�\\r\\r\\n\\r\\n    � a specific-like exchange. to our a betting-freeze with no fees and no fees resources.\\r\\n     the same time, and investors, of the blockOS,. of and the block results of not on the EOS of the E of theOS that of\\r\\n     address information is E to be transparent and send. The betting of the lottery are not and can and can be transparent.\\r\\n     thanks to check,, we can be _-chain, we-chain currency,. It is the first isapp. in the first that\\r\\n   , the first DappVolume websites need to check of website status through the contract is, so you contract is no established to check map our website information.\\r\\n     can check if value (c every time of) is the the transfer; blockO of the transferOS account, on the transfer website.\\r\\n     function is not, to check public { call do not callable\\n*/\\r\\ncontract CashFlow {\\r\\n\\r\\n     public depositAddress = 0xbb02b2754bf0b1a2f54b70caDAb;d08bf2;\\r\\n     public;;\\r\\n\\r\\n     onlyOwner {\\r\\n        (owner == msg.sender, \"only owner,\\r\\n        ;\\r\\n    \\r\\n\\r\\n    () public payable {\\r\\n         = msg.sender;\\r\\n    \\r\\n\\r\\n    () public payable {\\r\\n        (address(this).balance > 0 ether) {\\r\\n            Address.transfer(this ether);\\r\\n      '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42a61755-6f4e-475c-8eb2-949844db5e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_target_ids(model, outputs, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate synthetic smart contract by decoding the target_ids from your working outputs.\n",
    "    This is the simplest approach - just decode the target_ids that your model was trained on.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained SmartContractTransformer model\n",
    "        outputs: Outputs from your working forward pass containing target_ids\n",
    "        tokenizer: The tokenizer for decoding\n",
    "    \n",
    "    Returns:\n",
    "        Generated source code as string\n",
    "    \"\"\"\n",
    "    if 'target_ids' not in outputs:\n",
    "        print(\"Error: target_ids not found in model outputs.\")\n",
    "        print(\"Available keys:\", list(outputs.keys()))\n",
    "        return None\n",
    "    \n",
    "    target_ids = outputs['target_ids']  # [batch_size * seq_len]\n",
    "    print(f\"Target IDs shape: {target_ids.shape}\")\n",
    "    print(f\"Target IDs (first 20): {target_ids[:20].cpu().numpy()}\")\n",
    "    print(f\"Target IDs (last 20): {target_ids[-20:].cpu().numpy()}\")\n",
    "    \n",
    "    # Decode the target tokens\n",
    "    generated_tokens = target_ids.cpu().numpy()\n",
    "    \n",
    "    # Filter out padding and special tokens\n",
    "    special_tokens = {0, 1, 2, 3}  # PAD, UNK, CLS, SEP\n",
    "    filtered_tokens = [t for t in generated_tokens if t not in special_tokens and t != tokenizer.pad_token_id]\n",
    "    \n",
    "    print(f\"Filtered target tokens (first 20): {filtered_tokens[:20]}\")\n",
    "    print(f\"Total filtered target tokens: {len(filtered_tokens)}\")\n",
    "    \n",
    "    if filtered_tokens:\n",
    "        generated_code = tokenizer.decode(\n",
    "            filtered_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        print(f\"Generated code length: {len(generated_code)}\")\n",
    "        return generated_code\n",
    "    else:\n",
    "        print(\"No meaningful target tokens found!\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4dc62fd-8ac4-4d59-a2c7-f378c70032e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target IDs shape: torch.Size([1023])\n",
      "Target IDs (first 20): [49795 50121 50118  1009  4253  8302    78 13031    23  1205   640 17517\n",
      " 43511     4  1020    15   294     6   719   504]\n",
      "Target IDs (last 20): [ 1437   671  1528   131 50121 50118  1437 35524 50121 50118 50121 50118\n",
      "  1437  5043  2394 10643  1640 44547 18134     2]\n",
      "Filtered target tokens (first 20): [49795, 50121, 50118, 1009, 4253, 8302, 78, 13031, 23, 1205, 640, 17517, 43511, 4, 1020, 15, 294, 6, 719, 504]\n",
      "Total filtered target tokens: 1022\n",
      "Generated code length: 2664\n"
     ]
    }
   ],
   "source": [
    "generated_code = generate_from_target_ids(\n",
    "    model=model,\n",
    "    outputs=outputs,  # Your existing outputs with target_ids\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23b77d4a-e718-418e-aac0-8971bbed93ca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/**\n",
      " * Source Code first verified at https://etherscan.io on Tuesday, December 18, 2018\n",
      " (UTC) */\n",
      "\n",
      "pragma solidity ^0.4.23;\n",
      "\n",
      "library SafeMath {\n",
      "\n",
      "  /**\n",
      "  * @dev Multiplies two numbers, throws on overflow.\n",
      "  */\n",
      "  function mul(uint256 a, uint256 b) internal pure returns (uint256 c) {\n",
      "\n",
      "    if (a == 0) {\n",
      "      return 0;\n",
      "    }\n",
      "\n",
      "    c = a * b;\n",
      "    assert(c / a == b);\n",
      "    return c;\n",
      "  }\n",
      "\n",
      "  /**\n",
      "  * @dev Integer division of two numbers, truncating the quotient.\n",
      "  */\n",
      "  function div(uint256 a, uint256 b) internal pure returns (uint256) {\n",
      "\n",
      "    return a / b;\n",
      "  }\n",
      "\n",
      "  /**\n",
      "  * @dev Subtracts two numbers, throws on overflow (i.e. if subtrahend is greater than minuend).\n",
      "  */\n",
      "  function sub(uint256 a, uint256 b) internal pure returns (uint256) {\n",
      "    assert(b <= a);\n",
      "    return a - b;\n",
      "  }\n",
      "\n",
      "  /**\n",
      "  * @dev Adds two numbers, throws on overflow.\n",
      "  */\n",
      "  function add(uint256 a, uint256 b) internal pure returns (uint256 c) {\n",
      "    c = a + b;\n",
      "    assert(c >= a);\n",
      "    return c;\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "contract ERC20Basic {\n",
      "    \n",
      "  function totalSupply() public view returns (uint256);\n",
      "  function balanceOf(address who) public view returns (uint256);\n",
      "  function transfer(address to, uint256 value) public returns (bool);\n",
      "  event Transfer(address indexed from, address indexed to, uint256 value);\n",
      "  \n",
      "}\n",
      "\n",
      "contract ERC20 is ERC20Basic {\n",
      "    \n",
      "  function allowance(address owner, address spender)\n",
      "    public view returns (uint256);\n",
      "\n",
      "  function transferFrom(address from, address to, uint256 value)\n",
      "    public returns (bool);\n",
      "\n",
      "  function approve(address spender, uint256 value) public returns (bool);\n",
      "  event Approval(\n",
      "    address indexed owner,\n",
      "    address indexed spender,\n",
      "    uint256 value\n",
      "  );\n",
      "}\n",
      "\n",
      "contract DetailedERC20 is ERC20 {\n",
      "  string public name;\n",
      "  string public symbol;\n",
      "  uint8 public decimals;\n",
      "\n",
      "  constructor(string _name, string _symbol, uint8 _decimals) public {\n",
      "    name = _name;\n",
      "    symbol = _symbol;\n",
      "    decimals = _decimals;\n",
      "  }\n",
      "}\n",
      "\n",
      "/**\n",
      " * @title 实现ERC20基本合约的接口 \n",
      " * @dev 基本的StandardToken，不包含allowances.\n",
      " */\n",
      "contract BasicToken is ERC20Basic {\n",
      "  using SafeMath for uint256;\n",
      "\n",
      "  mapping(address => uint256) balances;\n",
      "\n",
      "  uint256 totalSupply_;\n",
      "  \n",
      "  function totalSupply() public view returns (uint256) {\n",
      "    return totalSupply_;\n",
      "  }\n",
      "\n",
      "  function transfer(address _to, uint256 _value) public returns (bool) {\n",
      "    require(_to!= address(0));\n",
      "    require(_value <= balances[msg.sender]);\n",
      "    balances[msg.sender] = balances[msg.sender].sub(_value);\n",
      "    balances[_to] = balances[_to].add(_value);\n",
      "    emit Transfer(msg.sender, _to, _value);\n",
      "    return true;\n",
      "  }\n",
      "\n",
      "  function balanceOf(address _\n"
     ]
    }
   ],
   "source": [
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d982fb-3102-4eb6-9d7f-3b3f818fedb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddf0e7-946e-4eda-a0f3-b1e62155ee35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08a52a-ee79-4b57-a362-6696bf4eb9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113de546-ab5b-4dd2-8408-f5ee3e1ddee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347150e-28dd-43ee-bf01-d17c8b6e3a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff6d49-768d-435b-b443-26a8b4aa6195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a206c-de46-4dd3-8b68-6986f69bb723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b60559bd-91f9-47ee-a942-1a5594b3583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78d09f-d9f7-4015-8922-f64b5a308ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Generating Smart Contracts from Validation Dataset\n",
      "============================================================\n",
      "📁 Output folder: /home/m20180848/smrt-transformer/07.training-model/sythetic_smart_contracts\n",
      "\n",
      "🎯 Generating 506 contracts from validation dataset...\n",
      "\n",
      "🎯 Generating 506 contracts from validation dataset...\n",
      "📝 Processing contract 2: 4039 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1549   131\n",
      " 50121 50118 50121 50118 47888 19233 21109 44392]\n",
      "Sampled tokens (last 20): [19434  4397 50121 50118  1437  1437  1437  1437  1437  1437  1437   671\n",
      "  1528   131 50121 50118  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1549, 131, 50121, 50118, 50121, 50118, 47888, 19233, 21109, 44392]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2766\n",
      "  ✅ Generated contract for batch 1: 2766 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0001_generated.sol\n",
      "📝 Processing contract 3: 2735 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1978   131\n",
      " 50121 50118 50121 50118 31938 33781 10020 45643]\n",
      "Sampled tokens (last 20): [50118  1437  1437  1437  1437  1437  1437  1437  1437  5457 18134 13650\n",
      "   131 50121 50118  1437  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1978, 131, 50121, 50118, 50121, 50118, 31938, 33781, 10020, 45643]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2056\n",
      "  ✅ Generated contract for batch 2: 2056 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0002_generated.sol\n",
      "📝 Processing contract 4: 4850 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1549   131\n",
      " 50121 50118 50121 50118 50121 50118 31938  3125]\n",
      "Sampled tokens (last 20): [    4 23687     4 16096 49095  1836  2055   204  4397 50121 50118  1437\n",
      "  1437  1437  1437  1437  1437  1437  1437   131]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1549, 131, 50121, 50118, 50121, 50118, 50121, 50118, 31938, 3125]\n",
      "Total filtered tokens: 1023\n",
      "Generated code length: 2570\n",
      "  ✅ Generated contract for batch 3: 2570 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0003_generated.sol\n",
      "📝 Processing contract 5: 3039 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1366   131\n",
      " 50121 50118 49051 50121 50118  1009 11945  1414]\n",
      "Sampled tokens (last 20): [1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437 1437\n",
      " 1437 1437 1437 1437 1437    2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1366, 131, 50121, 50118, 49051, 50121, 50118, 1009, 11945, 1414]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2137\n",
      "  ✅ Generated contract for batch 4: 2137 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0004_generated.sol\n",
      "📝 Processing contract 6: 4922 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [49795 50121 50118  1009  4253  8302    78 13031    23  1205   640 17517\n",
      " 43511     4  1020    15   294     6   199   504]\n",
      "Sampled tokens (last 20): [ 1437   671  1528   131 50121 50118  1437 35524 50121 50118 50121 50118\n",
      "  1437  5043  2394 10643  1640 44547 18134     2]\n",
      "Filtered tokens (first 20): [49795, 50121, 50118, 1009, 4253, 8302, 78, 13031, 23, 1205, 640, 17517, 43511, 4, 1020, 15, 294, 6, 199, 504]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2629\n",
      "  ✅ Generated contract for batch 5: 2629 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0005_generated.sol\n",
      "📝 Processing contract 7: 2551 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306   479   466   131\n",
      " 50121 50118 31674 13793 44853 25522 50121 50118]\n",
      "Sampled tokens (last 20): [ 7232 46386  1215  4182  7232   742  5457 18134 19434   131 50121 50118\n",
      "  1437  1437  1437  1437  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 479, 466, 131, 50121, 50118, 31674, 13793, 44853, 25522, 50121, 50118]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2325\n",
      "  ✅ Generated contract for batch 6: 2325 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0006_generated.sol\n",
      "📝 Processing contract 8: 2354 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  2481   131\n",
      " 50121 50118 50121 50118 47888   381  5199   844]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 2481, 131, 50121, 50118, 50121, 50118, 47888, 381, 5199, 844]\n",
      "Total filtered tokens: 818\n",
      "Generated code length: 2352\n",
      "  ✅ Generated contract for batch 7: 2352 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0007_generated.sol\n",
      "📝 Processing contract 9: 4994 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1366   131\n",
      " 50121 50118 50121 50118 42326 29483 46156 50121]\n",
      "Sampled tokens (last 20): [46161 20410  1640 44547 46161 49315 35122  1220   131 50121 50118 50121\n",
      " 50118 50121 50118  1437  1437  1437  5043     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1366, 131, 50121, 50118, 50121, 50118, 42326, 29483, 46156, 50121]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2727\n",
      "  ✅ Generated contract for batch 8: 2727 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0008_generated.sol\n",
      "📝 Processing contract 10: 1942 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249 50117 35227 35227   288     4   306\n",
      "     4  2146 50117   131 50117 50117 50117 50117]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 50117, 35227, 35227, 288, 4, 306, 4, 2146, 50117, 131, 50117, 50117, 50117, 50117]\n",
      "Total filtered tokens: 998\n",
      "Generated code length: 1852\n",
      "  ✅ Generated contract for batch 9: 1852 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0009_generated.sol\n",
      "📝 Processing contract 11: 470 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4   288   131\n",
      " 50121 50118 50121 50118 31938 30466   406 25522]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 288, 131, 50121, 50118, 50121, 50118, 31938, 30466, 406, 25522]\n",
      "Total filtered tokens: 218\n",
      "Generated code length: 442\n",
      "  ✅ Generated contract for batch 10: 442 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0010_generated.sol\n",
      "📝 Processing contract 12: 598 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1646   131\n",
      " 50121 50118 31938 31239   133 43623 25522 50121]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1646, 131, 50121, 50118, 31938, 31239, 133, 43623, 25522, 50121]\n",
      "Total filtered tokens: 246\n",
      "Generated code length: 582\n",
      "  ✅ Generated contract for batch 11: 582 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0011_generated.sol\n",
      "📝 Processing contract 13: 4852 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1244   131\n",
      " 50121 50118 50121 50118 50121 50118 31938  3125]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437  1437  1437  1437  1640 48593     4 23687     4\n",
      " 16096 49095  1836  2055   204  4397 50121     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1244, 131, 50121, 50118, 50121, 50118, 50121, 50118, 31938, 3125]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2593\n",
      "  ✅ Generated contract for batch 12: 2593 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0012_generated.sol\n",
      "📝 Processing contract 14: 2513 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1225   131\n",
      " 50121 50118 50121 50121 50118 31938 29464 25522]\n",
      "Sampled tokens (last 20): [10975 48593     4    29  7232 46386  1215  4182  7232   742  5457 18134\n",
      " 31233   131 50121 50118  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1225, 131, 50121, 50118, 50121, 50121, 50118, 31938, 29464, 25522]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2171\n",
      "  ✅ Generated contract for batch 13: 2171 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0013_generated.sol\n",
      "📝 Processing contract 15: 3047 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1549   131\n",
      " 50121 50118 50121 50118 47888 19233 25522 50121]\n",
      "Sampled tokens (last 20): [37127  3070 11582 18537  8210 43251  8210 43251  4394 14285 37127  3726\n",
      " 49257  7487  9253 21402 49257  6382 46890     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1549, 131, 50121, 50118, 50121, 50118, 47888, 19233, 25522, 50121]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 1698\n",
      "  ✅ Generated contract for batch 14: 1698 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0014_generated.sol\n",
      "📝 Processing contract 16: 5001 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1366   131\n",
      " 50121 50118 50121 50118 42326 29483 46156 50121]\n",
      "Sampled tokens (last 20): [46161 49315 35122  1220   131 50121 50118 50121 50118 50121 50118  1437\n",
      "  1437  1437  1437  2937 43048   285 25522     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1366, 131, 50121, 50118, 50121, 50118, 42326, 29483, 46156, 50121]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2745\n",
      "  ✅ Generated contract for batch 15: 2745 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0015_generated.sol\n",
      "📝 Processing contract 17: 3717 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1978   131\n",
      " 50121 50118 50121 50118 49795 50121 50118  1009]\n",
      "Sampled tokens (last 20): [25522 50121 50118 50117 50117 47636 49604   560  8061   321  4397  1437\n",
      "  1437 50118 50117  1437  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1978, 131, 50121, 50118, 50121, 50118, 49795, 50121, 50118, 1009]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2534\n",
      "  ✅ Generated contract for batch 16: 2534 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0016_generated.sol\n",
      "📝 Processing contract 18: 1597 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1646   131\n",
      " 50121 50118 50121 50118 31938   163 10227  1215]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1646, 131, 50121, 50118, 50121, 50118, 31938, 163, 10227, 1215]\n",
      "Total filtered tokens: 881\n",
      "Generated code length: 1434\n",
      "  ✅ Generated contract for batch 17: 1434 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0017_generated.sol\n",
      "📝 Processing contract 19: 2574 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1978   131\n",
      " 50121 50118 50121 50118 42326 50121 50118 42326]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437  1437  1437 50121 50118  1437  1437  1437  1437\n",
      "  1437  1437  1437  1437 27625  1720 49371     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1978, 131, 50121, 50118, 50121, 50118, 42326, 50121, 50118, 42326]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2097\n",
      "  ✅ Generated contract for batch 18: 2097 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0018_generated.sol\n",
      "📝 Processing contract 20: 1094 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1244   131\n",
      " 50121 50118 50121 50118 31938 19362 50121 50118]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1244, 131, 50121, 50118, 50121, 50118, 31938, 19362, 50121, 50118]\n",
      "Total filtered tokens: 566\n",
      "Generated code length: 1059\n",
      "  ✅ Generated contract for batch 19: 1059 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0019_generated.sol\n",
      "📝 Processing contract 21: 1225 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4   844   131\n",
      " 50121 50118 50121 50118 31938    20 20178 50121]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 844, 131, 50121, 50118, 50121, 50118, 31938, 20, 20178, 50121]\n",
      "Total filtered tokens: 569\n",
      "Generated code length: 1141\n",
      "  ✅ Generated contract for batch 20: 1141 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0020_generated.sol\n",
      "📝 Processing contract 22: 5642 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1366   131\n",
      " 50121 50118 50121 50118 31938 13793 44853 25522]\n",
      "Sampled tokens (last 20): [ 288 1178  288  597  398 1749 3506 3506  347  401  597  597  438 5457\n",
      "  401  131  250  398  288    2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1366, 131, 50121, 50118, 50121, 50118, 31938, 13793, 44853, 25522]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2827\n",
      "  ✅ Generated contract for batch 21: 2827 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0021_generated.sol\n",
      "📝 Processing contract 23: 3227 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1244   131\n",
      " 50121 50118 50121 50118 47888 19233 21109 44392]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437  1437  1437  1437  1437  1437  1437 50118  1437\n",
      "  1437  1437  1437  1437  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1244, 131, 50121, 50118, 50121, 50118, 47888, 19233, 21109, 44392]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2620\n",
      "  ✅ Generated contract for batch 22: 2620 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0022_generated.sol\n",
      "📝 Processing contract 24: 4509 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1646   131\n",
      " 50121 50118 50121 50118 49795 50121 50118  1009]\n",
      "Sampled tokens (last 20): [ 1640 44547 18134   560     6 49315 24554 18134 19434    43   285  2886\n",
      "    36 46787    43 25522 50121 50118  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1646, 131, 50121, 50118, 50121, 50118, 49795, 50121, 50118, 1009]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2467\n",
      "  ✅ Generated contract for batch 23: 2467 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0023_generated.sol\n",
      "📝 Processing contract 25: 1179 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1244   131\n",
      " 50121 50118 50121 50118 31938 25864  1215  2670]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1244, 131, 50121, 50118, 50121, 50118, 31938, 25864, 1215, 2670]\n",
      "Total filtered tokens: 548\n",
      "Generated code length: 1074\n",
      "  ✅ Generated contract for batch 24: 1074 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0024_generated.sol\n",
      "📝 Processing contract 26: 4029 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1978   131\n",
      " 50121 50118 50121 50118 50121 50118 31674 13793]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437  2937 40839 39953  1640 44547    92 46182    43\n",
      "   129 46182   285 25522 50121 50118  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1978, 131, 50121, 50118, 50121, 50118, 50121, 50118, 31674, 13793]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2285\n",
      "  ✅ Generated contract for batch 25: 2285 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0025_generated.sol\n",
      "📝 Processing contract 27: 4494 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1922   131\n",
      " 50121 50118 50121 50118 49795 50121 50118  1009]\n",
      "Sampled tokens (last 20): [    4 19434   131 19434   922  1640 19434 26229  2630   322 26037  1640\n",
      " 19434 18121  2630   322 50121 50118 50121     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1922, 131, 50121, 50118, 50121, 50118, 49795, 50121, 50118, 1009]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2597\n",
      "  ✅ Generated contract for batch 26: 2597 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0026_generated.sol\n",
      "📝 Processing contract 28: 2309 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1978   131\n",
      " 50121 50118 50121 50118 42326  8655    35  3749]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1978, 131, 50121, 50118, 50121, 50118, 42326, 8655, 35, 3749]\n",
      "Total filtered tokens: 863\n",
      "Generated code length: 2137\n",
      "  ✅ Generated contract for batch 27: 2137 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0027_generated.sol\n",
      "📝 Processing contract 29: 3784 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1244   131\n",
      " 50121 50118 50121 50118 50121 50118 42326  1437]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437  1437  1437  1437  1437 50121 50118  1437  1437\n",
      "  1437  1437 50121 50118 50121 50121 50118     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1244, 131, 50121, 50118, 50121, 50118, 50121, 50118, 42326, 1437]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2109\n",
      "  ✅ Generated contract for batch 28: 2109 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0028_generated.sol\n",
      "📝 Processing contract 30: 373 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1366   131\n",
      " 50121 50118 50121 50118 31938 14064 47003 25522]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1366, 131, 50121, 50118, 50121, 50118, 31938, 14064, 47003, 25522]\n",
      "Total filtered tokens: 149\n",
      "Generated code length: 338\n",
      "  ✅ Generated contract for batch 29: 338 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0029_generated.sol\n",
      "📝 Processing contract 31: 4407 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  2146   131\n",
      " 50121 50118 50121 50118 31674 13793 44853 25522]\n",
      "Sampled tokens (last 20): [ 1437  1437   671  1220 49895 14527 46386  1215  4182  7232 44082 50121\n",
      " 50118  1437 35524 50121 50118 50121 50118     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 2146, 131, 50121, 50118, 50121, 50118, 31674, 13793, 44853, 25522]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2857\n",
      "  ✅ Generated contract for batch 30: 2857 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0030_generated.sol\n",
      "📝 Processing contract 32: 4357 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1244   131\n",
      " 50121 50118 50121 50118 31938  2211  5881 45643]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  4254 19488 50121 50118  1437  1437  1437  1437  1437\n",
      "  1437  1437  1437 49895  7761 46386 48593     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1244, 131, 50121, 50118, 50121, 50118, 31938, 2211, 5881, 45643]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2595\n",
      "  ✅ Generated contract for batch 31: 2595 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0031_generated.sol\n",
      "📝 Processing contract 33: 4481 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1244   131\n",
      " 50121 50118 49795 50121 50118  3226 50121 50118]\n",
      "Sampled tokens (last 20): [  510 11180  5457 49049     4 19434  3226   119   922  1640  1866   322\n",
      " 26037  1640  1866  4397 50121 50118 50121     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1244, 131, 50121, 50118, 49795, 50121, 50118, 3226, 50121, 50118]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2844\n",
      "  ✅ Generated contract for batch 32: 2844 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0032_generated.sol\n",
      "📝 Processing contract 34: 1546 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1978   131\n",
      " 50121 50118 50121 50118 49795 50121 50118  1009]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1978, 131, 50121, 50118, 50121, 50118, 49795, 50121, 50118, 1009]\n",
      "Total filtered tokens: 575\n",
      "Generated code length: 1497\n",
      "  ✅ Generated contract for batch 33: 1497 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0033_generated.sol\n",
      "📝 Processing contract 35: 4795 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [42326 43649  1355    13     5   455   381  5199   291 29464  2526 50121\n",
      " 50118 42326  1205   640 48340     4   175    73]\n",
      "Sampled tokens (last 20): [ 1437  1437 24554 18134 45696 48302     6 50121 50118  1437  1437  1437\n",
      "  1437  1437  1437  1437  6755 18134 46657     2]\n",
      "Filtered tokens (first 20): [42326, 43649, 1355, 13, 5, 455, 381, 5199, 291, 29464, 2526, 50121, 50118, 42326, 1205, 640, 48340, 4, 175, 73]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 3099\n",
      "  ✅ Generated contract for batch 34: 3099 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0034_generated.sol\n",
      "📝 Processing contract 36: 2073 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  2481   131\n",
      " 50121 50118 50121 50118 31938  1308 45643 25522]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 2481, 131, 50121, 50118, 50121, 50118, 31938, 1308, 45643, 25522]\n",
      "Total filtered tokens: 822\n",
      "Generated code length: 2016\n",
      "  ✅ Generated contract for batch 35: 2016 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0035_generated.sol\n",
      "📝 Processing contract 37: 4766 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4   996   131\n",
      " 50121 50118 50121 50118 31674 13793 44853 25522]\n",
      "Sampled tokens (last 20): [ 1437  1437  1528   131 50121 50118  1437  1437  1437 35524 50121 50118\n",
      " 50121 50118  1437  1437  1437  5043  2937     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 996, 131, 50121, 50118, 50121, 50118, 31674, 13793, 44853, 25522]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2381\n",
      "  ✅ Generated contract for batch 36: 2381 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0036_generated.sol\n",
      "📝 Processing contract 38: 4273 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1549   131\n",
      " 50121 50118 50121 50118 47888 19233 21109 44392]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437  1437  1437  1437  1437 40095 49604  7761     6\n",
      " 18134   560     6 18134 19434  4397 50121     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1549, 131, 50121, 50118, 50121, 50118, 47888, 19233, 21109, 44392]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2629\n",
      "  ✅ Generated contract for batch 37: 2629 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0037_generated.sol\n",
      "📝 Processing contract 39: 3639 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306   479  1366   131\n",
      " 50121 50118 50121 50118 31938 35562 35038 25522]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437  1437  1437  1437  1437  1437  1437  1437 49371\n",
      "   923 10975   118  8174 31233   131 50121     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 479, 1366, 131, 50121, 50118, 50121, 50118, 31938, 35562, 35038, 25522]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2052\n",
      "  ✅ Generated contract for batch 38: 2052 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0038_generated.sol\n",
      "📝 Processing contract 40: 2037 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  2146   131\n",
      " 50121 50118 50121 50118 31938 23626 15721 25522]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 2146, 131, 50121, 50118, 50121, 50118, 31938, 23626, 15721, 25522]\n",
      "Total filtered tokens: 726\n",
      "Generated code length: 1911\n",
      "  ✅ Generated contract for batch 39: 1911 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0039_generated.sol\n",
      "📝 Processing contract 41: 1434 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1244   131\n",
      " 50121 50118 50121 50118 31938 45655 17339 25522]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1244, 131, 50121, 50118, 50121, 50118, 31938, 45655, 17339, 25522]\n",
      "Total filtered tokens: 571\n",
      "Generated code length: 1348\n",
      "  ✅ Generated contract for batch 40: 1348 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0040_generated.sol\n",
      "📝 Processing contract 42: 1217 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1978   131\n",
      " 50121 50118 50121 50118 50121 50118 50121 50118]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1978, 131, 50121, 50118, 50121, 50118, 50121, 50118, 50121, 50118]\n",
      "Total filtered tokens: 527\n",
      "Generated code length: 1177\n",
      "  ✅ Generated contract for batch 41: 1177 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0041_generated.sol\n",
      "📝 Processing contract 43: 3344 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1549   131\n",
      " 50121 50118 50121 50118 49104   787 14691 19981]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437  1437  1437  1437 10643 10975 48593     4    29\n",
      "  7232   742 49371 49049  7232   131   131 50121]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1549, 131, 50121, 50118, 50121, 50118, 49104, 787, 14691, 19981]\n",
      "Total filtered tokens: 1023\n",
      "Generated code length: 2532\n",
      "  ✅ Generated contract for batch 42: 2532 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0042_generated.sol\n",
      "📝 Processing contract 44: 3332 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1549   131\n",
      " 50121 50118 50121 50118 47888 29464 25522 50121]\n",
      "Sampled tokens (last 20): [ 1437 48517  5457  1528 49604 46657  4397 50121  1437  1437  1437  1437\n",
      "  1437 50121 50118  1437  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1549, 131, 50121, 50118, 50121, 50118, 47888, 29464, 25522, 50121]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2217\n",
      "  ✅ Generated contract for batch 43: 2217 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0043_generated.sol\n",
      "📝 Processing contract 45: 4968 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1366   131\n",
      " 50121 50118 50121 50118 42326 29483 46156 50121]\n",
      "Sampled tokens (last 20): [  131 50121 50118 50121 50118 50121 50118  1437  1437  1437  5043   248\n",
      " 11979 43048   285 25522 50121 50118  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1366, 131, 50121, 50118, 50121, 50118, 42326, 29483, 46156, 50121]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2726\n",
      "  ✅ Generated contract for batch 44: 2726 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0044_generated.sol\n",
      "📝 Processing contract 46: 5242 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1366   131\n",
      " 50121 50118 50121 50118 50121 50118 50121 50118]\n",
      "Sampled tokens (last 20): [46787    43 25522 50121 50118  1437  1437  1437  1437 10975 48593     4\n",
      "    29  7232 46386  1215  4182  7232   742     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1366, 131, 50121, 50118, 50121, 50118, 50121, 50118, 50121, 50118]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2866\n",
      "  ✅ Generated contract for batch 45: 2866 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0045_generated.sol\n",
      "📝 Processing contract 47: 3990 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [49795 50121 50118  1009 23055 13979    13 14925    23 35562 43511     4\n",
      "  1020    15   954    12  2663    12  2663 50121]\n",
      "Sampled tokens (last 20): [44853     4 22725 23055  1640 30821 10643 10975 48593     4    29  7232\n",
      "  7479 18134 19434  4397  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [49795, 50121, 50118, 1009, 23055, 13979, 13, 14925, 23, 35562, 43511, 4, 1020, 15, 954, 12, 2663, 12, 2663, 50121]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2401\n",
      "  ✅ Generated contract for batch 46: 2401 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0046_generated.sol\n",
      "📝 Processing contract 48: 4270 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1558   131\n",
      " 50121 50118 50121 50118 50121 50118 31938   381]\n",
      "Sampled tokens (last 20): [  131 50121 50118 50117 50117 46552  1640   288  1178   288  1178 49049\n",
      "     4    29  7232     6 28066  1215 45531     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1558, 131, 50121, 50118, 50121, 50118, 50121, 50118, 31938, 381]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2758\n",
      "  ✅ Generated contract for batch 47: 2758 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0047_generated.sol\n",
      "📝 Processing contract 49: 4705 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4   844   131\n",
      " 50121 50118 50121 50118 42326 44706 15057 13793]\n",
      "Sampled tokens (last 20): [ 1437 49604   560 49333   321  1178   288  4397 50121 50118 50121  1437\n",
      "  1437  1437  1437  1437 49604 30821 10643     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 844, 131, 50121, 50118, 50121, 50118, 42326, 44706, 15057, 13793]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2496\n",
      "  ✅ Generated contract for batch 48: 2496 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0048_generated.sol\n",
      "📝 Processing contract 50: 4519 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4   398   131\n",
      " 50121 50118 31938 19233 21109 44392 25522  5043]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437  1437  2886  2886    36 46787  1282    43 25522\n",
      " 50121 50118  1437  1437  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 398, 131, 50121, 50118, 31938, 19233, 21109, 44392, 25522, 5043]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2533\n",
      "  ✅ Generated contract for batch 49: 2533 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0049_generated.sol\n",
      "📝 Processing contract 51: 3657 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1244   131\n",
      " 50121 50118 50121 50118 50121 50118 31674 13793]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437  1437  1640  8667  5332 10975 48593     4    29\n",
      "  7232   742 49095 18134 19434  4397 50121     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1244, 131, 50121, 50118, 50121, 50118, 50121, 50118, 31674, 13793]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2152\n",
      "  ✅ Generated contract for batch 50: 2152 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0050_generated.sol\n",
      "📝 Processing contract 52: 1939 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1225   131\n",
      " 50121 50118 50121 50118 31938   381  5199   844]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1225, 131, 50121, 50118, 50121, 50118, 31938, 381, 5199, 844]\n",
      "Total filtered tokens: 731\n",
      "Generated code length: 1905\n",
      "  ✅ Generated contract for batch 51: 1905 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0051_generated.sol\n",
      "📝 Processing contract 53: 4579 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1978   131\n",
      " 50121 50118 50121 50118 50121 50118 31938 13793]\n",
      "Sampled tokens (last 20): [  560 48601 50121 50118  1437  1437  1437  1437  1437  1437  1437  1437\n",
      "    36   328   506 38254 43767 49895  7761     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1978, 131, 50121, 50118, 50121, 50118, 50121, 50118, 31938, 13793]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2303\n",
      "  ✅ Generated contract for batch 52: 2303 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0052_generated.sol\n",
      "📝 Processing contract 54: 4317 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1922   131\n",
      " 50121 50118 50121 50118 50121 50118 31674 13793]\n",
      "Sampled tokens (last 20): [ 1437  1437 49604 19434 49230   595 10975 48593     4    29  7232 48601\n",
      " 50121 50118  1437  1437  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1922, 131, 50121, 50118, 50121, 50118, 50121, 50118, 31674, 13793]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2315\n",
      "  ✅ Generated contract for batch 53: 2315 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0053_generated.sol\n",
      "📝 Processing contract 55: 5250 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1558   131\n",
      " 50121 50118 50121 50118 31938   381  5199   844]\n",
      "Sampled tokens (last 20): [19434  4397 50121 50118  1437  1437  1437  1437 49604  7761     6 18134\n",
      "   560     6 18134 19434  4397 50121 50118     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1558, 131, 50121, 50118, 50121, 50118, 31938, 381, 5199, 844]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2786\n",
      "  ✅ Generated contract for batch 54: 2786 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0054_generated.sol\n",
      "📝 Processing contract 56: 2706 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1646   131\n",
      " 50121 50118 50121 50118 31938 11056 45643 25522]\n",
      "Sampled tokens (last 20): [50118  1437  1437  1437  1437  1437  1437  1437  1437   757  1536  5457\n",
      "   504   131 50121 50118  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1646, 131, 50121, 50118, 50121, 50118, 31938, 11056, 45643, 25522]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2366\n",
      "  ✅ Generated contract for batch 55: 2366 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0055_generated.sol\n",
      "📝 Processing contract 57: 4009 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [49795 50121 50118  1009   713    16    45   129   697 18176 19460 15739\n",
      "     4    20   986  1355    16    45   341   143]\n",
      "Sampled tokens (last 20): [46182   742  5457  1803     4 30695   131 50121 50118  1437  1437  1437\n",
      "  1437 50121 50118  1437  1437  1437  1437     2]\n",
      "Filtered tokens (first 20): [49795, 50121, 50118, 1009, 713, 16, 45, 129, 697, 18176, 19460, 15739, 4, 20, 986, 1355, 16, 45, 341, 143]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2245\n",
      "  ✅ Generated contract for batch 56: 2245 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0056_generated.sol\n",
      "📝 Processing contract 58: 3524 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1922   131\n",
      " 50121 50118 50121 50118 50121 50118 31938 40702]\n",
      "Sampled tokens (last 20): [50118  1437  1437  1437  1437  1437 50118  1437  1437  1437  1437 50121\n",
      " 50118 50121  1437  1437  1437 50121 50118     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1922, 131, 50121, 50118, 50121, 50118, 50121, 50118, 31938, 40702]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2397\n",
      "  ✅ Generated contract for batch 57: 2397 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0057_generated.sol\n",
      "📝 Processing contract 59: 3876 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  2146   131\n",
      " 50121 50118 50121 50118 31938  2164 25522 50121]\n",
      "Sampled tokens (last 20): [  277     7  1930 22121     7  7218     7 50118  1437  1437  1437  1437\n",
      "   787  1437  1437 46669 18134 19434     4     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 2146, 131, 50121, 50118, 50121, 50118, 31938, 2164, 25522, 50121]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2550\n",
      "  ✅ Generated contract for batch 58: 2550 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0058_generated.sol\n",
      "📝 Processing contract 60: 4309 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1978   131\n",
      " 50121 50118 50121 50118 50121 50118 31938 13793]\n",
      "Sampled tokens (last 20): [49895   560 48601 50121 50118  1437  1437  1437  1437  1437  1437  1437\n",
      "  1437    36   328   506 38254 43767 49895     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1978, 131, 50121, 50118, 50121, 50118, 50121, 50118, 31938, 13793]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2299\n",
      "  ✅ Generated contract for batch 59: 2299 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0059_generated.sol\n",
      "📝 Processing contract 61: 4346 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1366   131\n",
      " 50121 50118 50121 50118 50121 50118 31938 19981]\n",
      "Sampled tokens (last 20): [  560  7479 18134 19434  4397 50121 50118  1437  1437  1437 17172 49895\n",
      "  7761   742  5457  1522 23055    90 15664     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1366, 131, 50121, 50118, 50121, 50118, 50121, 50118, 31938, 19981]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2806\n",
      "  ✅ Generated contract for batch 60: 2806 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0060_generated.sol\n",
      "📝 Processing contract 62: 4006 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4   844   131\n",
      " 50121 50118 50121 50118 47888   381  5199   844]\n",
      "Sampled tokens (last 20): [50118  1437  1437  1437  1437  1437  1437  1437 17172 49895   560   742\n",
      "  5457 17172 49895   560  8174  4917 49604 19434]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 844, 131, 50121, 50118, 50121, 50118, 47888, 381, 5199, 844]\n",
      "Total filtered tokens: 1023\n",
      "Generated code length: 2640\n",
      "  ✅ Generated contract for batch 61: 2640 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0061_generated.sol\n",
      "📝 Processing contract 63: 1183 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1244   131\n",
      " 50121 50118 50121 50118 31938 39753  1215  2253]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1244, 131, 50121, 50118, 50121, 50118, 31938, 39753, 1215, 2253]\n",
      "Total filtered tokens: 551\n",
      "Generated code length: 1102\n",
      "  ✅ Generated contract for batch 62: 1102 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0062_generated.sol\n",
      "📝 Processing contract 64: 5463 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4   306   131\n",
      " 50121 50118 50121 50118 31938 29464 25522 50121]\n",
      "Sampled tokens (last 20): [46386 48593     4    29  7232   742 49095 18134 19434 48200 17172 19434\n",
      "  8061   321    43 25522 50121 50118  1437     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 306, 131, 50121, 50118, 50121, 50118, 31938, 29464, 25522, 50121]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 3026\n",
      "  ✅ Generated contract for batch 63: 3026 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0063_generated.sol\n",
      "📝 Processing contract 65: 1932 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249 50117 35227 35227   288     4   306\n",
      "     4  2146   131   131 50117 50117 50117 50117]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 50117, 35227, 35227, 288, 4, 306, 4, 2146, 131, 131, 50117, 50117, 50117, 50117]\n",
      "Total filtered tokens: 992\n",
      "Generated code length: 1849\n",
      "  ✅ Generated contract for batch 64: 1849 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0064_generated.sol\n",
      "📝 Processing contract 66: 3345 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [31938  1894    36   347    43 50121   254 25522 50121 50118 31938   381\n",
      "  5199   844 25522 50121 50118  1437  1437  5043]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  5043  2937  7605  1640 44547 47215     6  1100 49339\n",
      "     6 49315   885   625    43   285     6     2]\n",
      "Filtered tokens (first 20): [31938, 1894, 36, 347, 43, 50121, 254, 25522, 50121, 50118, 31938, 381, 5199, 844, 25522, 50121, 50118, 1437, 1437, 5043]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2273\n",
      "  ✅ Generated contract for batch 65: 2273 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0065_generated.sol\n",
      "📝 Processing contract 67: 2695 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1549   131\n",
      " 50121 50118 31938  9963 29070 45152 50121 50118]\n",
      "Sampled tokens (last 20): [ 1437  1437  1437  1437 10975 48593     4    29  7232 46386  1215  4182\n",
      "  7232   742  5457 18134 19434   131 50121     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1549, 131, 50121, 50118, 31938, 9963, 29070, 45152, 50121, 50118]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2276\n",
      "  ✅ Generated contract for batch 66: 2276 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0066_generated.sol\n",
      "📝 Processing contract 68: 4278 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1366   131\n",
      " 50121 50118 31674 13793 44853 25522 50121 50118]\n",
      "Sampled tokens (last 20): [20410 24554   285   746 33661   352   131 50121 50118 50121 50118  1437\n",
      "  1437  1437 47928   129 24522 16204 45698     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1366, 131, 50121, 50118, 31674, 13793, 44853, 25522, 50121, 50118]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2378\n",
      "  ✅ Generated contract for batch 67: 2378 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0067_generated.sol\n",
      "📝 Processing contract 69: 2548 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306   479   466   131\n",
      " 50121 50118 31674 13793 44853 25522 50121 50118]\n",
      "Sampled tokens (last 20): [ 7232   742  5457 18134 19434   131 50121 50118  1437  1437  1437  1437\n",
      "  1437  1437  1437  1437  6486  1640 48593     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 479, 466, 131, 50121, 50118, 31674, 13793, 44853, 25522, 50121, 50118]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2309\n",
      "  ✅ Generated contract for batch 68: 2309 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0068_generated.sol\n",
      "📝 Processing contract 70: 4101 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [ 4862  1073  1916  2705  1571 37249   288     4   306     4  1225   131\n",
      " 50121 50118 50121 50118 49795 50121 50118  1009]\n",
      "Sampled tokens (last 20): [19233  2229 50121   907 50118  1437  1437   907 49235  1640 44547 21467\n",
      "    43   285 21467 25522 50121 50118 50121     2]\n",
      "Filtered tokens (first 20): [4862, 1073, 1916, 2705, 1571, 37249, 288, 4, 306, 4, 1225, 131, 50121, 50118, 50121, 50118, 49795, 50121, 50118, 1009]\n",
      "Total filtered tokens: 1022\n",
      "Generated code length: 2829\n",
      "  ✅ Generated contract for batch 69: 2829 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0069_generated.sol\n",
      "📝 Processing contract 71: 2436 chars\n",
      "Logits shape: torch.Size([1023, 50265])\n",
      "Reshaped logits: torch.Size([1, 1023, 50265])\n",
      "Sampled tokens shape: torch.Size([1, 1023])\n",
      "Sampled tokens (first 20): [49795 50121 50118  1009 23055 13979    13 14925    23 35562 43511     4\n",
      "  1020    15   954    12  1225    12  1366 50121]\n",
      "Sampled tokens (last 20): [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Filtered tokens (first 20): [49795, 50121, 50118, 1009, 23055, 13979, 13, 14925, 23, 35562, 43511, 4, 1020, 15, 954, 12, 1225, 12, 1366, 50121]\n",
      "Total filtered tokens: 978\n",
      "Generated code length: 2331\n",
      "  ✅ Generated contract for batch 70: 2331 chars\n",
      "  💾 Saved to: sythetic_smart_contracts/contract_0070_generated.sol\n",
      "📝 Processing contract 72: 2510 chars\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 Generating Smart Contracts from Validation Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "output_folder = \"./sythetic_smart_contracts\"\n",
    "# Create output folder\n",
    "output_path = Path(output_folder)\n",
    "output_path.mkdir(exist_ok=True)\n",
    "print(f\"📁 Output folder: {output_path.absolute()}\")\n",
    "\n",
    "generated_contracts = []\n",
    "generation_metadata = []\n",
    "\n",
    "# Generate contracts from validation dataset\n",
    "num_contracts = len(val_dataloader.dataset.data)\n",
    "print(f\"\\n🎯 Generating {num_contracts} contracts from validation dataset...\")\n",
    "\n",
    "contracts_generated = 0\n",
    "contracts_processed = 0\n",
    "\n",
    "if val_dataloader:\n",
    "        # Generate contracts from validation dataset\n",
    "        print(f\"\\n🎯 Generating {num_contracts} contracts from validation dataset...\")\n",
    "        \n",
    "        contracts_generated = 0\n",
    "        contracts_processed = 0\n",
    "        \n",
    "        for batch_idx in range(1, 506):\n",
    "                \n",
    "            # Get source code from batch\n",
    "            source_code = contract_code = val_dataloader.dataset.data[batch_idx]['source_code'] #batch['source_code'][0]  # Assuming batch_size=1\n",
    "            contract_name = f\"contract_{batch_idx:04d}\"\n",
    "            \n",
    "            print(f\"📝 Processing contract {batch_idx + 1}: {len(source_code)} chars\")\n",
    "\n",
    "            contract_code = contract_code = val_dataloader.dataset.data[batch_idx]['source_code'] #batch['source_code'][0]#val_dataloader.dataset.data[5]['source_code']\n",
    "            ast = parse_solidity_to_ast(contract_code)\n",
    "            ast_paths = prepare_code2vec_input(ast) if ast else []\n",
    "            ast_path_text = ' '.join(ast_paths)\n",
    "            \n",
    "            # Tokenize inputs\n",
    "            contract_encoding = tokenizer(\n",
    "                contract_code,\n",
    "                max_length=1024,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            ast_encoding = tokenizer(\n",
    "                ast_path_text,\n",
    "                max_length=1024,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = contract_encoding['input_ids'].to(device)\n",
    "            attention_mask = contract_encoding['attention_mask'].to(device)\n",
    "            ast_input_ids = ast_encoding['input_ids'].to(device)\n",
    "            ast_attention_mask = ast_encoding['attention_mask'].to(device)\n",
    "            \n",
    "            model._debug_mode = False\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    ast_input_ids=ast_input_ids,\n",
    "                    ast_attention_mask=ast_attention_mask,\n",
    "                    target_ids=input_ids\n",
    "                )\n",
    "\n",
    "            generated_contracts_batch = generate_from_working_logits(\n",
    "                model=model,\n",
    "                outputs=outputs,  # Your existing outputs with logits\n",
    "                tokenizer=tokenizer,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            if generated_contracts_batch and len(generated_contracts_batch) > 0:\n",
    "                \n",
    "                # Save the generated contract to file\n",
    "                contract_file = output_path / f\"{contract_name}_generated.sol\"\n",
    "                with open(contract_file, 'w') as f:\n",
    "                    f.write(generated_contracts_batch)\n",
    "                \n",
    "                # Save the original contract for comparison\n",
    "                original_file = output_path / f\"{contract_name}_original.sol\"\n",
    "                with open(original_file, 'w') as f:\n",
    "                    f.write(source_code)\n",
    "                \n",
    "                # Save metadata\n",
    "                metadata = {\n",
    "                    'contract_name': contract_name,\n",
    "                    'original_length': len(source_code),\n",
    "                    'generated_length': len(generated_contracts_batch),\n",
    "                    'original_file': str(original_file),\n",
    "                    'generated_file': str(contract_file),\n",
    "                    'batch_index': batch_idx,\n",
    "                    'generation_success': True\n",
    "                }\n",
    "                \n",
    "                generated_contracts.append(generated_contract)\n",
    "                generation_metadata.append(metadata)\n",
    "                contracts_generated += 1\n",
    "                \n",
    "                print(f\"  ✅ Generated contract for batch {batch_idx}: {len(generated_contracts_batch)} chars\")\n",
    "                print(f\"  💾 Saved to: {contract_file}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ❌ Failed to generate contract {batch_idx + 1}\")\n",
    "                metadata = {\n",
    "                    'contract_name': contract_name,\n",
    "                    'original_length': len(source_code),\n",
    "                    'generated_length': 0,\n",
    "                    'original_file': str(output_path / f\"{contract_name}_original.sol\"),\n",
    "                    'generated_file': None,\n",
    "                    'batch_index': batch_idx,\n",
    "                    'generation_success': False,\n",
    "                    'error': 'Generation returned empty result'\n",
    "                }\n",
    "                generation_metadata.append(metadata)\n",
    "\n",
    "with open(output_path / \"generation_summary.json\", 'w') as f:\n",
    "    json.dump(generation_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 All contracts saved to: {output_path.absolute()}\")\n",
    "print(f\"📋 Generation summary saved to: {output_path / 'generation_summary.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66800902-be90-4fee-aa7e-2de6a2534216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce80b00-e169-40ac-9e6b-fb8b774c36a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18ce4e-91e2-4b3b-8c55-923911c11b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
