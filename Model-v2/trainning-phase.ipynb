{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3acb4f-edae-4342-9a26-7f8bf3a57e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv pip install torch transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1898cf2f-ccf6-4cc8-8aec-c2ce42f7b561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/usr/lib/python3.11/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.11/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3365, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3610, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3670, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2586391/944889248.py\", line 2, in <module>\n",
      "    import torch\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Model imports\n",
    "from model import SmartContractTransformer\n",
    "\n",
    "# Training imports\n",
    "from train import SmartContractTrainer\n",
    "\n",
    "# Optional but useful imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # for progress bars\n",
    "import logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7854810c-ced1-4bcf-b1ef-2fd5323dfa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0379b7b-bd04-495b-8521-9cb9be0fbaa2",
   "metadata": {},
   "source": [
    "- Dataset.py script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d5051a-5c5f-46fc-b3d6-25b687eb0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "Number of GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8342c217-fa0e-486d-9ace-e2ab37ef266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def parse_solidity_to_ast(code: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse Solidity code into a simplified AST structure\n",
    "    \"\"\"\n",
    "    def extract_contract_info(code: str) -> Dict[str, Any]:\n",
    "        # Extract contract name\n",
    "        contract_match = re.search(r'contract\\s+(\\w+)', code)\n",
    "        contract_name = contract_match.group(1) if contract_match else \"Unknown\"\n",
    "        \n",
    "        # Extract functions\n",
    "        functions = []\n",
    "        function_pattern = r'function\\s+(\\w+)\\s*\\(([^)]*)\\)\\s*(?:public|private|internal|external)?\\s*(?:view|pure|payable)?\\s*(?:returns\\s*\\(([^)]*)\\))?\\s*{'\n",
    "        for match in re.finditer(function_pattern, code):\n",
    "            func_name = match.group(1)\n",
    "            params = match.group(2).split(',') if match.group(2) else []\n",
    "            returns = match.group(3).split(',') if match.group(3) else []\n",
    "            \n",
    "            functions.append({\n",
    "                'name': func_name,\n",
    "                'parameters': [p.strip() for p in params],\n",
    "                'returns': [r.strip() for r in returns]\n",
    "            })\n",
    "        \n",
    "        # Extract state variables\n",
    "        variables = []\n",
    "        var_pattern = r'(?:uint|address|string|bool|mapping)\\s+(?:\\w+)\\s+(\\w+)'\n",
    "        for match in re.finditer(var_pattern, code):\n",
    "            variables.append(match.group(1))\n",
    "        \n",
    "        return {\n",
    "            'type': 'Contract',\n",
    "            'name': contract_name,\n",
    "            'functions': functions,\n",
    "            'variables': variables\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Clean the code\n",
    "        code = re.sub(r'//.*?\\n|/\\*.*?\\*/', '', code)  # Remove comments\n",
    "        code = re.sub(r'\\s+', ' ', code)  # Normalize whitespace\n",
    "        \n",
    "        # Parse the code\n",
    "        ast = extract_contract_info(code)\n",
    "        return ast\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing code: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def prepare_code2vec_input(ast: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert AST to codeBert input format\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    \n",
    "    def extract_paths(node: Dict[str, Any], current_path: List[str] = None):\n",
    "        if current_path is None:\n",
    "            current_path = []\n",
    "            \n",
    "        # Add current node to path\n",
    "        if 'name' in node:\n",
    "            current_path.append(node['name'])\n",
    "            \n",
    "        # Process functions\n",
    "        if 'functions' in node:\n",
    "            for func in node['functions']:\n",
    "                func_path = current_path + [func['name']]\n",
    "                paths.append(' '.join(func_path))\n",
    "                \n",
    "                # Add parameter paths\n",
    "                for param in func['parameters']:\n",
    "                    param_path = func_path + [param]\n",
    "                    paths.append(' '.join(param_path))\n",
    "                \n",
    "                # Add return paths\n",
    "                for ret in func['returns']:\n",
    "                    ret_path = func_path + [ret]\n",
    "                    paths.append(' '.join(ret_path))\n",
    "        \n",
    "        # Process variables\n",
    "        if 'variables' in node:\n",
    "            for var in node['variables']:\n",
    "                var_path = current_path + [var]\n",
    "                paths.append(' '.join(var_path))\n",
    "    \n",
    "    extract_paths(ast)\n",
    "    return paths\n",
    "\n",
    "class SmartContractVulnerabilityDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 1024,\n",
    "        split: str = \"train\",\n",
    "        vulnerability_types: List[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: Path to the CSV file containing the dataset\n",
    "            tokenizer: Tokenizer for encoding the source code\n",
    "            max_length: Maximum sequence length\n",
    "            split: \"train\" or \"val\" to specify which split to load\n",
    "            vulnerability_types: List of vulnerability types to consider\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.vulnerability_types = vulnerability_types or [\n",
    "            'ARTHM', 'DOS', 'LE', 'RENT', 'TimeM', 'TimeO', 'Tx-Origin', 'UE'\n",
    "        ]\n",
    "        \n",
    "        # Load the dataset\n",
    "        self.data = self._load_dataset(data_path)\n",
    "        \n",
    "    def _load_dataset(self, data_path: str) -> List[Dict]:\n",
    "        \"\"\"Load and preprocess the dataset from CSV\"\"\"\n",
    "        dataset = []\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Split into train/val if needed\n",
    "        if self.split == \"train\":\n",
    "            df = df.sample(frac=0.8, random_state=42)\n",
    "        else:\n",
    "            df = df.sample(frac=0.2, random_state=42)\n",
    "        \n",
    "        # Process each contract\n",
    "        for _, row in df.iterrows():\n",
    "            try:\n",
    "                source_code = row['source_code']\n",
    "                contract_name = row['contract_name']\n",
    "                \n",
    "                # Parse AST and get paths\n",
    "                ast = parse_solidity_to_ast(source_code)\n",
    "                ast_paths = prepare_code2vec_input(ast) if ast else []\n",
    "                ast_path_text = ' '.join(ast_paths)\n",
    "                \n",
    "                # Split source code into lines\n",
    "                lines = source_code.split('\\n')\n",
    "                \n",
    "                # Create token-to-line mapping\n",
    "                token_to_line = []\n",
    "                current_line = 0\n",
    "                \n",
    "                # Tokenize each line separately to maintain mapping\n",
    "                for line in lines:\n",
    "                    line_tokens = self.tokenizer.encode(line, add_special_tokens=False)\n",
    "                    token_to_line.extend([current_line] * len(line_tokens))\n",
    "                    current_line += 1\n",
    "                \n",
    "                # Add special tokens\n",
    "                token_to_line = [0] + token_to_line + [0]  # [CLS] and [SEP] tokens\n",
    "                \n",
    "                # Truncate if too long\n",
    "                if len(token_to_line) > self.max_length:\n",
    "                    token_to_line = token_to_line[:self.max_length]\n",
    "                \n",
    "                # Pad if too short\n",
    "                if len(token_to_line) < self.max_length:\n",
    "                    token_to_line.extend([0] * (self.max_length - len(token_to_line)))\n",
    "                \n",
    "                # Create multi-label line labels for each vulnerability type\n",
    "                line_labels = self._create_multi_label_line_labels(source_code, row)\n",
    "                \n",
    "                # Create contract-level vulnerability labels\n",
    "                contract_labels = self._create_contract_vulnerability_labels(row)\n",
    "                \n",
    "                # Tokenize the source code\n",
    "                encoding = self.tokenizer(\n",
    "                    source_code,\n",
    "                    max_length=self.max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # Tokenize AST paths\n",
    "                ast_encoding = self.tokenizer(\n",
    "                    ast_path_text,\n",
    "                    max_length=self.max_length,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # Convert line labels to tensor and ensure consistent shape\n",
    "                vuln_tensor = torch.zeros((len(self.vulnerability_types), self.max_length), dtype=torch.long)\n",
    "                for i, labels in enumerate(line_labels):\n",
    "                    if len(labels) > self.max_length:\n",
    "                        labels = labels[:self.max_length]\n",
    "                    vuln_tensor[i, :len(labels)] = torch.tensor(labels, dtype=torch.long)\n",
    "                \n",
    "                # Convert contract labels to tensor\n",
    "                contract_vuln_tensor = torch.tensor(contract_labels, dtype=torch.long)\n",
    "                \n",
    "                # Convert token_to_line to tensor\n",
    "                token_to_line_tensor = torch.tensor(token_to_line, dtype=torch.long)\n",
    "                \n",
    "                # Ensure attention masks are boolean\n",
    "                attention_mask = encoding['attention_mask'].squeeze(0).bool()\n",
    "                ast_attention_mask = ast_encoding['attention_mask'].squeeze(0).bool()\n",
    "                \n",
    "                # Ensure input_ids are the right length\n",
    "                input_ids = encoding['input_ids'].squeeze(0)\n",
    "                ast_input_ids = ast_encoding['input_ids'].squeeze(0)\n",
    "                \n",
    "                if len(input_ids) > self.max_length:\n",
    "                    input_ids = input_ids[:self.max_length]\n",
    "                if len(ast_input_ids) > self.max_length:\n",
    "                    ast_input_ids = ast_input_ids[:self.max_length]\n",
    "                \n",
    "                # Pad if necessary\n",
    "                if len(input_ids) < self.max_length:\n",
    "                    input_ids = torch.nn.functional.pad(input_ids, (0, self.max_length - len(input_ids)))\n",
    "                if len(ast_input_ids) < self.max_length:\n",
    "                    ast_input_ids = torch.nn.functional.pad(ast_input_ids, (0, self.max_length - len(ast_input_ids)))\n",
    "                \n",
    "                dataset.append({\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'ast_input_ids': ast_input_ids,\n",
    "                    'ast_attention_mask': ast_attention_mask,\n",
    "                    'vulnerable_lines': vuln_tensor,\n",
    "                    'contract_vulnerabilities': contract_vuln_tensor,\n",
    "                    'token_to_line': token_to_line_tensor,\n",
    "                    'source_code': source_code,\n",
    "                    'contract_name': contract_name\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing contract {contract_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def _create_contract_vulnerability_labels(self, row: pd.Series) -> List[int]:\n",
    "        \"\"\"Create contract-level vulnerability labels\"\"\"\n",
    "        contract_labels = []\n",
    "        for vuln_type in self.vulnerability_types:\n",
    "            # Check if contract has this vulnerability type\n",
    "            vuln_lines = row[f'{vuln_type}_lines']\n",
    "            if isinstance(vuln_lines, str):\n",
    "                try:\n",
    "                    vuln_lines = eval(vuln_lines)\n",
    "                except:\n",
    "                    vuln_lines = [vuln_lines]\n",
    "            \n",
    "            # Contract is vulnerable if it has any vulnerable lines\n",
    "            has_vulnerability = len(vuln_lines) > 0\n",
    "            contract_labels.append(1 if has_vulnerability else 0)\n",
    "        \n",
    "        return contract_labels\n",
    "    \n",
    "    def _create_multi_label_line_labels(self, source_code: str, row: pd.Series) -> List[List[int]]:\n",
    "        \"\"\"Create multi-label line labels for each vulnerability type\"\"\"\n",
    "        total_lines = len(source_code.split('\\n'))\n",
    "        line_labels = {vuln_type: [0] * total_lines for vuln_type in self.vulnerability_types}\n",
    "        \n",
    "        # Process each vulnerability type\n",
    "        for vuln_type in self.vulnerability_types:\n",
    "            vuln_lines = row[f'{vuln_type}_lines']\n",
    "            if isinstance(vuln_lines, str):\n",
    "                try:\n",
    "                    vuln_lines = eval(vuln_lines)\n",
    "                except:\n",
    "                    vuln_lines = [vuln_lines]\n",
    "            \n",
    "            # Process each vulnerable line/snippet\n",
    "            for line_or_snippet in vuln_lines:\n",
    "                if isinstance(line_or_snippet, int):\n",
    "                    # If it's a line number, mark that line\n",
    "                    if 0 <= line_or_snippet < total_lines:\n",
    "                        line_labels[vuln_type][line_or_snippet] = 1\n",
    "                else:\n",
    "                    # If it's a code snippet, find matching lines\n",
    "                    source_lines = source_code.split('\\n')\n",
    "                    for i, line in enumerate(source_lines):\n",
    "                        # Clean both the line and snippet for comparison\n",
    "                        clean_line = re.sub(r'\\s+', ' ', line.strip())\n",
    "                        clean_snippet = re.sub(r'\\s+', ' ', str(line_or_snippet).strip())\n",
    "                        if clean_snippet in clean_line:\n",
    "                            line_labels[vuln_type][i] = 1\n",
    "        \n",
    "        # Convert to list format\n",
    "        return [line_labels[vuln_type] for vuln_type in self.vulnerability_types]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        return self.data[idx]\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable length inputs\n",
    "    \"\"\"\n",
    "    # Get the maximum length in this batch for each type of tensor\n",
    "    max_input_len = max(item['input_ids'].size(0) for item in batch)\n",
    "    \n",
    "    # Pad all tensors to their respective maximum lengths\n",
    "    padded_batch = {\n",
    "        'input_ids': torch.stack([\n",
    "            torch.nn.functional.pad(item['input_ids'], (0, max_input_len - item['input_ids'].size(0)))\n",
    "            for item in batch\n",
    "        ]),\n",
    "        'attention_mask': torch.stack([\n",
    "            torch.nn.functional.pad(item['attention_mask'], (0, max_input_len - item['attention_mask'].size(0)))\n",
    "            for item in batch\n",
    "        ]),\n",
    "        'ast_input_ids': torch.stack([item['ast_input_ids'] for item in batch]),\n",
    "        'ast_attention_mask': torch.stack([item['ast_attention_mask'] for item in batch]),\n",
    "        'vulnerable_lines': torch.stack([item['vulnerable_lines'] for item in batch]),\n",
    "        'contract_vulnerabilities': torch.stack([item['contract_vulnerabilities'] for item in batch]),\n",
    "        'token_to_line': torch.stack([item['token_to_line'] for item in batch]),\n",
    "        'source_code': [item['source_code'] for item in batch],\n",
    "        'contract_name': [item['contract_name'] for item in batch]\n",
    "    }\n",
    "    \n",
    "    return padded_batch\n",
    "\n",
    "def create_dataloaders(\n",
    "    data_path: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    batch_size: int = 8,\n",
    "    max_length: int = 1024,\n",
    "    num_workers: int = 4,\n",
    "    vulnerability_types: List[str] = None\n",
    ") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Create train and validation dataloaders\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the CSV file containing the dataset\n",
    "        tokenizer: Tokenizer for encoding the source code\n",
    "        batch_size: Batch size for training\n",
    "        max_length: Maximum sequence length\n",
    "        num_workers: Number of workers for data loading\n",
    "        vulnerability_types: List of vulnerability types to consider\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_dataloader, val_dataloader)\n",
    "    \"\"\"\n",
    "    # Create datasets\n",
    "    train_dataset = SmartContractVulnerabilityDataset(\n",
    "        data_path=data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        split=\"train\",\n",
    "        vulnerability_types=vulnerability_types\n",
    "    )\n",
    "    \n",
    "    val_dataset = SmartContractVulnerabilityDataset(\n",
    "        data_path=data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        split=\"val\",\n",
    "        vulnerability_types=vulnerability_types\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders with custom collate function\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35cba34-ea45-434f-ad6d-9b4449db435a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1211 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader, val_dataloader = create_dataloaders(\n",
    "    data_path=\"contract_sources_with_vulnerabilities_2048_token_size.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=8,\n",
    "    max_length=1024,\n",
    "    vulnerability_types=['ARTHM', 'DOS', 'LE', 'RENT', 'TimeM', 'TimeO', 'Tx-Origin', 'UE']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd90546-c363-4914-89bf-cc2ed0c53bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial learning rate: 0.01\n",
      "WARNING: Learning rate is too high! Setting to 1e-3\n",
      "\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:31<00:00,  1.20it/s, gen_loss=4.0454, contract_vuln_loss=0.0252, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7148\n",
      "Val Loss: 4.5698\n",
      "Contract Vulnerability Loss: 0.0255\n",
      "Line Vulnerability Loss: 0.0073\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_1.pt\n",
      "\n",
      "Epoch 2/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:32<00:00,  1.19it/s, gen_loss=3.3163, contract_vuln_loss=0.0199, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.8983\n",
      "Val Loss: 3.3172\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_2.pt\n",
      "\n",
      "Epoch 3/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:31<00:00,  1.19it/s, gen_loss=3.3195, contract_vuln_loss=0.0300, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.1145\n",
      "Val Loss: 2.9461\n",
      "Contract Vulnerability Loss: 0.0228\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_3.pt\n",
      "\n",
      "Epoch 4/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:31<00:00,  1.19it/s, gen_loss=3.2664, contract_vuln_loss=0.0402, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.8630\n",
      "Val Loss: 2.7486\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_4.pt\n",
      "\n",
      "Epoch 5/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:32<00:00,  1.19it/s, gen_loss=2.9030, contract_vuln_loss=0.0296, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.7105\n",
      "Val Loss: 2.5856\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_5.pt\n",
      "\n",
      "Epoch 6/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:32<00:00,  1.19it/s, gen_loss=2.3392, contract_vuln_loss=0.0221, line_vuln_loss=0.0005, lr=0.001000, grad_norm=0.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.5816\n",
      "Val Loss: 2.4701\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_6.pt\n",
      "\n",
      "Epoch 7/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:32<00:00,  1.19it/s, gen_loss=2.4676, contract_vuln_loss=0.0158, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4781\n",
      "Val Loss: 2.3776\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_7.pt\n",
      "\n",
      "Epoch 8/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:31<00:00,  1.19it/s, gen_loss=2.3322, contract_vuln_loss=0.0262, line_vuln_loss=0.0005, lr=0.001000, grad_norm=0.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.4005\n",
      "Val Loss: 2.2867\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_8.pt\n",
      "\n",
      "Epoch 9/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:22<00:00,  1.25it/s, gen_loss=2.3350, contract_vuln_loss=0.0257, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3359\n",
      "Val Loss: 2.2343\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_9.pt\n",
      "\n",
      "Epoch 10/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:31<00:00,  1.20it/s, gen_loss=2.2550, contract_vuln_loss=0.0292, line_vuln_loss=0.0002, lr=0.001000, grad_norm=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2784\n",
      "Val Loss: 2.1723\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_10.pt\n",
      "\n",
      "Epoch 11/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:30<00:00,  1.20it/s, gen_loss=2.1922, contract_vuln_loss=0.0257, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.2267\n",
      "Val Loss: 2.0999\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_11.pt\n",
      "\n",
      "Epoch 12/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:30<00:00,  1.20it/s, gen_loss=1.9724, contract_vuln_loss=0.0186, line_vuln_loss=0.0005, lr=0.001000, grad_norm=0.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1761\n",
      "Val Loss: 2.0488\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_12.pt\n",
      "\n",
      "Epoch 13/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:30<00:00,  1.20it/s, gen_loss=2.1813, contract_vuln_loss=0.0212, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1338\n",
      "Val Loss: 2.0102\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_13.pt\n",
      "\n",
      "Epoch 14/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:30<00:00,  1.20it/s, gen_loss=2.0196, contract_vuln_loss=0.0247, line_vuln_loss=0.0006, lr=0.001000, grad_norm=0.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0954\n",
      "Val Loss: 1.9607\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_14.pt\n",
      "\n",
      "Epoch 15/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:32<00:00,  1.19it/s, gen_loss=1.9382, contract_vuln_loss=0.0235, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0600\n",
      "Val Loss: 1.9329\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_15.pt\n",
      "\n",
      "Epoch 16/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:30<00:00,  1.20it/s, gen_loss=2.0836, contract_vuln_loss=0.0142, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0267\n",
      "Val Loss: 1.8937\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_16.pt\n",
      "\n",
      "Epoch 17/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:30<00:00,  1.20it/s, gen_loss=2.1255, contract_vuln_loss=0.0253, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9990\n",
      "Val Loss: 1.8688\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_17.pt\n",
      "\n",
      "Epoch 18/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:31<00:00,  1.20it/s, gen_loss=1.9387, contract_vuln_loss=0.0219, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9682\n",
      "Val Loss: 1.8464\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_18.pt\n",
      "\n",
      "Epoch 19/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:30<00:00,  1.20it/s, gen_loss=1.9910, contract_vuln_loss=0.0220, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9440\n",
      "Val Loss: 1.8215\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_19.pt\n",
      "\n",
      "Epoch 20/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:31<00:00,  1.20it/s, gen_loss=1.7560, contract_vuln_loss=0.0209, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9210\n",
      "Val Loss: 1.7981\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_20.pt\n",
      "\n",
      "Epoch 21/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:30<00:00,  1.20it/s, gen_loss=1.7604, contract_vuln_loss=0.0185, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8969\n",
      "Val Loss: 1.7790\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_21.pt\n",
      "\n",
      "Epoch 22/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [03:15<00:00,  1.29it/s, gen_loss=1.8763, contract_vuln_loss=0.0219, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8772\n",
      "Val Loss: 1.7587\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_22.pt\n",
      "\n",
      "Epoch 23/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.8296, contract_vuln_loss=0.0227, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8584\n",
      "Val Loss: 1.7355\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_23.pt\n",
      "\n",
      "Epoch 24/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:56<00:00,  1.44it/s, gen_loss=1.7791, contract_vuln_loss=0.0253, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8398\n",
      "Val Loss: 1.7195\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_24.pt\n",
      "\n",
      "Epoch 25/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.7106, contract_vuln_loss=0.0272, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8231\n",
      "Val Loss: 1.7173\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_25.pt\n",
      "\n",
      "Epoch 26/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.8245, contract_vuln_loss=0.0182, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8037\n",
      "Val Loss: 1.6961\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_26.pt\n",
      "\n",
      "Epoch 27/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.7376, contract_vuln_loss=0.0205, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7828\n",
      "Val Loss: 1.6600\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_27.pt\n",
      "\n",
      "Epoch 28/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:56<00:00,  1.44it/s, gen_loss=1.8504, contract_vuln_loss=0.0229, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7586\n",
      "Val Loss: 1.6370\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_28.pt\n",
      "\n",
      "Epoch 29/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:56<00:00,  1.44it/s, gen_loss=1.6954, contract_vuln_loss=0.0155, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7356\n",
      "Val Loss: 1.6286\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_29.pt\n",
      "\n",
      "Epoch 30/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:56<00:00,  1.44it/s, gen_loss=1.7600, contract_vuln_loss=0.0373, line_vuln_loss=0.0006, lr=0.001000, grad_norm=0.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7124\n",
      "Val Loss: 1.6081\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_30.pt\n",
      "\n",
      "Epoch 31/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.6145, contract_vuln_loss=0.0221, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6922\n",
      "Val Loss: 1.5894\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_31.pt\n",
      "\n",
      "Epoch 32/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:56<00:00,  1.44it/s, gen_loss=1.6418, contract_vuln_loss=0.0207, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6700\n",
      "Val Loss: 1.5726\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_32.pt\n",
      "\n",
      "Epoch 33/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:56<00:00,  1.44it/s, gen_loss=1.7813, contract_vuln_loss=0.0241, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6524\n",
      "Val Loss: 1.5598\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_33.pt\n",
      "\n",
      "Epoch 34/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.6632, contract_vuln_loss=0.0223, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6361\n",
      "Val Loss: 1.5447\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_34.pt\n",
      "\n",
      "Epoch 35/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.6376, contract_vuln_loss=0.0238, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6188\n",
      "Val Loss: 1.5353\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_35.pt\n",
      "\n",
      "Epoch 36/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5964, contract_vuln_loss=0.0212, line_vuln_loss=0.0005, lr=0.001000, grad_norm=0.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6047\n",
      "Val Loss: 1.5287\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_36.pt\n",
      "\n",
      "Epoch 37/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.7082, contract_vuln_loss=0.0292, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5919\n",
      "Val Loss: 1.5311\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 38/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.6149, contract_vuln_loss=0.0244, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5820\n",
      "Val Loss: 1.5297\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 39/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5091, contract_vuln_loss=0.0195, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5729\n",
      "Val Loss: 1.5264\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_39.pt\n",
      "\n",
      "Epoch 40/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.6037, contract_vuln_loss=0.0157, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5662\n",
      "Val Loss: 1.5138\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_40.pt\n",
      "\n",
      "Epoch 41/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5683, contract_vuln_loss=0.0155, line_vuln_loss=0.0002, lr=0.001000, grad_norm=0.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5602\n",
      "Val Loss: 1.5212\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 42/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5611, contract_vuln_loss=0.0344, line_vuln_loss=0.0005, lr=0.001000, grad_norm=0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5533\n",
      "Val Loss: 1.5005\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_42.pt\n",
      "\n",
      "Epoch 43/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5404, contract_vuln_loss=0.0142, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5467\n",
      "Val Loss: 1.5061\n",
      "Contract Vulnerability Loss: 0.0228\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 44/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5371, contract_vuln_loss=0.0258, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5406\n",
      "Val Loss: 1.5021\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 45/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4965, contract_vuln_loss=0.0285, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5350\n",
      "Val Loss: 1.4974\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_45.pt\n",
      "\n",
      "Epoch 46/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5612, contract_vuln_loss=0.0286, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5295\n",
      "Val Loss: 1.4996\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 47/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5023, contract_vuln_loss=0.0188, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5253\n",
      "Val Loss: 1.5030\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 48/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5020, contract_vuln_loss=0.0228, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5203\n",
      "Val Loss: 1.4943\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_48.pt\n",
      "\n",
      "Epoch 49/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4942, contract_vuln_loss=0.0181, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5170\n",
      "Val Loss: 1.5038\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 50/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4934, contract_vuln_loss=0.0200, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5130\n",
      "Val Loss: 1.4929\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_50.pt\n",
      "\n",
      "Epoch 51/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5571, contract_vuln_loss=0.0191, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5109\n",
      "Val Loss: 1.4883\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_51.pt\n",
      "\n",
      "Epoch 52/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5300, contract_vuln_loss=0.0190, line_vuln_loss=0.0006, lr=0.001000, grad_norm=0.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5087\n",
      "Val Loss: 1.4891\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 53/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5137, contract_vuln_loss=0.0168, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5058\n",
      "Val Loss: 1.4936\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 54/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5455, contract_vuln_loss=0.0302, line_vuln_loss=0.0007, lr=0.001000, grad_norm=0.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5037\n",
      "Val Loss: 1.4907\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 55/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4820, contract_vuln_loss=0.0333, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5009\n",
      "Val Loss: 1.4936\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 4 epochs\n",
      "\n",
      "Epoch 56/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4841, contract_vuln_loss=0.0263, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4991\n",
      "Val Loss: 1.4881\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_56.pt\n",
      "\n",
      "Epoch 57/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4847, contract_vuln_loss=0.0212, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4971\n",
      "Val Loss: 1.4890\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 58/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4950, contract_vuln_loss=0.0256, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4961\n",
      "Val Loss: 1.4868\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_58.pt\n",
      "\n",
      "Epoch 59/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4703, contract_vuln_loss=0.0214, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4941\n",
      "Val Loss: 1.4808\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_59.pt\n",
      "\n",
      "Epoch 60/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4784, contract_vuln_loss=0.0243, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4930\n",
      "Val Loss: 1.4773\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_60.pt\n",
      "\n",
      "Epoch 61/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4806, contract_vuln_loss=0.0162, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.41]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4924\n",
      "Val Loss: 1.4832\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 62/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5037, contract_vuln_loss=0.0178, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4916\n",
      "Val Loss: 1.4743\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_62.pt\n",
      "\n",
      "Epoch 63/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5145, contract_vuln_loss=0.0207, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4892\n",
      "Val Loss: 1.4751\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 64/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4975, contract_vuln_loss=0.0171, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4893\n",
      "Val Loss: 1.4860\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 65/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4655, contract_vuln_loss=0.0133, line_vuln_loss=0.0006, lr=0.001000, grad_norm=0.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4881\n",
      "Val Loss: 1.4782\n",
      "Contract Vulnerability Loss: 0.0228\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 66/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5326, contract_vuln_loss=0.0241, line_vuln_loss=0.0002, lr=0.001000, grad_norm=0.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4864\n",
      "Val Loss: 1.4734\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_66.pt\n",
      "\n",
      "Epoch 67/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5211, contract_vuln_loss=0.0351, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4859\n",
      "Val Loss: 1.4845\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 68/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5260, contract_vuln_loss=0.0262, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4846\n",
      "Val Loss: 1.4766\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 69/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5014, contract_vuln_loss=0.0212, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4832\n",
      "Val Loss: 1.4730\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_69.pt\n",
      "\n",
      "Epoch 70/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5096, contract_vuln_loss=0.0388, line_vuln_loss=0.0005, lr=0.001000, grad_norm=0.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4814\n",
      "Val Loss: 1.4791\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 71/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5215, contract_vuln_loss=0.0174, line_vuln_loss=0.0005, lr=0.001000, grad_norm=0.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4803\n",
      "Val Loss: 1.4763\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 72/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4853, contract_vuln_loss=0.0252, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4804\n",
      "Val Loss: 1.4750\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 73/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.5016, contract_vuln_loss=0.0278, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4787\n",
      "Val Loss: 1.4727\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_73.pt\n",
      "\n",
      "Epoch 74/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4784, contract_vuln_loss=0.0220, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4794\n",
      "Val Loss: 1.4656\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_74.pt\n",
      "\n",
      "Epoch 75/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4538, contract_vuln_loss=0.0219, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4775\n",
      "Val Loss: 1.4760\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 76/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4536, contract_vuln_loss=0.0163, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4779\n",
      "Val Loss: 1.4708\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 77/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.45it/s, gen_loss=1.4577, contract_vuln_loss=0.0190, line_vuln_loss=0.0005, lr=0.001000, grad_norm=0.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4779\n",
      "Val Loss: 1.4748\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 78/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.45it/s, gen_loss=1.4721, contract_vuln_loss=0.0237, line_vuln_loss=0.0004, lr=0.001000, grad_norm=0.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4772\n",
      "Val Loss: 1.4733\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 4 epochs\n",
      "\n",
      "Epoch 79/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4957, contract_vuln_loss=0.0209, line_vuln_loss=0.0006, lr=0.001000, grad_norm=0.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4762\n",
      "Val Loss: 1.4704\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 5 epochs\n",
      "\n",
      "Epoch 80/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4694, contract_vuln_loss=0.0140, line_vuln_loss=0.0003, lr=0.001000, grad_norm=0.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4765\n",
      "Val Loss: 1.4722\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.001000\n",
      "No improvement for 6 epochs\n",
      "\n",
      "Epoch 81/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4900, contract_vuln_loss=0.0193, line_vuln_loss=0.0004, lr=0.000500, grad_norm=0.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4567\n",
      "Val Loss: 1.4639\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000500\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_81.pt\n",
      "\n",
      "Epoch 82/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4486, contract_vuln_loss=0.0176, line_vuln_loss=0.0003, lr=0.000500, grad_norm=0.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4530\n",
      "Val Loss: 1.4593\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000500\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_82.pt\n",
      "\n",
      "Epoch 83/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4657, contract_vuln_loss=0.0217, line_vuln_loss=0.0006, lr=0.000500, grad_norm=0.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4527\n",
      "Val Loss: 1.4615\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000500\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 84/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4527, contract_vuln_loss=0.0238, line_vuln_loss=0.0002, lr=0.000500, grad_norm=0.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4517\n",
      "Val Loss: 1.4543\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000500\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_84.pt\n",
      "\n",
      "Epoch 85/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4497, contract_vuln_loss=0.0147, line_vuln_loss=0.0005, lr=0.000500, grad_norm=0.30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4510\n",
      "Val Loss: 1.4588\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000500\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 86/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4804, contract_vuln_loss=0.0364, line_vuln_loss=0.0004, lr=0.000500, grad_norm=0.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4509\n",
      "Val Loss: 1.4597\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000500\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 87/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4562, contract_vuln_loss=0.0252, line_vuln_loss=0.0004, lr=0.000500, grad_norm=0.30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4501\n",
      "Val Loss: 1.4573\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000500\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 88/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4496, contract_vuln_loss=0.0253, line_vuln_loss=0.0003, lr=0.000500, grad_norm=0.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4499\n",
      "Val Loss: 1.4586\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000500\n",
      "No improvement for 4 epochs\n",
      "\n",
      "Epoch 89/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.45it/s, gen_loss=1.4667, contract_vuln_loss=0.0212, line_vuln_loss=0.0003, lr=0.000500, grad_norm=0.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4496\n",
      "Val Loss: 1.4579\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000500\n",
      "No improvement for 5 epochs\n",
      "\n",
      "Epoch 90/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.45it/s, gen_loss=1.4685, contract_vuln_loss=0.0364, line_vuln_loss=0.0004, lr=0.000500, grad_norm=0.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4492\n",
      "Val Loss: 1.4619\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000500\n",
      "No improvement for 6 epochs\n",
      "\n",
      "Epoch 91/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4442, contract_vuln_loss=0.0214, line_vuln_loss=0.0004, lr=0.000250, grad_norm=0.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4409\n",
      "Val Loss: 1.4527\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000250\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_91.pt\n",
      "\n",
      "Epoch 92/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4339, contract_vuln_loss=0.0246, line_vuln_loss=0.0004, lr=0.000250, grad_norm=0.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4395\n",
      "Val Loss: 1.4513\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000250\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_92.pt\n",
      "\n",
      "Epoch 93/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4636, contract_vuln_loss=0.0193, line_vuln_loss=0.0003, lr=0.000250, grad_norm=0.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4391\n",
      "Val Loss: 1.4546\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000250\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 94/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.45it/s, gen_loss=1.4438, contract_vuln_loss=0.0209, line_vuln_loss=0.0006, lr=0.000250, grad_norm=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4394\n",
      "Val Loss: 1.4536\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000250\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 95/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4325, contract_vuln_loss=0.0159, line_vuln_loss=0.0003, lr=0.000250, grad_norm=0.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4386\n",
      "Val Loss: 1.4534\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000250\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 96/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4375, contract_vuln_loss=0.0180, line_vuln_loss=0.0003, lr=0.000250, grad_norm=0.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4382\n",
      "Val Loss: 1.4513\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000250\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_96.pt\n",
      "\n",
      "Epoch 97/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4352, contract_vuln_loss=0.0114, line_vuln_loss=0.0003, lr=0.000250, grad_norm=0.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4381\n",
      "Val Loss: 1.4518\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000250\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 98/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4394, contract_vuln_loss=0.0228, line_vuln_loss=0.0004, lr=0.000250, grad_norm=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4381\n",
      "Val Loss: 1.4512\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000250\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_98.pt\n",
      "\n",
      "Epoch 99/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4292, contract_vuln_loss=0.0204, line_vuln_loss=0.0003, lr=0.000125, grad_norm=0.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4342\n",
      "Val Loss: 1.4501\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000125\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_99.pt\n",
      "\n",
      "Epoch 100/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4313, contract_vuln_loss=0.0177, line_vuln_loss=0.0003, lr=0.000125, grad_norm=0.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4336\n",
      "Val Loss: 1.4503\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000125\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 101/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.45it/s, gen_loss=1.4411, contract_vuln_loss=0.0243, line_vuln_loss=0.0004, lr=0.000125, grad_norm=0.40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4336\n",
      "Val Loss: 1.4501\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000125\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 102/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [5:54:44<00:00, 84.13s/it, gen_loss=1.4306, contract_vuln_loss=0.0139, line_vuln_loss=0.0003, lr=0.000125, grad_norm=0.17]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4334\n",
      "Val Loss: 1.4498\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000125\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_102.pt\n",
      "\n",
      "Epoch 103/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:55<00:00,  1.44it/s, gen_loss=1.4339, contract_vuln_loss=0.0262, line_vuln_loss=0.0003, lr=0.000125, grad_norm=0.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4331\n",
      "Val Loss: 1.4506\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000125\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 104/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4446, contract_vuln_loss=0.0197, line_vuln_loss=0.0003, lr=0.000125, grad_norm=0.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4330\n",
      "Val Loss: 1.4503\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000125\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 105/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4457, contract_vuln_loss=0.0277, line_vuln_loss=0.0003, lr=0.000125, grad_norm=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4329\n",
      "Val Loss: 1.4501\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000125\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 106/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4304, contract_vuln_loss=0.0158, line_vuln_loss=0.0006, lr=0.000125, grad_norm=0.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4329\n",
      "Val Loss: 1.4502\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000125\n",
      "No improvement for 4 epochs\n",
      "\n",
      "Epoch 107/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4346, contract_vuln_loss=0.0215, line_vuln_loss=0.0002, lr=0.000125, grad_norm=0.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4328\n",
      "Val Loss: 1.4499\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000125\n",
      "No improvement for 5 epochs\n",
      "\n",
      "Epoch 108/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4298, contract_vuln_loss=0.0235, line_vuln_loss=0.0004, lr=0.000125, grad_norm=0.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4324\n",
      "Val Loss: 1.4498\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000125\n",
      "No improvement for 6 epochs\n",
      "\n",
      "Epoch 109/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4338, contract_vuln_loss=0.0309, line_vuln_loss=0.0005, lr=0.000063, grad_norm=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4308\n",
      "Val Loss: 1.4498\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000063\n",
      "No improvement for 7 epochs\n",
      "\n",
      "Epoch 110/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4375, contract_vuln_loss=0.0114, line_vuln_loss=0.0004, lr=0.000063, grad_norm=0.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4308\n",
      "Val Loss: 1.4493\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000063\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_110.pt\n",
      "\n",
      "Epoch 111/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4656, contract_vuln_loss=0.0221, line_vuln_loss=0.0004, lr=0.000063, grad_norm=0.30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4306\n",
      "Val Loss: 1.4495\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000063\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 112/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4256, contract_vuln_loss=0.0175, line_vuln_loss=0.0004, lr=0.000063, grad_norm=0.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4304\n",
      "Val Loss: 1.4496\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000063\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 113/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4391, contract_vuln_loss=0.0236, line_vuln_loss=0.0003, lr=0.000063, grad_norm=0.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4305\n",
      "Val Loss: 1.4492\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000063\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_113.pt\n",
      "\n",
      "Epoch 114/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4448, contract_vuln_loss=0.0220, line_vuln_loss=0.0004, lr=0.000063, grad_norm=0.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4304\n",
      "Val Loss: 1.4495\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000063\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 115/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4419, contract_vuln_loss=0.0237, line_vuln_loss=0.0004, lr=0.000063, grad_norm=0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4304\n",
      "Val Loss: 1.4494\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000063\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 116/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4241, contract_vuln_loss=0.0171, line_vuln_loss=0.0003, lr=0.000063, grad_norm=0.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4304\n",
      "Val Loss: 1.4494\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000063\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 117/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4387, contract_vuln_loss=0.0205, line_vuln_loss=0.0003, lr=0.000031, grad_norm=0.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4297\n",
      "Val Loss: 1.4490\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000031\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_117.pt\n",
      "\n",
      "Epoch 118/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4395, contract_vuln_loss=0.0219, line_vuln_loss=0.0004, lr=0.000031, grad_norm=0.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4295\n",
      "Val Loss: 1.4490\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000031\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_118.pt\n",
      "\n",
      "Epoch 119/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4429, contract_vuln_loss=0.0328, line_vuln_loss=0.0005, lr=0.000031, grad_norm=0.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4294\n",
      "Val Loss: 1.4491\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000031\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 120/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4355, contract_vuln_loss=0.0288, line_vuln_loss=0.0004, lr=0.000031, grad_norm=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4295\n",
      "Val Loss: 1.4492\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000031\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 121/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4294, contract_vuln_loss=0.0183, line_vuln_loss=0.0005, lr=0.000031, grad_norm=0.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4292\n",
      "Val Loss: 1.4492\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000031\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 122/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4404, contract_vuln_loss=0.0267, line_vuln_loss=0.0002, lr=0.000031, grad_norm=0.22]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4294\n",
      "Val Loss: 1.4491\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000031\n",
      "No improvement for 4 epochs\n",
      "\n",
      "Epoch 123/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4220, contract_vuln_loss=0.0191, line_vuln_loss=0.0004, lr=0.000031, grad_norm=0.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4293\n",
      "Val Loss: 1.4491\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000031\n",
      "No improvement for 5 epochs\n",
      "\n",
      "Epoch 124/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4294, contract_vuln_loss=0.0258, line_vuln_loss=0.0004, lr=0.000016, grad_norm=0.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4291\n",
      "Val Loss: 1.4490\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000016\n",
      "No improvement for 6 epochs\n",
      "\n",
      "Epoch 125/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4298, contract_vuln_loss=0.0244, line_vuln_loss=0.0005, lr=0.000016, grad_norm=0.20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4290\n",
      "Val Loss: 1.4491\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000016\n",
      "No improvement for 7 epochs\n",
      "\n",
      "Epoch 126/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4280, contract_vuln_loss=0.0198, line_vuln_loss=0.0004, lr=0.000016, grad_norm=0.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4287\n",
      "Val Loss: 1.4488\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000016\n",
      "ðŸŽ‰ New best validation loss! Saved checkpoint to checkpoints_v2_2048_output/best_model_epoch_126.pt\n",
      "\n",
      "Epoch 127/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4275, contract_vuln_loss=0.0210, line_vuln_loss=0.0005, lr=0.000016, grad_norm=0.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4289\n",
      "Val Loss: 1.4490\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000016\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 128/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4276, contract_vuln_loss=0.0232, line_vuln_loss=0.0004, lr=0.000016, grad_norm=0.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4289\n",
      "Val Loss: 1.4489\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000016\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 129/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4238, contract_vuln_loss=0.0203, line_vuln_loss=0.0005, lr=0.000016, grad_norm=0.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4288\n",
      "Val Loss: 1.4492\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000016\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 130/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4497, contract_vuln_loss=0.0219, line_vuln_loss=0.0004, lr=0.000016, grad_norm=0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4287\n",
      "Val Loss: 1.4492\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000016\n",
      "No improvement for 4 epochs\n",
      "\n",
      "Epoch 131/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4330, contract_vuln_loss=0.0269, line_vuln_loss=0.0005, lr=0.000016, grad_norm=0.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4289\n",
      "Val Loss: 1.4492\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000016\n",
      "No improvement for 5 epochs\n",
      "\n",
      "Epoch 132/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4303, contract_vuln_loss=0.0178, line_vuln_loss=0.0005, lr=0.000016, grad_norm=0.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4288\n",
      "Val Loss: 1.4491\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000016\n",
      "No improvement for 6 epochs\n",
      "\n",
      "Epoch 133/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4247, contract_vuln_loss=0.0197, line_vuln_loss=0.0005, lr=0.000008, grad_norm=0.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4287\n",
      "Val Loss: 1.4490\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000008\n",
      "No improvement for 7 epochs\n",
      "\n",
      "Epoch 134/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4264, contract_vuln_loss=0.0255, line_vuln_loss=0.0004, lr=0.000008, grad_norm=0.10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4287\n",
      "Val Loss: 1.4491\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000008\n",
      "No improvement for 8 epochs\n",
      "\n",
      "Epoch 135/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4236, contract_vuln_loss=0.0253, line_vuln_loss=0.0005, lr=0.000008, grad_norm=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4285\n",
      "Val Loss: 1.4491\n",
      "Contract Vulnerability Loss: 0.0226\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000008\n",
      "No improvement for 9 epochs\n",
      "\n",
      "Epoch 136/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253/253 [02:54<00:00,  1.45it/s, gen_loss=1.4298, contract_vuln_loss=0.0173, line_vuln_loss=0.0004, lr=0.000008, grad_norm=0.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4286\n",
      "Val Loss: 1.4490\n",
      "Contract Vulnerability Loss: 0.0227\n",
      "Line Vulnerability Loss: 0.0004\n",
      "Learning Rate: 0.000008\n",
      "No improvement for 10 epochs\n",
      "Early stopping triggered after 10 epochs without improvement\n"
     ]
    }
   ],
   "source": [
    "model = SmartContractTransformer()\n",
    "\n",
    "trainer = SmartContractTrainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm= 1.0\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train(num_epochs=400, checkpoint_dir='checkpoints_v2_2048_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2cbc0e8-e189-41a4-95f5-4d65a9c2e5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeac150-5016-4f26-ac9c-e8345e61a804",
   "metadata": {},
   "source": [
    "## Re-trainning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b8776-c9b0-4d6c-91e2-495414474d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ed63565-adb9-4465-9bfe-7b9f1939ad16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 40\n",
      "Validation loss: 4.5804\n",
      "Reset learning rate to 0.002\n"
     ]
    }
   ],
   "source": [
    "def load_trained_model(checkpoint_path):\n",
    "    \"\"\"\n",
    "    Load a trained model and discriminator from checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device('cuda:1')\n",
    "    \n",
    "    # Initialize model and discriminator\n",
    "    model = SmartContractTransformer()\n",
    "    discriminator = Discriminator()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load model and discriminator states\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "\n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        for param_group in checkpoint['optimizer_state_dict']['param_groups']:\n",
    "            param_group['lr'] = 0.02\n",
    "    if 'discriminator_optimizer_state_dict' in checkpoint:\n",
    "        for param_group in checkpoint['discriminator_optimizer_state_dict']['param_groups']:\n",
    "            param_group['lr'] = 0.02\n",
    "    \n",
    "    print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"Reset learning rate to 0.02\")\n",
    "    \n",
    "    return model, discriminator, checkpoint\n",
    "\n",
    "# Load pre-trained model\n",
    "checkpoint_path = 'checkpoints_v1_512_output/best_model_epoch_40.pt'  \n",
    "model, discriminator, checkpoint = load_trained_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9013060-daab-41aa-b7c1-67f564808280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m20180848/pytorch_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [05:39<00:00,  3.85s/it, gen_loss=3.78, vuln_loss=0.377, synth_loss=0.766, diversity_loss=-0, d_penalty=0, g_penalty=0, is_synthetic=0]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0873\n",
      "Val Loss: 4.6308\n",
      "Vulnerability Loss: 9.4490\n",
      "Synthetic Loss: 23.9306\n",
      "Learning Rate: 0.020000\n",
      "Saved checkpoint to checkpoints_v1_512_retrain/best_model_epoch_1.pt\n",
      "\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [05:47<00:00,  3.95s/it, gen_loss=4.87, vuln_loss=0.378, synth_loss=0.317, diversity_loss=-0, d_penalty=0, g_penalty=0, is_synthetic=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1122\n",
      "Val Loss: 4.6044\n",
      "Vulnerability Loss: 0.3398\n",
      "Synthetic Loss: 0.5832\n",
      "Learning Rate: 0.020000\n",
      "Saved checkpoint to checkpoints_v1_512_retrain/best_model_epoch_2.pt\n",
      "\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [05:29<00:00,  3.74s/it, gen_loss=4.2, vuln_loss=0.246, synth_loss=0.84, diversity_loss=-0, d_penalty=0, g_penalty=0, is_synthetic=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0973\n",
      "Val Loss: 4.6069\n",
      "Vulnerability Loss: 0.3393\n",
      "Synthetic Loss: 0.5637\n",
      "Learning Rate: 0.020000\n",
      "\n",
      "Epoch 4/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [05:19<00:00,  3.64s/it, gen_loss=3.8, vuln_loss=0.247, synth_loss=0.258, diversity_loss=-0, d_penalty=0, g_penalty=0, is_synthetic=1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0883\n",
      "Val Loss: 4.6367\n",
      "Vulnerability Loss: 0.3413\n",
      "Synthetic Loss: 0.5390\n",
      "Learning Rate: 0.020000\n",
      "\n",
      "Epoch 5/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–Œ         | 5/88 [00:22<06:18,  4.57s/it, gen_loss=4.96, vuln_loss=0.444, synth_loss=0.864, diversity_loss=-0, d_penalty=0, g_penalty=0, is_synthetic=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     trainer.discriminator_optimizer.load_state_dict(checkpoint[\u001b[33m'\u001b[39m\u001b[33mdiscriminator_optimizer_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcheckpoints_v1_512_retrain/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/smrt-transformer/02. Research-model/train.py:370\u001b[39m, in \u001b[36mSmartContractTrainer.train\u001b[39m\u001b[34m(self, num_epochs, checkpoint_dir)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m train_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[32m    373\u001b[39m val_metrics = \u001b[38;5;28mself\u001b[39m.validate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/smrt-transformer/02. Research-model/train.py:174\u001b[39m, in \u001b[36mSmartContractTrainer.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# Generate synthetic data\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     synthetic_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     synthetic_encoder_output = synthetic_outputs[\u001b[33m'\u001b[39m\u001b[33mencoder_output\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Train discriminator on synthetic data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/smrt-transformer/02. Research-model/model.py:138\u001b[39m, in \u001b[36mSmartContractTransformer.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, path_input_ids, path_attention_mask, target_ids)\u001b[39m\n\u001b[32m    135\u001b[39m max_len = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_length, \u001b[32m50\u001b[39m)  \u001b[38;5;66;03m# Limit generation length to prevent infinite loops\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len - \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     tgt_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_square_subsequent_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m    139\u001b[39m     tgt_pos = torch.arange(\u001b[32m0\u001b[39m, tgt.size(\u001b[32m1\u001b[39m), device=device).unsqueeze(\u001b[32m0\u001b[39m).expand(batch_size, -\u001b[32m1\u001b[39m)\n\u001b[32m    141\u001b[39m     tgt_emb = \u001b[38;5;28mself\u001b[39m.embedding(tgt) * (\u001b[38;5;28mself\u001b[39m.d_model ** \u001b[32m0.5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/smrt-transformer/02. Research-model/model.py:72\u001b[39m, in \u001b[36mSmartContractTransformer.generate_square_subsequent_mask\u001b[39m\u001b[34m(self, sz)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_square_subsequent_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, sz):\n\u001b[32m     71\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate a square mask for the sequence\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     mask = (\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43msz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m == \u001b[32m1\u001b[39m).transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     73\u001b[39m     mask = mask.float().masked_fill(mask == \u001b[32m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m'\u001b[39m)).masked_fill(mask == \u001b[32m1\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[32m0.0\u001b[39m))\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = SmartContractTrainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    learning_rate=0.02,\n",
    "    weight_decay=0.002,\n",
    "    max_grad_norm=1.0,\n",
    "    gpu_id=1\n",
    ")\n",
    "\n",
    "# Load optimizer states if available\n",
    "if 'optimizer_state_dict' in checkpoint:\n",
    "    trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "if 'discriminator_optimizer_state_dict' in checkpoint:\n",
    "    trainer.discriminator_optimizer.load_state_dict(checkpoint['discriminator_optimizer_state_dict'])\n",
    "\n",
    "# Start training\n",
    "trainer.train(num_epochs=200, checkpoint_dir='checkpoints_v1_512_retrain/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915c3b0-0b6d-46bc-99af-bf58c4d2f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e226c989-7536-4937-bd16-6d8ed7c5a181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [5.714763154154238,\n",
       "  3.898336564128107,\n",
       "  3.114519550866289,\n",
       "  2.8629532830988467,\n",
       "  2.710476746201044,\n",
       "  2.5816148477109526,\n",
       "  2.478081943489346,\n",
       "  2.4004833038616558,\n",
       "  2.3359127299116533,\n",
       "  2.278445080334961,\n",
       "  2.226736070610318,\n",
       "  2.176064258507589,\n",
       "  2.133767006896701,\n",
       "  2.0953575744930464,\n",
       "  2.059959278747498,\n",
       "  2.0266608089326397,\n",
       "  1.9989639464103186,\n",
       "  1.96819977703773,\n",
       "  1.9439609291054043,\n",
       "  1.920995210941601,\n",
       "  1.8968769354311374,\n",
       "  1.877222005557637,\n",
       "  1.8584496979656897,\n",
       "  1.8397725193868042,\n",
       "  1.8230667844591404,\n",
       "  1.803662594128032,\n",
       "  1.7828009001351157,\n",
       "  1.7586080471988723,\n",
       "  1.7356457554775735,\n",
       "  1.7124147005231956,\n",
       "  1.6921908695236025,\n",
       "  1.6700494166890623,\n",
       "  1.6523505355058452,\n",
       "  1.6360928625928555,\n",
       "  1.6187931600766692,\n",
       "  1.6047024387615942,\n",
       "  1.5919291949554866,\n",
       "  1.5820124040950427,\n",
       "  1.5728680969698157,\n",
       "  1.5661580242187139,\n",
       "  1.560153073001756,\n",
       "  1.553331805783298,\n",
       "  1.5466893115062488,\n",
       "  1.540573139435689,\n",
       "  1.5350496019770505,\n",
       "  1.529484899147697,\n",
       "  1.525331354423945,\n",
       "  1.5202832325645115,\n",
       "  1.5170413576095942,\n",
       "  1.5129811452782673,\n",
       "  1.5109079383578696,\n",
       "  1.5086980539819468,\n",
       "  1.505758800996622,\n",
       "  1.5036841800561536,\n",
       "  1.5009469081290625,\n",
       "  1.4990620377506663,\n",
       "  1.497122821129358,\n",
       "  1.4960700937881772,\n",
       "  1.4941040503648901,\n",
       "  1.4930102603708801,\n",
       "  1.4924381688649475,\n",
       "  1.491629820567346,\n",
       "  1.4892306469174712,\n",
       "  1.489339700329445,\n",
       "  1.4881300841395564,\n",
       "  1.486363574450195,\n",
       "  1.4858707726708513,\n",
       "  1.484618207211551,\n",
       "  1.483211068767804,\n",
       "  1.4813852475094702,\n",
       "  1.4803031291886282,\n",
       "  1.4803517321824085,\n",
       "  1.4786603375385872,\n",
       "  1.4793856893132327,\n",
       "  1.4774583257705327,\n",
       "  1.4778869764606943,\n",
       "  1.4778828324065378,\n",
       "  1.4772464856799883,\n",
       "  1.4762270643777056,\n",
       "  1.4765104319267122,\n",
       "  1.4567231882231038,\n",
       "  1.4529858526033845,\n",
       "  1.4527218836569504,\n",
       "  1.4516811441527053,\n",
       "  1.4509816617362583,\n",
       "  1.4509448939161338,\n",
       "  1.4501252334579648,\n",
       "  1.4499009027782637,\n",
       "  1.4495947295026816,\n",
       "  1.4492156948496702,\n",
       "  1.4408768500263982,\n",
       "  1.439543883791083,\n",
       "  1.4390656651244333,\n",
       "  1.4393862669646975,\n",
       "  1.4386324778847073,\n",
       "  1.4382277091030076,\n",
       "  1.4381309380173213,\n",
       "  1.4380882389460627,\n",
       "  1.4342162147341038,\n",
       "  1.4335752080080537,\n",
       "  1.4336234783466626,\n",
       "  1.4334049130616924,\n",
       "  1.4331109698110889,\n",
       "  1.4329729009522751,\n",
       "  1.4329336278523381,\n",
       "  1.4329222558515344,\n",
       "  1.4327569747630786,\n",
       "  1.4324472793948508,\n",
       "  1.4308124573334404,\n",
       "  1.4308223710229746,\n",
       "  1.4306438327306816,\n",
       "  1.430414132449938,\n",
       "  1.4305071783631216,\n",
       "  1.4304476608871943,\n",
       "  1.4303834334663723,\n",
       "  1.4303653979961108,\n",
       "  1.4296575530244429,\n",
       "  1.4295242958860435,\n",
       "  1.429404311500519,\n",
       "  1.4294759911510784,\n",
       "  1.4291893050604658,\n",
       "  1.4294376783220193,\n",
       "  1.4292976375625068,\n",
       "  1.4290626204532126,\n",
       "  1.4289905063719617,\n",
       "  1.4287454214963047,\n",
       "  1.428941427483389,\n",
       "  1.4289201799588713,\n",
       "  1.4288446130488701,\n",
       "  1.4287344514145681,\n",
       "  1.428909230609185,\n",
       "  1.428786999623295,\n",
       "  1.4286535568388083,\n",
       "  1.4287059066795078,\n",
       "  1.4285205974880415,\n",
       "  1.4285972146648662],\n",
       " 'val_loss': [4.569788109511137,\n",
       "  3.3172163777053356,\n",
       "  2.9460654444992542,\n",
       "  2.748626656830311,\n",
       "  2.5855635963380337,\n",
       "  2.470141179859638,\n",
       "  2.3775800988078117,\n",
       "  2.286674350500107,\n",
       "  2.2343258932232857,\n",
       "  2.1722591519355774,\n",
       "  2.099857000634074,\n",
       "  2.0488332360982895,\n",
       "  2.0101976171135902,\n",
       "  1.9607390351593494,\n",
       "  1.9329051729291677,\n",
       "  1.8937088176608086,\n",
       "  1.8688418976962566,\n",
       "  1.8463642038404942,\n",
       "  1.8214576430618763,\n",
       "  1.7981350664049387,\n",
       "  1.7789932563900948,\n",
       "  1.758696036413312,\n",
       "  1.735545575618744,\n",
       "  1.719496250152588,\n",
       "  1.7173063661903143,\n",
       "  1.6961412504315376,\n",
       "  1.6600014604628086,\n",
       "  1.637044845148921,\n",
       "  1.6286408118903637,\n",
       "  1.6080755237489939,\n",
       "  1.589432803913951,\n",
       "  1.5725995022803545,\n",
       "  1.5597865357995033,\n",
       "  1.544653134420514,\n",
       "  1.5352830924093723,\n",
       "  1.5286907609552145,\n",
       "  1.5310860238969326,\n",
       "  1.5296916756778955,\n",
       "  1.526393610984087,\n",
       "  1.5137705691158772,\n",
       "  1.5211698655039072,\n",
       "  1.500455379486084,\n",
       "  1.5061101578176022,\n",
       "  1.5020876303315163,\n",
       "  1.4973610900342464,\n",
       "  1.4995757266879082,\n",
       "  1.5030089654028416,\n",
       "  1.4942582659423351,\n",
       "  1.5038392022252083,\n",
       "  1.4929310195147991,\n",
       "  1.4882628303021193,\n",
       "  1.4890762772411108,\n",
       "  1.4935882166028023,\n",
       "  1.4907204490154982,\n",
       "  1.493597811087966,\n",
       "  1.4881037715822458,\n",
       "  1.4890065621584654,\n",
       "  1.486839883029461,\n",
       "  1.480757337063551,\n",
       "  1.477338895201683,\n",
       "  1.4831950180232525,\n",
       "  1.4743375070393085,\n",
       "  1.4751391299068928,\n",
       "  1.4859788734465837,\n",
       "  1.4781673327088356,\n",
       "  1.4734110943973064,\n",
       "  1.4845246765762568,\n",
       "  1.4765803646296263,\n",
       "  1.4730436373502016,\n",
       "  1.4790857508778572,\n",
       "  1.4762633703649044,\n",
       "  1.4749655313789845,\n",
       "  1.4727130606770515,\n",
       "  1.46557847969234,\n",
       "  1.476034689694643,\n",
       "  1.4708253350108862,\n",
       "  1.474815085530281,\n",
       "  1.4733268581330776,\n",
       "  1.470391470938921,\n",
       "  1.4722155760973692,\n",
       "  1.4639015104621649,\n",
       "  1.4593322854489088,\n",
       "  1.4615103658288717,\n",
       "  1.4542944766581059,\n",
       "  1.458794740960002,\n",
       "  1.459693169221282,\n",
       "  1.4572679717093706,\n",
       "  1.4585966896265745,\n",
       "  1.4579156842082739,\n",
       "  1.461875956505537,\n",
       "  1.4526603277772665,\n",
       "  1.451333075761795,\n",
       "  1.4545704238116741,\n",
       "  1.4536266550421715,\n",
       "  1.4534063693135977,\n",
       "  1.4513319302350283,\n",
       "  1.4517639353871346,\n",
       "  1.4511886220425367,\n",
       "  1.450055480003357,\n",
       "  1.4503355789929628,\n",
       "  1.4500991888344288,\n",
       "  1.4497654400765896,\n",
       "  1.4505800511687994,\n",
       "  1.4502953384071589,\n",
       "  1.4500595666468143,\n",
       "  1.4501823354512453,\n",
       "  1.4499141536653042,\n",
       "  1.449816182255745,\n",
       "  1.449805235490203,\n",
       "  1.449262471869588,\n",
       "  1.4495046455413103,\n",
       "  1.4496306218206882,\n",
       "  1.4492274299263954,\n",
       "  1.4494899287819862,\n",
       "  1.4493901524692774,\n",
       "  1.4493707213550806,\n",
       "  1.4489901941269636,\n",
       "  1.4489601496607065,\n",
       "  1.4490810222923756,\n",
       "  1.4492072202265263,\n",
       "  1.4491811841726303,\n",
       "  1.4491117987781763,\n",
       "  1.4491137322038412,\n",
       "  1.4489741660654545,\n",
       "  1.4490795005112886,\n",
       "  1.448835600167513,\n",
       "  1.4489763416349888,\n",
       "  1.4488502703607082,\n",
       "  1.4491836596280336,\n",
       "  1.4492255877703428,\n",
       "  1.4491680935025215,\n",
       "  1.4490576069802046,\n",
       "  1.449041772633791,\n",
       "  1.4490887857973576,\n",
       "  1.4490729551762342,\n",
       "  1.4489745441824198],\n",
       " 'contract_vuln_loss': [0.02549472167764021,\n",
       "  0.022737455631609962,\n",
       "  0.022755987728714,\n",
       "  0.022700513492194797,\n",
       "  0.022662370495144794,\n",
       "  0.022718636061363068,\n",
       "  0.02273131331259554,\n",
       "  0.022710617985061035,\n",
       "  0.022724525123216183,\n",
       "  0.02271048593603575,\n",
       "  0.0227046967171162,\n",
       "  0.02270844541931812,\n",
       "  0.022719862557977085,\n",
       "  0.022704649749745728,\n",
       "  0.022725366667030827,\n",
       "  0.02272182377607455,\n",
       "  0.0227038770796282,\n",
       "  0.022703884640642307,\n",
       "  0.022663203877923283,\n",
       "  0.02270967703268462,\n",
       "  0.022723490699919552,\n",
       "  0.02270472416720489,\n",
       "  0.02270738360114955,\n",
       "  0.02272058727786593,\n",
       "  0.022734554178953877,\n",
       "  0.022712310371191605,\n",
       "  0.02271220767171133,\n",
       "  0.022743216811462116,\n",
       "  0.02273995488911514,\n",
       "  0.02273132698794481,\n",
       "  0.02269701551454576,\n",
       "  0.02273434122634146,\n",
       "  0.02274039057874868,\n",
       "  0.022723224978496433,\n",
       "  0.022722634906501404,\n",
       "  0.022689365030895817,\n",
       "  0.02271885669278533,\n",
       "  0.02271685867456226,\n",
       "  0.022689045145518696,\n",
       "  0.02272085051821626,\n",
       "  0.02270231750451411,\n",
       "  0.022713076444747654,\n",
       "  0.022753774921001182,\n",
       "  0.022694483134350758,\n",
       "  0.022700640137340477,\n",
       "  0.022711450186201,\n",
       "  0.02270672205658887,\n",
       "  0.022696122681729408,\n",
       "  0.02269498376992969,\n",
       "  0.022739685114782318,\n",
       "  0.022681204274502903,\n",
       "  0.022674060007005104,\n",
       "  0.022690652567790195,\n",
       "  0.022709947937120327,\n",
       "  0.02271369113237137,\n",
       "  0.022716062294370808,\n",
       "  0.022687678483016643,\n",
       "  0.022678878180417618,\n",
       "  0.02272050442328684,\n",
       "  0.022719045217506027,\n",
       "  0.0226920179566258,\n",
       "  0.022721346003912655,\n",
       "  0.022727913079643437,\n",
       "  0.02271634119419241,\n",
       "  0.022754478665885952,\n",
       "  0.02268588403423902,\n",
       "  0.022704207758040062,\n",
       "  0.02268866050929538,\n",
       "  0.022717672841910553,\n",
       "  0.02269279797793377,\n",
       "  0.022735084211932342,\n",
       "  0.022662296033512226,\n",
       "  0.022731244236436993,\n",
       "  0.022679313630778564,\n",
       "  0.02270784588317155,\n",
       "  0.02270036645734263,\n",
       "  0.02272144151785275,\n",
       "  0.022716802176780144,\n",
       "  0.022693365348480907,\n",
       "  0.022697223387216862,\n",
       "  0.022650564814597485,\n",
       "  0.022663228169613678,\n",
       "  0.022660245879630562,\n",
       "  0.022656836035282246,\n",
       "  0.02265429267799666,\n",
       "  0.02271220493296036,\n",
       "  0.022683088597749534,\n",
       "  0.022675728708829567,\n",
       "  0.022678510039279114,\n",
       "  0.02266056006295761,\n",
       "  0.022687300439673687,\n",
       "  0.022662412485645222,\n",
       "  0.022649231090729416,\n",
       "  0.022671505554833195,\n",
       "  0.022668437454334125,\n",
       "  0.02266807725704588,\n",
       "  0.02267669252830414,\n",
       "  0.022669932050372772,\n",
       "  0.02264915035279254,\n",
       "  0.022647821484317657,\n",
       "  0.022638131304839146,\n",
       "  0.022634959124440968,\n",
       "  0.02264419592651925,\n",
       "  0.02263375023829018,\n",
       "  0.022698839342317326,\n",
       "  0.022666735412633938,\n",
       "  0.022666429213649433,\n",
       "  0.02264900984456063,\n",
       "  0.02265494151902293,\n",
       "  0.022648831531258205,\n",
       "  0.0226223526610864,\n",
       "  0.02262879206463989,\n",
       "  0.022658924266637077,\n",
       "  0.02267136648959911,\n",
       "  0.022659980445334564,\n",
       "  0.022682720515508896,\n",
       "  0.022643456795057762,\n",
       "  0.022621098445662163,\n",
       "  0.02265301258432064,\n",
       "  0.022655410519083263,\n",
       "  0.02265176636060118,\n",
       "  0.02265457174714964,\n",
       "  0.022630995069538418,\n",
       "  0.02264345693862132,\n",
       "  0.022640387027012737,\n",
       "  0.022661860958625206,\n",
       "  0.022617046953189987,\n",
       "  0.02263844230400008,\n",
       "  0.02265744716326592,\n",
       "  0.02264198051257567,\n",
       "  0.022624688465958057,\n",
       "  0.022659717407445663,\n",
       "  0.022628439656594997,\n",
       "  0.02263797150019247,\n",
       "  0.02264242142203297,\n",
       "  0.022675817604121483],\n",
       " 'line_vuln_loss': [0.007299088952918408,\n",
       "  0.0003897672565791962,\n",
       "  0.0003894617620684398,\n",
       "  0.00038909551699272185,\n",
       "  0.0003891972857831127,\n",
       "  0.0003896983321516852,\n",
       "  0.0003893747289615348,\n",
       "  0.00038928237146414494,\n",
       "  0.0003890668356279463,\n",
       "  0.0003896216199187591,\n",
       "  0.000389227179903353,\n",
       "  0.0003893830281546111,\n",
       "  0.00038958318422690195,\n",
       "  0.0003893423346151955,\n",
       "  0.00038930279240260643,\n",
       "  0.0003893524794282554,\n",
       "  0.00038968627856448,\n",
       "  0.00038922876301368797,\n",
       "  0.0003894830361057419,\n",
       "  0.0003893872851362312,\n",
       "  0.0003892761009115829,\n",
       "  0.000389397042281668,\n",
       "  0.0003896151056647551,\n",
       "  0.0003893862991145602,\n",
       "  0.00038943168176420903,\n",
       "  0.00038893930735931593,\n",
       "  0.00038915635774504733,\n",
       "  0.0003895933940921995,\n",
       "  0.00038948896155359495,\n",
       "  0.000389391559660674,\n",
       "  0.0003893204893145651,\n",
       "  0.0003891549815249854,\n",
       "  0.00038913797907873607,\n",
       "  0.00038899245354201526,\n",
       "  0.00038906462172372085,\n",
       "  0.000389261145223845,\n",
       "  0.0003891406463930317,\n",
       "  0.00038937327014641255,\n",
       "  0.0003889317049901645,\n",
       "  0.00038923279481448543,\n",
       "  0.00038897124317653197,\n",
       "  0.0003895028241796846,\n",
       "  0.0003891746562320312,\n",
       "  0.00038921028098851633,\n",
       "  0.00038911422120780856,\n",
       "  0.00038908293366978836,\n",
       "  0.00038928838543135583,\n",
       "  0.00038951055780366033,\n",
       "  0.000389371914230011,\n",
       "  0.0003888217432414303,\n",
       "  0.000388994210642641,\n",
       "  0.00038896236386243993,\n",
       "  0.0003895975504757969,\n",
       "  0.0003890425739616612,\n",
       "  0.00038910951317433387,\n",
       "  0.0003890712510701453,\n",
       "  0.00038937702787655365,\n",
       "  0.00038901402051569784,\n",
       "  0.00038942833143017685,\n",
       "  0.00038946592052266544,\n",
       "  0.0003887833336624961,\n",
       "  0.00038944315034141505,\n",
       "  0.00038896023077023375,\n",
       "  0.0003894429484551603,\n",
       "  0.00038918084068096835,\n",
       "  0.0003890166827109403,\n",
       "  0.0003890090880491274,\n",
       "  0.00038907467980046385,\n",
       "  0.0003891397402055834,\n",
       "  0.0003889906195405569,\n",
       "  0.0003891777219121959,\n",
       "  0.0003889697737781986,\n",
       "  0.0003887559230307765,\n",
       "  0.0003891861895163702,\n",
       "  0.00038951793729268723,\n",
       "  0.00038866871438060957,\n",
       "  0.0003891905995519287,\n",
       "  0.0003892106888447649,\n",
       "  0.0003888039446961348,\n",
       "  0.0003888801649247808,\n",
       "  0.0003885021654676403,\n",
       "  0.00038858968402183587,\n",
       "  0.00038905992134006715,\n",
       "  0.00038845516197620833,\n",
       "  0.0003885570617338363,\n",
       "  0.00038868941105773354,\n",
       "  0.00038885124739334947,\n",
       "  0.0003886543449682548,\n",
       "  0.0003888277087214296,\n",
       "  0.00038860378649569827,\n",
       "  0.00038878946346985435,\n",
       "  0.0003888042938270654,\n",
       "  0.00038860040464210266,\n",
       "  0.0003887068125025439,\n",
       "  0.00038860616466976524,\n",
       "  0.00038848845365230847,\n",
       "  0.0003887153768510716,\n",
       "  0.0003884973020795291,\n",
       "  0.00038854549716000366,\n",
       "  0.0003883662659365669,\n",
       "  0.00038845788226407683,\n",
       "  0.00038854308746637327,\n",
       "  0.00038855953015278465,\n",
       "  0.0003888179874093651,\n",
       "  0.0003884733818943535,\n",
       "  0.0003884686906559746,\n",
       "  0.00038853542188554263,\n",
       "  0.00038855437382829297,\n",
       "  0.00038851482770451724,\n",
       "  0.0003883557561752773,\n",
       "  0.00038847177917056757,\n",
       "  0.0003883396367944608,\n",
       "  0.0003885552851347945,\n",
       "  0.00038852183597591215,\n",
       "  0.000388261073189806,\n",
       "  0.0003882901194452938,\n",
       "  0.0003887176685188915,\n",
       "  0.00038835685912992716,\n",
       "  0.0003881276045763026,\n",
       "  0.00038845153400543785,\n",
       "  0.0003886427281110588,\n",
       "  0.000388431299020897,\n",
       "  0.0003883677146861352,\n",
       "  0.0003885617835139819,\n",
       "  0.000388295576240952,\n",
       "  0.00038854990995639986,\n",
       "  0.00038826757260430753,\n",
       "  0.0003882793178979727,\n",
       "  0.00038879233169260806,\n",
       "  0.000388582743908621,\n",
       "  0.00038829713036249143,\n",
       "  0.00038850641163597953,\n",
       "  0.00038846101293896174,\n",
       "  0.00038816933256708336,\n",
       "  0.00038847347075881606,\n",
       "  0.0003885729374132108],\n",
       " 'learning_rate': [0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.0005,\n",
       "  0.00025,\n",
       "  0.00025,\n",
       "  0.00025,\n",
       "  0.00025,\n",
       "  0.00025,\n",
       "  0.00025,\n",
       "  0.00025,\n",
       "  0.00025,\n",
       "  0.000125,\n",
       "  0.000125,\n",
       "  0.000125,\n",
       "  0.000125,\n",
       "  0.000125,\n",
       "  0.000125,\n",
       "  0.000125,\n",
       "  0.000125,\n",
       "  0.000125,\n",
       "  0.000125,\n",
       "  6.25e-05,\n",
       "  6.25e-05,\n",
       "  6.25e-05,\n",
       "  6.25e-05,\n",
       "  6.25e-05,\n",
       "  6.25e-05,\n",
       "  6.25e-05,\n",
       "  6.25e-05,\n",
       "  3.125e-05,\n",
       "  3.125e-05,\n",
       "  3.125e-05,\n",
       "  3.125e-05,\n",
       "  3.125e-05,\n",
       "  3.125e-05,\n",
       "  3.125e-05,\n",
       "  1.5625e-05,\n",
       "  1.5625e-05,\n",
       "  1.5625e-05,\n",
       "  1.5625e-05,\n",
       "  1.5625e-05,\n",
       "  1.5625e-05,\n",
       "  1.5625e-05,\n",
       "  1.5625e-05,\n",
       "  1.5625e-05,\n",
       "  7.8125e-06,\n",
       "  7.8125e-06,\n",
       "  7.8125e-06,\n",
       "  7.8125e-06]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e7129b-5411-4831-82fa-337a880d3d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot generator loss\n",
    "plt.subplot(2, 2, 1)\n",
    "#plt.plot(trainer.training_history['train_loss'], label='Train')\n",
    "plt.plot(trainer.training_history['val_loss'], label='Validation')\n",
    "plt.title('Generator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot vulnerability loss\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(trainer.training_history['contract_vuln_loss'])\n",
    "plt.title('Contract Vulnerability Detection Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot synthetic loss\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(trainer.training_history['line_vuln_loss'])\n",
    "plt.title('Line Vulnerability Detection Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot learning rate\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(trainer.training_history['learning_rate'])\n",
    "plt.title('Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd835ecb-c7c1-4b26-96bc-0e4904f46bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 20\n",
      "Previous validation loss: 0.0070\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_epoch_20_model_v3.pt')  # Change this to your checkpoint file\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load model states\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "model.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "model.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "# Load optimizer states\n",
    "trainer.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "trainer.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "trainer.optimizer_decoder.load_state_dict(checkpoint['optimizer_decoder_state_dict'])\n",
    "\n",
    "# Get the epoch to start from and best validation loss\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_val_loss = checkpoint['val_loss']\n",
    "\n",
    "print(f\"Loaded checkpoint from epoch {start_epoch + 1}\")\n",
    "print(f\"Previous validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26779f5-56bc-4d32-9c6a-81ce63c676c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training epoch...\n"
     ]
    }
   ],
   "source": [
    "# Training loop - start from the next epoch\n",
    "num_epochs = 120\n",
    "\n",
    "for epoch in range(start_epoch + 1, num_epochs):  # Start from the next epoch\n",
    "    # Start timer for this epoch\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    g_loss, d_loss, decoder_loss = trainer.train_epoch()\n",
    "    val_loss = trainer.validate()\n",
    "    \n",
    "    # Calculate epoch time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Print training progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Generator Loss: {g_loss:.4f}\")\n",
    "    print(f\"Discriminator Loss: {d_loss:.4f}\")\n",
    "    print(f\"Decoder Loss: {decoder_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Epoch Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint = {\n",
    "            # Model states\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'generator_state_dict': model.generator.state_dict(),\n",
    "            'discriminator_state_dict': model.discriminator.state_dict(),\n",
    "            'decoder_state_dict': model.decoder.state_dict(),\n",
    "            \n",
    "            # Optimizer states\n",
    "            'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "            'optimizer_decoder_state_dict': trainer.optimizer_decoder.state_dict(),\n",
    "            \n",
    "            # Loss values\n",
    "            'g_loss': g_loss,\n",
    "            'd_loss': d_loss,\n",
    "            'decoder_loss': decoder_loss,\n",
    "            'val_loss': val_loss,\n",
    "            \n",
    "            # Model configuration\n",
    "            'model_config': {\n",
    "                'vocab_size': model.decoder.vocab_size,\n",
    "                'max_length': model.decoder.max_length\n",
    "            },\n",
    "            \n",
    "            # Training metadata\n",
    "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'epoch_time': epoch_time\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}_model_v3.pt')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Saved checkpoint for epoch {epoch+1}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_path = os.path.join(checkpoint_dir, 'best_model_v3.pt')\n",
    "            torch.save(checkpoint, best_model_path)\n",
    "            print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a9277d-d896-44c6-af23-db7512b8f557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1088dc1-ee3e-457b-980f-cbfa67896dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6116b2b-55dc-4842-8794-88283e9e2acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ca8665-9d47-4383-8fbf-ec85abb2ba8d",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "\n",
    "1. Input Processing:\n",
    "Initial input: [32, 512] (batch_size=32, sequence_length=512)\n",
    "After embedding: [32, 512, 512] (batch_size=32, sequence_length=512, embedding_dim=512)\n",
    "This is correct because the embedding layer converts each token to a 512-dimensional vector\n",
    "\n",
    "2. Path Embeddings Processing:\n",
    "Initial path embeddings: [32, 768] (batch_size=32, code2vec_dim=768)\n",
    "After path embedding layer: [32, 512] (batch_size=32, transformer_dim=512)\n",
    "The linear layer converts from code2vec's 768 dimensions to transformer's 512 dimensions\n",
    "After expansion: [32, 512, 512] (batch_size=32, sequence_length=512, transformer_dim=512)\n",
    "The path embeddings are expanded to match the sequence length\n",
    "\n",
    "3. Final Shape:\n",
    "[32, 512, 512] (batch_size=32, sequence_length=512, transformer_dim=512)\n",
    "This is the correct shape for the transformer layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9d4bfa-25f5-4b56-b679-44fc3102049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for epoch 10\n"
     ]
    }
   ],
   "source": [
    "checkpoint = {\n",
    "    # Model states\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'generator_state_dict': model.generator.state_dict(),\n",
    "    'discriminator_state_dict': model.discriminator.state_dict(),\n",
    "    'decoder_state_dict': model.decoder.state_dict(),\n",
    "    \n",
    "    # Optimizer states\n",
    "    'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "    'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "    'optimizer_decoder_state_dict': trainer.optimizer_decoder.state_dict(),\n",
    "    \n",
    "    # Loss values\n",
    "    'g_loss': g_loss,\n",
    "    'd_loss': d_loss,\n",
    "    'decoder_loss': decoder_loss,\n",
    "    'val_loss': val_loss,\n",
    "    \n",
    "    # Model configuration\n",
    "    'model_config': {\n",
    "        #'d_model': model.d_model,\n",
    "        'vocab_size': model.decoder.vocab_size,\n",
    "        'max_length': model.decoder.max_length\n",
    "    },\n",
    "    \n",
    "    # Training metadata\n",
    "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'epoch_time': epoch_time\n",
    "}\n",
    "\n",
    "# Save regular checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}_model_v3.pt')\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Saved checkpoint for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "812eafcf-c1de-4283-905d-9943fcacffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [119/120]\n",
      "Generator Loss: 10.5703\n",
      "Discriminator Loss: 0.0062\n",
      "Validation Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "print(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "print(f\"Generator Loss: {g_loss:.4f}\")\n",
    "print(f\"Discriminator Loss: {d_loss:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f2e17-41c2-4553-a974-d92ec9add514",
   "metadata": {},
   "source": [
    "## Save model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c34b13-9ee1-466b-a751-f7d898d7bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'epoch': 100,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    #'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "    #'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "    'gen_loss': model.gen_loss,\n",
    "    'synth_loss': model.synth_loss,\n",
    "    'vul_loss': model.vul_loss,\n",
    "    'val_loss': model.val_loss,\n",
    "    'model_config': {\n",
    "        'd_model': 768,\n",
    "    }\n",
    "}, 'final_model_v1_512.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a542cf7e-3f9a-405f-b855-74f4ed22cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training loop\n",
    "# Save the final model and training state\n",
    "\n",
    "\n",
    "# If you want to save just the model for inference\n",
    "torch.save(model.state_dict(), 'model_weights_v1_512.pt')\n",
    "\n",
    "# If you want to save the entire model\n",
    "torch.save(model, 'full_model_v1_512.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4518ba2-796c-4576-b707-dce9b7b50f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model with additional information\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_G_state_dict': trainer.optimizer_G.state_dict(),\n",
    "    'optimizer_D_state_dict': trainer.optimizer_D.state_dict(),\n",
    "    'g_loss': g_loss,\n",
    "    'd_loss': d_loss,\n",
    "    'val_loss': val_loss,\n",
    "    'model_config': {\n",
    "        'd_model': 768\n",
    "    },\n",
    "    'training_config': {\n",
    "        'learning_rate': 0.0002,\n",
    "        'beta1': 0.5,\n",
    "        'batch_size': 32\n",
    "    },\n",
    "    'training_history': {\n",
    "        'g_losses': g_loss,  # List of generator losses\n",
    "        'd_losses': d_loss,  # List of discriminator losses\n",
    "        'val_losses': val_loss  # List of validation losses\n",
    "    }\n",
    "}, 'final_model_v4_with_history.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bdb45a-1d83-4f1c-ba3d-537122d6d20d",
   "metadata": {},
   "source": [
    "# Load Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9c635a8-7b46-4a65-8310-6752630ea0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full training state\n",
    "checkpoint = torch.load('final_model_v4.pt')\n",
    "model = SmartContractVulnerabilityGAN(**checkpoint['model_config'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.cuda() \n",
    "\n",
    "# Initialize trainer with loaded model\n",
    "trainer = VulnerabilityDetectionTrainer(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader\n",
    ")\n",
    "\n",
    "# Load optimizer states if needed\n",
    "trainer.optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "trainer.optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73a2faac-91c0-42d9-8166-27b46ca15918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you just want to load the model weights\n",
    "model = SmartContractVulnerabilityGAN(d_model=768)\n",
    "model.load_state_dict(torch.load('model_weights.pt'))\n",
    "model = model.cuda()  # Move to GPU if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084e260-1dab-4400-8cd1-23de24a06f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or if you saved the entire model\n",
    "model = torch.load('full_model.pt')\n",
    "model = model.cuda()  # Move to GPU if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d56c1-6276-4a53-a46f-eef634bdbcb5",
   "metadata": {},
   "source": [
    "## Model Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53f7a67e-c86b-4b37-83cd-ac2bd8f74be7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of model: <class 'model.SmartContractVulnerabilityGAN'>\n",
      "\n",
      "Model Components:\n",
      "--------------------------------------------------\n",
      "\n",
      "Model Attributes:\n",
      "- T_destination\n",
      "- add_module\n",
      "- apply\n",
      "- bfloat16\n",
      "- buffers\n",
      "- call_super_init\n",
      "- children\n",
      "- codebert\n",
      "- compile\n",
      "- cpu\n",
      "- cuda\n",
      "- decode_embeddings\n",
      "- decoder\n",
      "- discriminator\n",
      "- double\n",
      "- dump_patches\n",
      "- eval\n",
      "- extra_repr\n",
      "- float\n",
      "- forward\n",
      "- generate_code\n",
      "- generator\n",
      "- get_buffer\n",
      "- get_extra_state\n",
      "- get_parameter\n",
      "- get_submodule\n",
      "- half\n",
      "- ipu\n",
      "- load_state_dict\n",
      "- modules\n",
      "- named_buffers\n",
      "- named_children\n",
      "- named_modules\n",
      "- named_parameters\n",
      "- parameters\n",
      "- register_backward_hook\n",
      "- register_buffer\n",
      "- register_forward_hook\n",
      "- register_forward_pre_hook\n",
      "- register_full_backward_hook\n",
      "- register_full_backward_pre_hook\n",
      "- register_load_state_dict_post_hook\n",
      "- register_module\n",
      "- register_parameter\n",
      "- register_state_dict_pre_hook\n",
      "- requires_grad_\n",
      "- set_extra_state\n",
      "- share_memory\n",
      "- state_dict\n",
      "- to\n",
      "- to_empty\n",
      "- tokenizer\n",
      "- train\n",
      "- training\n",
      "- transformer\n",
      "- type\n",
      "- xpu\n",
      "- zero_grad\n",
      "\n",
      "Model Structure:\n",
      "SmartContractVulnerabilityGAN(\n",
      "  (codebert): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (transformer): SmartContractTransformer(\n",
      "    (transformer): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (generator): Generator(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (5): ReLU()\n",
      "      (6): Linear(in_features=256, out_features=768, bias=True)\n",
      "      (7): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (discriminator): Discriminator(\n",
      "    (main): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (6): LeakyReLU(negative_slope=0.2)\n",
      "      (7): Dropout(p=0.3, inplace=False)\n",
      "      (8): Linear(in_features=256, out_features=1, bias=True)\n",
      "      (9): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (decoder): CodeDecoder(\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=1536, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1536, out_features=768, bias=True)\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embedding): Embedding(50000, 768)\n",
      "    (output_layer): Linear(in_features=768, out_features=50000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of model:\", type(model))\n",
    "\n",
    "# If it's a SmartContractVulnerabilityGAN object, we can inspect its components directly\n",
    "print(\"\\nModel Components:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Print model attributes\n",
    "print(\"\\nModel Attributes:\")\n",
    "for attr in dir(model):\n",
    "    if not attr.startswith('_'):  # Skip private attributes\n",
    "        print(f\"- {attr}\")\n",
    "\n",
    "# Print model structure\n",
    "print(\"\\nModel Structure:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b9b5e4b-7ac0-4cdf-a029-11b68f2d6b00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model State Dict:\n",
      "- codebert.embeddings.word_embeddings.weight\n",
      "- codebert.embeddings.position_embeddings.weight\n",
      "- codebert.embeddings.token_type_embeddings.weight\n",
      "- codebert.embeddings.LayerNorm.weight\n",
      "- codebert.embeddings.LayerNorm.bias\n",
      "- codebert.encoder.layer.0.attention.self.query.weight\n",
      "- codebert.encoder.layer.0.attention.self.query.bias\n",
      "- codebert.encoder.layer.0.attention.self.key.weight\n",
      "- codebert.encoder.layer.0.attention.self.key.bias\n",
      "- codebert.encoder.layer.0.attention.self.value.weight\n",
      "- codebert.encoder.layer.0.attention.self.value.bias\n",
      "- codebert.encoder.layer.0.attention.output.dense.weight\n",
      "- codebert.encoder.layer.0.attention.output.dense.bias\n",
      "- codebert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.0.intermediate.dense.weight\n",
      "- codebert.encoder.layer.0.intermediate.dense.bias\n",
      "- codebert.encoder.layer.0.output.dense.weight\n",
      "- codebert.encoder.layer.0.output.dense.bias\n",
      "- codebert.encoder.layer.0.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.0.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.1.attention.self.query.weight\n",
      "- codebert.encoder.layer.1.attention.self.query.bias\n",
      "- codebert.encoder.layer.1.attention.self.key.weight\n",
      "- codebert.encoder.layer.1.attention.self.key.bias\n",
      "- codebert.encoder.layer.1.attention.self.value.weight\n",
      "- codebert.encoder.layer.1.attention.self.value.bias\n",
      "- codebert.encoder.layer.1.attention.output.dense.weight\n",
      "- codebert.encoder.layer.1.attention.output.dense.bias\n",
      "- codebert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.1.intermediate.dense.weight\n",
      "- codebert.encoder.layer.1.intermediate.dense.bias\n",
      "- codebert.encoder.layer.1.output.dense.weight\n",
      "- codebert.encoder.layer.1.output.dense.bias\n",
      "- codebert.encoder.layer.1.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.1.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.2.attention.self.query.weight\n",
      "- codebert.encoder.layer.2.attention.self.query.bias\n",
      "- codebert.encoder.layer.2.attention.self.key.weight\n",
      "- codebert.encoder.layer.2.attention.self.key.bias\n",
      "- codebert.encoder.layer.2.attention.self.value.weight\n",
      "- codebert.encoder.layer.2.attention.self.value.bias\n",
      "- codebert.encoder.layer.2.attention.output.dense.weight\n",
      "- codebert.encoder.layer.2.attention.output.dense.bias\n",
      "- codebert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.2.intermediate.dense.weight\n",
      "- codebert.encoder.layer.2.intermediate.dense.bias\n",
      "- codebert.encoder.layer.2.output.dense.weight\n",
      "- codebert.encoder.layer.2.output.dense.bias\n",
      "- codebert.encoder.layer.2.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.2.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.3.attention.self.query.weight\n",
      "- codebert.encoder.layer.3.attention.self.query.bias\n",
      "- codebert.encoder.layer.3.attention.self.key.weight\n",
      "- codebert.encoder.layer.3.attention.self.key.bias\n",
      "- codebert.encoder.layer.3.attention.self.value.weight\n",
      "- codebert.encoder.layer.3.attention.self.value.bias\n",
      "- codebert.encoder.layer.3.attention.output.dense.weight\n",
      "- codebert.encoder.layer.3.attention.output.dense.bias\n",
      "- codebert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.3.intermediate.dense.weight\n",
      "- codebert.encoder.layer.3.intermediate.dense.bias\n",
      "- codebert.encoder.layer.3.output.dense.weight\n",
      "- codebert.encoder.layer.3.output.dense.bias\n",
      "- codebert.encoder.layer.3.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.3.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.4.attention.self.query.weight\n",
      "- codebert.encoder.layer.4.attention.self.query.bias\n",
      "- codebert.encoder.layer.4.attention.self.key.weight\n",
      "- codebert.encoder.layer.4.attention.self.key.bias\n",
      "- codebert.encoder.layer.4.attention.self.value.weight\n",
      "- codebert.encoder.layer.4.attention.self.value.bias\n",
      "- codebert.encoder.layer.4.attention.output.dense.weight\n",
      "- codebert.encoder.layer.4.attention.output.dense.bias\n",
      "- codebert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.4.intermediate.dense.weight\n",
      "- codebert.encoder.layer.4.intermediate.dense.bias\n",
      "- codebert.encoder.layer.4.output.dense.weight\n",
      "- codebert.encoder.layer.4.output.dense.bias\n",
      "- codebert.encoder.layer.4.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.4.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.5.attention.self.query.weight\n",
      "- codebert.encoder.layer.5.attention.self.query.bias\n",
      "- codebert.encoder.layer.5.attention.self.key.weight\n",
      "- codebert.encoder.layer.5.attention.self.key.bias\n",
      "- codebert.encoder.layer.5.attention.self.value.weight\n",
      "- codebert.encoder.layer.5.attention.self.value.bias\n",
      "- codebert.encoder.layer.5.attention.output.dense.weight\n",
      "- codebert.encoder.layer.5.attention.output.dense.bias\n",
      "- codebert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.5.intermediate.dense.weight\n",
      "- codebert.encoder.layer.5.intermediate.dense.bias\n",
      "- codebert.encoder.layer.5.output.dense.weight\n",
      "- codebert.encoder.layer.5.output.dense.bias\n",
      "- codebert.encoder.layer.5.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.5.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.6.attention.self.query.weight\n",
      "- codebert.encoder.layer.6.attention.self.query.bias\n",
      "- codebert.encoder.layer.6.attention.self.key.weight\n",
      "- codebert.encoder.layer.6.attention.self.key.bias\n",
      "- codebert.encoder.layer.6.attention.self.value.weight\n",
      "- codebert.encoder.layer.6.attention.self.value.bias\n",
      "- codebert.encoder.layer.6.attention.output.dense.weight\n",
      "- codebert.encoder.layer.6.attention.output.dense.bias\n",
      "- codebert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.6.intermediate.dense.weight\n",
      "- codebert.encoder.layer.6.intermediate.dense.bias\n",
      "- codebert.encoder.layer.6.output.dense.weight\n",
      "- codebert.encoder.layer.6.output.dense.bias\n",
      "- codebert.encoder.layer.6.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.6.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.7.attention.self.query.weight\n",
      "- codebert.encoder.layer.7.attention.self.query.bias\n",
      "- codebert.encoder.layer.7.attention.self.key.weight\n",
      "- codebert.encoder.layer.7.attention.self.key.bias\n",
      "- codebert.encoder.layer.7.attention.self.value.weight\n",
      "- codebert.encoder.layer.7.attention.self.value.bias\n",
      "- codebert.encoder.layer.7.attention.output.dense.weight\n",
      "- codebert.encoder.layer.7.attention.output.dense.bias\n",
      "- codebert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.7.intermediate.dense.weight\n",
      "- codebert.encoder.layer.7.intermediate.dense.bias\n",
      "- codebert.encoder.layer.7.output.dense.weight\n",
      "- codebert.encoder.layer.7.output.dense.bias\n",
      "- codebert.encoder.layer.7.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.7.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.8.attention.self.query.weight\n",
      "- codebert.encoder.layer.8.attention.self.query.bias\n",
      "- codebert.encoder.layer.8.attention.self.key.weight\n",
      "- codebert.encoder.layer.8.attention.self.key.bias\n",
      "- codebert.encoder.layer.8.attention.self.value.weight\n",
      "- codebert.encoder.layer.8.attention.self.value.bias\n",
      "- codebert.encoder.layer.8.attention.output.dense.weight\n",
      "- codebert.encoder.layer.8.attention.output.dense.bias\n",
      "- codebert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.8.intermediate.dense.weight\n",
      "- codebert.encoder.layer.8.intermediate.dense.bias\n",
      "- codebert.encoder.layer.8.output.dense.weight\n",
      "- codebert.encoder.layer.8.output.dense.bias\n",
      "- codebert.encoder.layer.8.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.8.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.9.attention.self.query.weight\n",
      "- codebert.encoder.layer.9.attention.self.query.bias\n",
      "- codebert.encoder.layer.9.attention.self.key.weight\n",
      "- codebert.encoder.layer.9.attention.self.key.bias\n",
      "- codebert.encoder.layer.9.attention.self.value.weight\n",
      "- codebert.encoder.layer.9.attention.self.value.bias\n",
      "- codebert.encoder.layer.9.attention.output.dense.weight\n",
      "- codebert.encoder.layer.9.attention.output.dense.bias\n",
      "- codebert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.9.intermediate.dense.weight\n",
      "- codebert.encoder.layer.9.intermediate.dense.bias\n",
      "- codebert.encoder.layer.9.output.dense.weight\n",
      "- codebert.encoder.layer.9.output.dense.bias\n",
      "- codebert.encoder.layer.9.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.9.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.10.attention.self.query.weight\n",
      "- codebert.encoder.layer.10.attention.self.query.bias\n",
      "- codebert.encoder.layer.10.attention.self.key.weight\n",
      "- codebert.encoder.layer.10.attention.self.key.bias\n",
      "- codebert.encoder.layer.10.attention.self.value.weight\n",
      "- codebert.encoder.layer.10.attention.self.value.bias\n",
      "- codebert.encoder.layer.10.attention.output.dense.weight\n",
      "- codebert.encoder.layer.10.attention.output.dense.bias\n",
      "- codebert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.10.intermediate.dense.weight\n",
      "- codebert.encoder.layer.10.intermediate.dense.bias\n",
      "- codebert.encoder.layer.10.output.dense.weight\n",
      "- codebert.encoder.layer.10.output.dense.bias\n",
      "- codebert.encoder.layer.10.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.10.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.11.attention.self.query.weight\n",
      "- codebert.encoder.layer.11.attention.self.query.bias\n",
      "- codebert.encoder.layer.11.attention.self.key.weight\n",
      "- codebert.encoder.layer.11.attention.self.key.bias\n",
      "- codebert.encoder.layer.11.attention.self.value.weight\n",
      "- codebert.encoder.layer.11.attention.self.value.bias\n",
      "- codebert.encoder.layer.11.attention.output.dense.weight\n",
      "- codebert.encoder.layer.11.attention.output.dense.bias\n",
      "- codebert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "- codebert.encoder.layer.11.intermediate.dense.weight\n",
      "- codebert.encoder.layer.11.intermediate.dense.bias\n",
      "- codebert.encoder.layer.11.output.dense.weight\n",
      "- codebert.encoder.layer.11.output.dense.bias\n",
      "- codebert.encoder.layer.11.output.LayerNorm.weight\n",
      "- codebert.encoder.layer.11.output.LayerNorm.bias\n",
      "- codebert.pooler.dense.weight\n",
      "- codebert.pooler.dense.bias\n",
      "- transformer.transformer.self_attn.in_proj_weight\n",
      "- transformer.transformer.self_attn.in_proj_bias\n",
      "- transformer.transformer.self_attn.out_proj.weight\n",
      "- transformer.transformer.self_attn.out_proj.bias\n",
      "- transformer.transformer.linear1.weight\n",
      "- transformer.transformer.linear1.bias\n",
      "- transformer.transformer.linear2.weight\n",
      "- transformer.transformer.linear2.bias\n",
      "- transformer.transformer.norm1.weight\n",
      "- transformer.transformer.norm1.bias\n",
      "- transformer.transformer.norm2.weight\n",
      "- transformer.transformer.norm2.bias\n",
      "- transformer.layer_norm.weight\n",
      "- transformer.layer_norm.bias\n",
      "- generator.main.0.weight\n",
      "- generator.main.0.bias\n",
      "- generator.main.1.weight\n",
      "- generator.main.1.bias\n",
      "- generator.main.3.weight\n",
      "- generator.main.3.bias\n",
      "- generator.main.4.weight\n",
      "- generator.main.4.bias\n",
      "- generator.main.6.weight\n",
      "- generator.main.6.bias\n",
      "- discriminator.main.0.weight\n",
      "- discriminator.main.0.bias\n",
      "- discriminator.main.1.weight\n",
      "- discriminator.main.1.bias\n",
      "- discriminator.main.4.weight\n",
      "- discriminator.main.4.bias\n",
      "- discriminator.main.5.weight\n",
      "- discriminator.main.5.bias\n",
      "- discriminator.main.8.weight\n",
      "- discriminator.main.8.bias\n",
      "- decoder.pos_encoder\n",
      "- decoder.decoder.layers.0.self_attn.in_proj_weight\n",
      "- decoder.decoder.layers.0.self_attn.in_proj_bias\n",
      "- decoder.decoder.layers.0.self_attn.out_proj.weight\n",
      "- decoder.decoder.layers.0.self_attn.out_proj.bias\n",
      "- decoder.decoder.layers.0.multihead_attn.in_proj_weight\n",
      "- decoder.decoder.layers.0.multihead_attn.in_proj_bias\n",
      "- decoder.decoder.layers.0.multihead_attn.out_proj.weight\n",
      "- decoder.decoder.layers.0.multihead_attn.out_proj.bias\n",
      "- decoder.decoder.layers.0.linear1.weight\n",
      "- decoder.decoder.layers.0.linear1.bias\n",
      "- decoder.decoder.layers.0.linear2.weight\n",
      "- decoder.decoder.layers.0.linear2.bias\n",
      "- decoder.decoder.layers.0.norm1.weight\n",
      "- decoder.decoder.layers.0.norm1.bias\n",
      "- decoder.decoder.layers.0.norm2.weight\n",
      "- decoder.decoder.layers.0.norm2.bias\n",
      "- decoder.decoder.layers.0.norm3.weight\n",
      "- decoder.decoder.layers.0.norm3.bias\n",
      "- decoder.decoder.layers.1.self_attn.in_proj_weight\n",
      "- decoder.decoder.layers.1.self_attn.in_proj_bias\n",
      "- decoder.decoder.layers.1.self_attn.out_proj.weight\n",
      "- decoder.decoder.layers.1.self_attn.out_proj.bias\n",
      "- decoder.decoder.layers.1.multihead_attn.in_proj_weight\n",
      "- decoder.decoder.layers.1.multihead_attn.in_proj_bias\n",
      "- decoder.decoder.layers.1.multihead_attn.out_proj.weight\n",
      "- decoder.decoder.layers.1.multihead_attn.out_proj.bias\n",
      "- decoder.decoder.layers.1.linear1.weight\n",
      "- decoder.decoder.layers.1.linear1.bias\n",
      "- decoder.decoder.layers.1.linear2.weight\n",
      "- decoder.decoder.layers.1.linear2.bias\n",
      "- decoder.decoder.layers.1.norm1.weight\n",
      "- decoder.decoder.layers.1.norm1.bias\n",
      "- decoder.decoder.layers.1.norm2.weight\n",
      "- decoder.decoder.layers.1.norm2.bias\n",
      "- decoder.decoder.layers.1.norm3.weight\n",
      "- decoder.decoder.layers.1.norm3.bias\n",
      "- decoder.decoder.layers.2.self_attn.in_proj_weight\n",
      "- decoder.decoder.layers.2.self_attn.in_proj_bias\n",
      "- decoder.decoder.layers.2.self_attn.out_proj.weight\n",
      "- decoder.decoder.layers.2.self_attn.out_proj.bias\n",
      "- decoder.decoder.layers.2.multihead_attn.in_proj_weight\n",
      "- decoder.decoder.layers.2.multihead_attn.in_proj_bias\n",
      "- decoder.decoder.layers.2.multihead_attn.out_proj.weight\n",
      "- decoder.decoder.layers.2.multihead_attn.out_proj.bias\n",
      "- decoder.decoder.layers.2.linear1.weight\n",
      "- decoder.decoder.layers.2.linear1.bias\n",
      "- decoder.decoder.layers.2.linear2.weight\n",
      "- decoder.decoder.layers.2.linear2.bias\n",
      "- decoder.decoder.layers.2.norm1.weight\n",
      "- decoder.decoder.layers.2.norm1.bias\n",
      "- decoder.decoder.layers.2.norm2.weight\n",
      "- decoder.decoder.layers.2.norm2.bias\n",
      "- decoder.decoder.layers.2.norm3.weight\n",
      "- decoder.decoder.layers.2.norm3.bias\n",
      "- decoder.embedding.weight\n",
      "- decoder.output_layer.weight\n",
      "- decoder.output_layer.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel State Dict:\")\n",
    "for key in model.state_dict().keys():\n",
    "    print(f\"- {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a484d444-0ca8-4797-8cb4-bc6d43761d32",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter Shapes:\n",
      "- codebert.embeddings.word_embeddings.weight: torch.Size([50265, 768])\n",
      "- codebert.embeddings.position_embeddings.weight: torch.Size([514, 768])\n",
      "- codebert.embeddings.token_type_embeddings.weight: torch.Size([1, 768])\n",
      "- codebert.embeddings.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.embeddings.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.0.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.0.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.0.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.0.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.0.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.1.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.1.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.1.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.1.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.1.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.2.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.2.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.2.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.2.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.2.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.3.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.3.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.3.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.3.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.3.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.4.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.4.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.4.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.4.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.4.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.5.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.5.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.5.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.5.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.5.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.6.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.6.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.6.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.6.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.6.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.7.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.7.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.7.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.7.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.7.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.8.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.8.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.8.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.8.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.8.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.9.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.9.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.9.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.9.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.9.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.10.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.10.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.10.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.10.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.10.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.11.attention.self.query.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.11.attention.self.key.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.11.attention.self.value.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])\n",
      "- codebert.encoder.layer.11.attention.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])\n",
      "- codebert.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])\n",
      "- codebert.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])\n",
      "- codebert.encoder.layer.11.output.dense.bias: torch.Size([768])\n",
      "- codebert.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])\n",
      "- codebert.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])\n",
      "- codebert.pooler.dense.weight: torch.Size([768, 768])\n",
      "- codebert.pooler.dense.bias: torch.Size([768])\n",
      "- transformer.transformer.self_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- transformer.transformer.self_attn.in_proj_bias: torch.Size([2304])\n",
      "- transformer.transformer.self_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- transformer.transformer.self_attn.out_proj.bias: torch.Size([768])\n",
      "- transformer.transformer.linear1.weight: torch.Size([768, 768])\n",
      "- transformer.transformer.linear1.bias: torch.Size([768])\n",
      "- transformer.transformer.linear2.weight: torch.Size([768, 768])\n",
      "- transformer.transformer.linear2.bias: torch.Size([768])\n",
      "- transformer.transformer.norm1.weight: torch.Size([768])\n",
      "- transformer.transformer.norm1.bias: torch.Size([768])\n",
      "- transformer.transformer.norm2.weight: torch.Size([768])\n",
      "- transformer.transformer.norm2.bias: torch.Size([768])\n",
      "- transformer.layer_norm.weight: torch.Size([768])\n",
      "- transformer.layer_norm.bias: torch.Size([768])\n",
      "- generator.main.0.weight: torch.Size([512, 768])\n",
      "- generator.main.0.bias: torch.Size([512])\n",
      "- generator.main.1.weight: torch.Size([512])\n",
      "- generator.main.1.bias: torch.Size([512])\n",
      "- generator.main.3.weight: torch.Size([256, 512])\n",
      "- generator.main.3.bias: torch.Size([256])\n",
      "- generator.main.4.weight: torch.Size([256])\n",
      "- generator.main.4.bias: torch.Size([256])\n",
      "- generator.main.6.weight: torch.Size([768, 256])\n",
      "- generator.main.6.bias: torch.Size([768])\n",
      "- discriminator.main.0.weight: torch.Size([512, 768])\n",
      "- discriminator.main.0.bias: torch.Size([512])\n",
      "- discriminator.main.1.weight: torch.Size([512])\n",
      "- discriminator.main.1.bias: torch.Size([512])\n",
      "- discriminator.main.4.weight: torch.Size([256, 512])\n",
      "- discriminator.main.4.bias: torch.Size([256])\n",
      "- discriminator.main.5.weight: torch.Size([256])\n",
      "- discriminator.main.5.bias: torch.Size([256])\n",
      "- discriminator.main.8.weight: torch.Size([1, 256])\n",
      "- discriminator.main.8.bias: torch.Size([1])\n",
      "- decoder.pos_encoder: torch.Size([1, 512, 768])\n",
      "- decoder.decoder.layers.0.self_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.0.self_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.0.self_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.0.self_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.0.multihead_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.0.multihead_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.0.multihead_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.0.multihead_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.0.linear1.weight: torch.Size([1536, 768])\n",
      "- decoder.decoder.layers.0.linear1.bias: torch.Size([1536])\n",
      "- decoder.decoder.layers.0.linear2.weight: torch.Size([768, 1536])\n",
      "- decoder.decoder.layers.0.linear2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm1.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm1.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm2.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm3.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.0.norm3.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.self_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.1.self_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.1.self_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.1.self_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.multihead_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.1.multihead_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.1.multihead_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.1.multihead_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.linear1.weight: torch.Size([1536, 768])\n",
      "- decoder.decoder.layers.1.linear1.bias: torch.Size([1536])\n",
      "- decoder.decoder.layers.1.linear2.weight: torch.Size([768, 1536])\n",
      "- decoder.decoder.layers.1.linear2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm1.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm1.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm2.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm3.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.1.norm3.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.self_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.2.self_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.2.self_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.2.self_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.multihead_attn.in_proj_weight: torch.Size([2304, 768])\n",
      "- decoder.decoder.layers.2.multihead_attn.in_proj_bias: torch.Size([2304])\n",
      "- decoder.decoder.layers.2.multihead_attn.out_proj.weight: torch.Size([768, 768])\n",
      "- decoder.decoder.layers.2.multihead_attn.out_proj.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.linear1.weight: torch.Size([1536, 768])\n",
      "- decoder.decoder.layers.2.linear1.bias: torch.Size([1536])\n",
      "- decoder.decoder.layers.2.linear2.weight: torch.Size([768, 1536])\n",
      "- decoder.decoder.layers.2.linear2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm1.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm1.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm2.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm2.bias: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm3.weight: torch.Size([768])\n",
      "- decoder.decoder.layers.2.norm3.bias: torch.Size([768])\n",
      "- decoder.embedding.weight: torch.Size([50000, 768])\n",
      "- decoder.output_layer.weight: torch.Size([50000, 768])\n",
      "- decoder.output_layer.bias: torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nParameter Shapes:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"- {name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4c1baab-ba74-4351-ad40-73e789233402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Configuration:\n",
      "- d_model: N/A\n",
      "- vocab_size: 50000\n",
      "- max_length: 512\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"- d_model: {model.d_model if hasattr(model, 'd_model') else 'N/A'}\")\n",
    "print(f\"- vocab_size: {model.decoder.vocab_size if hasattr(model, 'decoder') else 'N/A'}\")\n",
    "print(f\"- max_length: {model.decoder.max_length if hasattr(model, 'decoder') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7fe1a1c-e84e-4c4c-a0dd-584d27020773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generator Architecture:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SmartContractVulnerabilityGAN' object has no attribute 'CodeDecoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m'\u001b[39m\u001b[33mgenerator\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerator Architecture:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCodeDecoder\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pytorch_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1687\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SmartContractVulnerabilityGAN' object has no attribute 'CodeDecoder'"
     ]
    }
   ],
   "source": [
    "if hasattr(model, 'generator'):\n",
    "    print(\"\\nGenerator Architecture:\")\n",
    "    print(model.CodeDecoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeeee1b-7d47-43c7-b359-156adc04026c",
   "metadata": {},
   "source": [
    "### This is a GAN (Generative Adversarial Network) combined with a Transformer architecture for smart contract vulnerability detection.\n",
    "\n",
    "#### Here's the technical breakdown:\n",
    "#### 1. Architecture Components:\n",
    "-Transformer Encoder: Processes smart contract code using self-attention\n",
    "-Generator: Creates synthetic vulnerable code patterns\n",
    "-Discriminator: Distinguishes between real and synthetic vulnerabilities\n",
    "\n",
    "#### 2. Input Processing:\n",
    "-Takes smart contract code and its AST (Abstract Syntax Tree) paths\n",
    "-Uses CodeBERT to generate embeddings (768-dimensional vectors)\n",
    "-Processes both contract code and path information\n",
    "\n",
    "#### 3. Training Process:\n",
    "3.1. Generator Training:\n",
    "-Takes random noise and contract embeddings\n",
    "-Generates synthetic vulnerable code patterns\n",
    "-Tries to fool the discriminator\n",
    "\n",
    "3.2. Discriminator Training:\n",
    "-Takes real contract embeddings and generator outputs\n",
    "-Learns to distinguish real from synthetic vulnerabilities\n",
    "-Uses binary classification (real/fake)\n",
    "\n",
    "#### 4. Output:\n",
    "-Vulnerability scores for input contracts\n",
    "-Synthetic vulnerable code patterns for training\n",
    "-Binary classification of real vs. synthetic vulnerabilities\n",
    "\n",
    "#### The model essentially learns to:\n",
    "-Understand code patterns through the transformer\n",
    "-Generate realistic vulnerable code examples\n",
    "-Detect vulnerabilities in real contracts\n",
    "-Improve detection through adversarial training\n",
    "\n",
    "#### This approach combines the strengths of:\n",
    "Transformers for code understanding\n",
    "GANs for synthetic data generation\n",
    "Binary classification for vulnerability detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7bafd2-5e74-4e56-ad5f-2de3d7c88431",
   "metadata": {},
   "source": [
    "# 3. Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9a39804-39cc-434c-9387-0aaffb46e1e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SmartContractTrainer' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# For a single contract\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m(validation_dataset[\u001b[32m0\u001b[39m])  \u001b[38;5;66;03m# Pass a single contract's data\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVulnerability Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mvulnerability_score\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSynthetic Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33msynthetic_score\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SmartContractTrainer' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# For a single contract\n",
    "results = trainer.predict(validation_dataset[0])  # Pass a single contract's data\n",
    "\n",
    "print(f\"Vulnerability Score: {results['vulnerability_score']:.4f}\")\n",
    "print(f\"Synthetic Score: {results['synthetic_score']:.4f}\")\n",
    "print(f\"Is Vulnerable: {results['is_vulnerable']}\")\n",
    "print(f\"Is Synthetic: {results['is_synthetic']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71c6de-dd38-4a48-9d16-dc9fa7b1966e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0523c01a-e501-427d-8e76-23a25e6f5210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import SmartContractTransformer\n",
    "from train import Discriminator\n",
    "\n",
    "def load_trained_model(checkpoint_path, device='cuda:1'):\n",
    "    \"\"\"\n",
    "    Load the trained model and discriminator from checkpoint\n",
    "    \"\"\"\n",
    "    # Initialize model and discriminator\n",
    "    model = SmartContractTransformer()\n",
    "    discriminator = Discriminator()\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Load state dicts\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    \n",
    "    # Move to device\n",
    "    model = model.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    \n",
    "    # Set to eval mode\n",
    "    model.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    return model, discriminator\n",
    "\n",
    "def analyze_contract(contract_data, model, discriminator, tokenizer=None, device='cuda:1'):\n",
    "    \"\"\"\n",
    "    Analyze a smart contract for vulnerabilities and generate synthetic version\n",
    "    \"\"\"\n",
    "    # Move input data to device\n",
    "    input_ids = contract_data['input_ids'].unsqueeze(0).to(device)\n",
    "    attention_mask = contract_data['attention_mask'].unsqueeze(0).to(device)\n",
    "    path_input_ids = contract_data['path_input_ids'].unsqueeze(0).to(device)\n",
    "    path_attention_mask = contract_data['path_attention_mask'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Decode original contract if tokenizer is provided\n",
    "    original_text = None\n",
    "    if tokenizer is not None:\n",
    "        original_text = tokenizer.decode(input_ids[0].cpu().tolist())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get vulnerability score\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            path_input_ids=path_input_ids,\n",
    "            path_attention_mask=path_attention_mask,\n",
    "            target_ids=None\n",
    "        )\n",
    "        \n",
    "        # Get discriminator predictions\n",
    "        vuln_pred, synth_pred = discriminator(outputs['encoder_output'])\n",
    "        vulnerability_score = torch.sigmoid(vuln_pred).item()\n",
    "        synthetic_score = torch.sigmoid(synth_pred).item()\n",
    "        \n",
    "        result = {\n",
    "            'vulnerability_score': vulnerability_score,\n",
    "            'synthetic_score': synthetic_score,\n",
    "            'is_vulnerable': vulnerability_score > 0.5,\n",
    "            'is_synthetic': synthetic_score > 0.5,\n",
    "            'original_contract': {\n",
    "                'text': original_text,\n",
    "                'input_ids': input_ids[0].cpu().tolist()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Generate synthetic contract\n",
    "        synthetic_outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            path_input_ids=path_input_ids,\n",
    "            path_attention_mask=path_attention_mask,\n",
    "            target_ids=None\n",
    "        )\n",
    "        \n",
    "        # Get the generated sequence\n",
    "        generated_sequence = synthetic_outputs['generated_sequence']\n",
    "        \n",
    "        # Get discriminator predictions for synthetic contract\n",
    "        synth_vuln_pred, synth_synth_pred = discriminator(synthetic_outputs['encoder_output'])\n",
    "        synth_vulnerability_score = torch.sigmoid(synth_vuln_pred).item()\n",
    "        synth_synthetic_score = torch.sigmoid(synth_synth_pred).item()\n",
    "        \n",
    "        result['synthetic_contract'] = {\n",
    "            'sequence': generated_sequence[0].cpu().tolist(),\n",
    "            'vulnerability_score': synth_vulnerability_score,\n",
    "            'synthetic_score': synth_synthetic_score,\n",
    "            'is_vulnerable': synth_vulnerability_score > 0.5,\n",
    "            'is_synthetic': synth_synthetic_score > 0.5\n",
    "        }\n",
    "        \n",
    "        # If tokenizer is provided, decode the synthetic contract\n",
    "        if tokenizer is not None:\n",
    "            synthetic_tokens = result['synthetic_contract']['sequence']\n",
    "            result['synthetic_contract']['text'] = tokenizer.decode(synthetic_tokens)\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d10b5e58-9742-416c-9cd8-38089f60f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "checkpoint_path = 'checkpoints_v1/latest_model.pt'  # or 'best_model_epoch_X.pt'\n",
    "model_loaded, discriminator = load_trained_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "280ba2f6-c926-4fd4-bcd9-dd6720bb44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_data = val_dataset[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "530e0dbd-3fe7-4191-922e-d9fd9f1cf971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contract Analysis Results:\n",
      "--------------------------------------------------\n",
      "Vulnerability Score: 0.0151\n",
      "Synthetic Score: 0.7673\n",
      "Vulnerability Status: Safe\n",
      "Synthetic Status: Synthetic\n",
      "\n",
      "Original Contract:\n",
      "--------------------------------------------------\n",
      "<s>/**\n",
      " *Submitted for verification at Etherscan.io on 2020-11-22\n",
      "*/\n",
      "\n",
      "// SPDX-License-Identifier: MIT + WTFPL\n",
      "// File: contracts/uniswapv2/interfaces/IUniswapV2Factory.sol\n",
      "\n",
      "pragma solidity >=0.5.0;\n",
      "\n",
      "interface IUniswapV2Factory {\n",
      "    event PairCreated(address indexed token0, address indexed token1, address pair, uint);\n",
      "\n",
      "    function feeTo() external view returns (address);\n",
      "    function feeToSetter() external view returns (address);\n",
      "    function migrator() external view returns (address);\n",
      "\n",
      "    function getPair(address tokenA, address tokenB) external view returns (address pair);\n",
      "    function allPairs(uint) external view returns (address pair);\n",
      "    function allPairsLength() external view returns (uint);\n",
      "\n",
      "    function createPair(address tokenA, address tokenB) external</s>\n",
      "\n",
      "Synthetic Contract Analysis:\n",
      "--------------------------------------------------\n",
      "Vulnerability Score: 0.0151\n",
      "Synthetic Score: 0.7673\n",
      "Vulnerability Status: Safe\n",
      "Synthetic Status: Synthetic\n",
      "\n",
      "Generated Synthetic Contract:\n",
      "--------------------------------------------------\n",
      "<s>/**\n",
      " *Submitted for verification at Etherscan.io on 2020-09-12\n",
      "*/\n",
      "\n",
      "pragma solidity 0.6.6;\n",
      "\n",
      "\n",
      "/**\n",
      " * @dev Wrappers over Solidity's arithmetic operations with added overflow\n",
      " * checks.\n",
      " *\n",
      " * Arithmetic operations in Solidity wrap on overflow. This can easily result\n",
      " * in bugs, because programmers usually assume that an overflow raises an\n",
      " * error, which is the standard behavior in high level programming languages.\n",
      " * `SafeMath` restores this intuition by reverting the transaction when an\n",
      " * operation overflows.\n",
      " *\n",
      " * Using this library instead of the unchecked operations eliminates an entire\n",
      " * class of bugs, so it's recommended to use it always.\n",
      " */\n",
      "library SafeMath {\n",
      "  /**\n",
      "    * @dev Returns the addition of two unsigned integers, reverting on\n",
      "    * overflow.\n",
      "    *\n",
      "    * Counterpart to Solidity's `+` operator.\n",
      "    *\n",
      "    * Requirements:\n",
      "    * - Addition cannot overflow.\n",
      "    */\n",
      "  function add(uint256 a, uint256 b</s>\n"
     ]
    }
   ],
   "source": [
    "# Analyze the contract\n",
    "results = analyze_contract(contract_data, model_loaded, discriminator, tokenizer)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nContract Analysis Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Vulnerability Score: {results['vulnerability_score']:.4f}\")\n",
    "print(f\"Synthetic Score: {results['synthetic_score']:.4f}\")\n",
    "print(f\"Vulnerability Status: {'Vulnerable' if results['is_vulnerable'] else 'Safe'}\")\n",
    "print(f\"Synthetic Status: {'Synthetic' if results['is_synthetic'] else 'Real'}\")\n",
    "\n",
    "# Print original contract\n",
    "if results['original_contract']['text']:\n",
    "    print(\"\\nOriginal Contract:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(results['original_contract']['text'])\n",
    "    \n",
    "\n",
    "print(\"\\nSynthetic Contract Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Vulnerability Score: {results['synthetic_contract']['vulnerability_score']:.4f}\")\n",
    "print(f\"Synthetic Score: {results['synthetic_contract']['synthetic_score']:.4f}\")\n",
    "print(f\"Vulnerability Status: {'Vulnerable' if results['synthetic_contract']['is_vulnerable'] else 'Safe'}\")\n",
    "print(f\"Synthetic Status: {'Synthetic' if results['synthetic_contract']['is_synthetic'] else 'Real'}\")\n",
    "\n",
    "if 'text' in results['synthetic_contract']:\n",
    "    print(\"\\nGenerated Synthetic Contract:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(results['synthetic_contract']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdadd5d6-6f7d-4f52-8f69-b912bc30d5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aee0c4-4061-4050-aed3-144a0c775e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b59e7-1f96-49ad-8d2a-0485b2ff953f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e94209-f5b9-4eea-bb24-2482d771a1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
